{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLite itegration\n",
    "\n",
    "If you want to process your sensor data and store it for later, you can use the sqlite integration. Gensor's `Timeseries` and `Dataset` come with a `.to_sql()` method which is uses `pandas.Series.to_sql()` method under the hood to save the data in a SQLite database. \n",
    "\n",
    "It is a simple implementation, where each timeseries is stored in a separate schema (database table) which is named in the following pattern: `f\"{location}_{sensor}_{variable}_{unit}\".lower()`. There is a double check on duplicates. First, when you create a `Dataset`, duplicates are nicely handled by merging timeseries from the same location, sensor and of the same variable and unit. Secondly the `Timeseries.to_sql()` method is designed to ignore conflicts, so only new records are inserted into the database if you attempt to run the same commend twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensor as gs\n",
    "from gensor.testdata import all_paths, pb02a_plain\n",
    "\n",
    "pattern = r\"[A-Za-z]{2}\\d{2}[A-Za-z]{1}|Barodiver\"\n",
    "\n",
    "ds = gs.read_from_csv(path=all_paths, file_format=\"vanessen\", location_pattern=pattern)\n",
    "\n",
    "\n",
    "ds2 = gs.read_from_csv(\n",
    "    path=pb02a_plain, file_format=\"plain\", location=\"PB02A\", sensor=\"AV336\"\n",
    ")\n",
    "\n",
    "ds.add(ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `DatabaseConnection`\n",
    "\n",
    "Both saving and loading data from sqlite require a `DatabaseConnection` object to be passed as attribute. You can just instanciate it with empty parentheses to create a new database in the current working directory, or specify the path and name of the database.\n",
    "\n",
    "If you have an existing Gensor database, you can use `DatabaseConnection.get_timeseries_metadata()` to see if there already are some tables in the database that you want to use. If no arguments are provided, all records are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = gs.db.DatabaseConnection()\n",
    "df = db.get_timeseries_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset to the database is straightforward. You just need to call `.to_sql()` on the dataset instance and check the tables again to see that now there are a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving dataset to SQLite database\n",
    "\n",
    "Dataset, like Timeseries, can be saved to a SQLite database by simply calling `.to_sql()` method and passing the `DatabaseConneciton` object as argument.\n",
    "\n",
    "You can also check which tables are currently in the database by calling `DatabaseConnection.get_timeseries_metadata()`. That method will give you a dataframe with all the tables in the database. The names of the tables are composed of the location name, variable measured, unit and a uniqur 5 character hash. This is a compromise between ensuring possible addition of slightly varrying timeseries to the dataset (e.g., the same sensor at the same location but with different rope length).\n",
    "\n",
    "After running the cells below, you should see a dataframe with 6 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_sql(db)\n",
    "df = db.get_timeseries_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from SQLite\n",
    "\n",
    "Use `read_from_sql()` to retrieve timeseries from the database. By default, `load_all` parameter is set to True, so all tables from the database are loaded as `Dataset`. You can also provide parameters to retrieve only some of the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds: gs.Dataset = gs.read_from_sql(db)\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more timeseries to SQLite\n",
    "\n",
    "You can always add more timeseries to the same database. Below, we make a copy of one of the timeseries, updating it's `sensor_alt`, hence, making it slightly different from the origina. Then we add it to the dataset and call `to_sql()` method again with the same `DatabaseConnection` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_with_sensor_alt = new_ds[2].model_copy(update={\"sensor_alt\": 32.0}, deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_with_sensor_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amended_ds = new_ds.add(ts_with_sensor_alt)\n",
    "amended_ds.to_sql(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see now, we have a Dataset of 7, because the new timeseries is not equal to any of the existing timeseries (differs by `sensor_alt`).\n",
    "\n",
    "Even though we called `to_sql()` again on the same dataset extended by just one timeseries, we see that only one new table was created. This is because the method will figure out which timeseries are already there, and at best update those that have new records in the amended `Dataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amended_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db.get_timeseries_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
