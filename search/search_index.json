{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"gensor","text":"<p>Python library allowing to load, parse and analyze Groundwater sensor data from files: G(s)ensor.</p> <p>Warning</p> <p>Please mind that this package is in active development. API changes can (and probably will) happen. If  you like this package but it is lacking some functionality you need, do not hesitate to submit an issue in Github and/or contribute!</p>"},{"location":"#supported-sensor-formats","title":"Supported sensor formats","text":"<p>Each company producing sensors has their own data formats used for data exchange. Sometimes they also include common formats, like CSV, as export method, but still they come uniquely structured. Gensor, as of the first release, has a <code>read_from_csv()</code> function used to read timeseries from data exported from sensors which requires to indicate which format is being used. Based on the user choice, an appropriate parser is selected, which is preconfigured for a particular brand of sensors.</p> <p>As of now, the only pre-configured parser implemented is one for van Essen instruments. The developers hope, that with the contributions from the community, we can include more prewritten formats with tests and examples, so not everybody has to parse the files themselves each time.</p>"},{"location":"#van-essen-format","title":"van Essen format","text":""},{"location":"#what-do-the-van-essen-instruments-loggers-measure","title":"What do the van Essen Instruments loggers measure?","text":"<p>The loggers measure the abolute pressure at the membrane of the sensor, therefore the datasets require compensation for barometric pressure.</p>"},{"location":"#how-the-data-from-van-essen-instruments-is-collected","title":"How the data from van Essen instruments is collected?","text":"<p>If the loggers are not equipped with some kind of telemetric system (which is the case most of the times), it is necessary to go to the field once in a while and collect the data. That is done either with a laptop (using DiverOffice or DiverField software) or a small handheld device called DiverMate and an Android device. Each time the data is collected, a timeseries is generated (that includes old records that may have been already collected before and are stored in another CSV file). Normall, in Project Grow we collect the data every two months with DiverMate and then share the CSV files to Google Drive. Each time we export the data, we end up with a new CSV file that need to be merged to obtain a consistent timeseries.</p>"},{"location":"modules/","title":"API reference","text":""},{"location":"modules/#gensor.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[T]</code></p> <p>Store and operate on a collection of Timeseries.</p> <p>Attributes:</p> Name Type Description <code>timeseries</code> <code>list[Timeseries]</code> <p>A list of Timeseries objects.</p> Source code in <code>gensor/core/dataset.py</code> <pre><code>class Dataset(pyd.BaseModel, Generic[T]):\n    \"\"\"Store and operate on a collection of Timeseries.\n\n    Attributes:\n        timeseries (list[Timeseries]): A list of Timeseries objects.\n    \"\"\"\n\n    timeseries: list[T | None] = pyd.Field(default_factory=list)\n\n    def __iter__(self) -&gt; Any:\n        \"\"\"Allows to iterate directly over the dataset.\"\"\"\n        return iter(self.timeseries)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Gives the number of timeseries in the Dataset.\"\"\"\n        return len(self.timeseries)\n\n    def __repr__(self) -&gt; str:\n        return f\"Dataset({len(self)})\"\n\n    def __getitem__(self, index: int) -&gt; T | None:\n        \"\"\"Retrieve a Timeseries object by its index in the dataset.\n\n        Parameters:\n            index (int): The index of the Timeseries to retrieve.\n\n        Returns:\n            Timeseries: The Timeseries object at the specified index.\n\n        Raises:\n            IndexError: If the index is out of range.\n        \"\"\"\n        try:\n            return self.timeseries[index].model_copy(deep=True)\n        except IndexError:\n            raise IndexOutOfRangeError(index, len(self)) from None\n\n    def get_locations(self) -&gt; list:\n        \"\"\"List all unique locations in the dataset.\"\"\"\n        return [ts.location for ts in self.timeseries if ts is not None]\n\n    def add(self, other: T | list[T] | Dataset) -&gt; Dataset:\n        \"\"\"Appends new Timeseries to the Dataset.\n\n        If an equal Timeseries already exists, merge the new data into the existing\n        Timeseries, dropping duplicate timestamps.\n\n        Parameters:\n            other (Timeseries): The Timeseries object to add.\n        \"\"\"\n\n        # I need to check for BaseTimeseries instance in the add() method, but also\n        # type hint VarType T.\n        if isinstance(other, list | Dataset):\n            for ts in other:\n                if isinstance(ts, BaseTimeseries):\n                    self._add_single_timeseries(ts)  # type: ignore[arg-type]\n\n        elif isinstance(other, BaseTimeseries):\n            self._add_single_timeseries(other)\n\n        return self\n\n    def _add_single_timeseries(self, ts: T) -&gt; None:\n        \"\"\"Adds a single Timeseries to the Dataset or merges if an equal one exists.\"\"\"\n        for i, existing_ts in enumerate(self.timeseries):\n            if existing_ts == ts:\n                self.timeseries[i] = existing_ts.concatenate(ts)\n                return\n\n        self.timeseries.append(ts)\n\n        return\n\n    def filter(\n        self,\n        location: str | list | None = None,\n        variable: str | list | None = None,\n        unit: str | list | None = None,\n        **kwargs: dict[str, str | list],\n    ) -&gt; T | Dataset:\n        \"\"\"Return a Timeseries or a new Dataset filtered by station, sensor,\n        and/or variable.\n\n        Parameters:\n            location (Optional[str]): The location name.\n            variable (Optional[str]): The variable being measured.\n            unit (Optional[str]): Unit of the measurement.\n            **kwargs (dict): Attributes of subclassed timeseries used for filtering\n                (e.g., sensor, method).\n\n        Returns:\n            Timeseries | Dataset: A single Timeseries if exactly one match is found,\n                                   or a new Dataset if multiple matches are found.\n        \"\"\"\n\n        def matches(ts: T, attr: str, value: dict[str, str | list]) -&gt; bool | None:\n            \"\"\"Check if the Timeseries object has the attribute and if it matches the value.\"\"\"\n            if not hasattr(ts, attr):\n                message = f\"'{ts.__class__.__name__}' object has no attribute '{attr}'\"\n                raise AttributeError(message)\n            return getattr(ts, attr) in value\n\n        if isinstance(location, str):\n            location = [location]\n        if isinstance(variable, str):\n            variable = [variable]\n        if isinstance(unit, str):\n            unit = [unit]\n        for key, value in kwargs.items():\n            if isinstance(value, str):\n                kwargs[key] = [value]\n\n        matching_timeseries = [\n            ts\n            for ts in self.timeseries\n            if ts is not None\n            and (location is None or ts.location in location)\n            and (variable is None or ts.variable in variable)\n            and (unit is None or ts.unit in unit)\n            and all(matches(ts, attr, value) for attr, value in kwargs.items())\n        ]\n\n        if not matching_timeseries:\n            return Dataset()\n\n        if len(matching_timeseries) == 1:\n            return matching_timeseries[0].model_copy(deep=True)\n\n        return self.model_copy(update={\"timeseries\": matching_timeseries})\n\n    def to_sql(self, db: DatabaseConnection) -&gt; None:\n        \"\"\"Save the entire timeseries to a SQLite database.\n\n        Parameters:\n            db (DatabaseConnection): SQLite database connection object.\n        \"\"\"\n        for ts in self.timeseries:\n            if ts:\n                ts.to_sql(db)\n        return\n\n    def plot(\n        self,\n        include_outliers: bool = False,\n        plot_kwargs: dict[str, Any] | None = None,\n        legend_kwargs: dict[str, Any] | None = None,\n    ) -&gt; tuple[Figure, Axes]:\n        \"\"\"Plots the timeseries data, grouping by variable type.\n\n        Parameters:\n            include_outliers (bool): Whether to include outliers in the plot.\n            plot_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.plot() method to customize the plot.\n            legend_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.legend() to customize the legend.\n\n        Returns:\n            (fig, ax): Matplotlib figure and axes to allow further customization.\n        \"\"\"\n\n        grouped_ts = defaultdict(list)\n\n        for ts in self.timeseries:\n            if ts:\n                grouped_ts[ts.variable].append(ts)\n\n        num_variables = len(grouped_ts)\n\n        fig, axes = plt.subplots(\n            num_variables, 1, figsize=(10, 5 * num_variables), sharex=True\n        )\n\n        if num_variables == 1:\n            axes = [axes]\n\n        for ax, (variable, ts_list) in zip(axes, grouped_ts.items(), strict=False):\n            for ts in ts_list:\n                ts.plot(\n                    include_outliers=include_outliers,\n                    ax=ax,\n                    plot_kwargs=plot_kwargs,\n                    legend_kwargs=legend_kwargs,\n                )\n\n            ax.set_title(f\"Timeseries for {variable.capitalize()}\")\n            ax.set_xlabel(\"Time\")\n\n        fig.tight_layout()\n        return fig, axes\n</code></pre>"},{"location":"modules/#gensor.Dataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Retrieve a Timeseries object by its index in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the Timeseries to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Timeseries</code> <code>T | None</code> <p>The Timeseries object at the specified index.</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If the index is out of range.</p> Source code in <code>gensor/core/dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; T | None:\n    \"\"\"Retrieve a Timeseries object by its index in the dataset.\n\n    Parameters:\n        index (int): The index of the Timeseries to retrieve.\n\n    Returns:\n        Timeseries: The Timeseries object at the specified index.\n\n    Raises:\n        IndexError: If the index is out of range.\n    \"\"\"\n    try:\n        return self.timeseries[index].model_copy(deep=True)\n    except IndexError:\n        raise IndexOutOfRangeError(index, len(self)) from None\n</code></pre>"},{"location":"modules/#gensor.Dataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Allows to iterate directly over the dataset.</p> Source code in <code>gensor/core/dataset.py</code> <pre><code>def __iter__(self) -&gt; Any:\n    \"\"\"Allows to iterate directly over the dataset.\"\"\"\n    return iter(self.timeseries)\n</code></pre>"},{"location":"modules/#gensor.Dataset.__len__","title":"<code>__len__()</code>","text":"<p>Gives the number of timeseries in the Dataset.</p> Source code in <code>gensor/core/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Gives the number of timeseries in the Dataset.\"\"\"\n    return len(self.timeseries)\n</code></pre>"},{"location":"modules/#gensor.Dataset.add","title":"<code>add(other)</code>","text":"<p>Appends new Timeseries to the Dataset.</p> <p>If an equal Timeseries already exists, merge the new data into the existing Timeseries, dropping duplicate timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Timeseries</code> <p>The Timeseries object to add.</p> required Source code in <code>gensor/core/dataset.py</code> <pre><code>def add(self, other: T | list[T] | Dataset) -&gt; Dataset:\n    \"\"\"Appends new Timeseries to the Dataset.\n\n    If an equal Timeseries already exists, merge the new data into the existing\n    Timeseries, dropping duplicate timestamps.\n\n    Parameters:\n        other (Timeseries): The Timeseries object to add.\n    \"\"\"\n\n    # I need to check for BaseTimeseries instance in the add() method, but also\n    # type hint VarType T.\n    if isinstance(other, list | Dataset):\n        for ts in other:\n            if isinstance(ts, BaseTimeseries):\n                self._add_single_timeseries(ts)  # type: ignore[arg-type]\n\n    elif isinstance(other, BaseTimeseries):\n        self._add_single_timeseries(other)\n\n    return self\n</code></pre>"},{"location":"modules/#gensor.Dataset.filter","title":"<code>filter(location=None, variable=None, unit=None, **kwargs)</code>","text":"<p>Return a Timeseries or a new Dataset filtered by station, sensor, and/or variable.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>Optional[str]</code> <p>The location name.</p> <code>None</code> <code>variable</code> <code>Optional[str]</code> <p>The variable being measured.</p> <code>None</code> <code>unit</code> <code>Optional[str]</code> <p>Unit of the measurement.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Attributes of subclassed timeseries used for filtering (e.g., sensor, method).</p> <code>{}</code> <p>Returns:</p> Type Description <code>T | Dataset</code> <p>Timeseries | Dataset: A single Timeseries if exactly one match is found,                    or a new Dataset if multiple matches are found.</p> Source code in <code>gensor/core/dataset.py</code> <pre><code>def filter(\n    self,\n    location: str | list | None = None,\n    variable: str | list | None = None,\n    unit: str | list | None = None,\n    **kwargs: dict[str, str | list],\n) -&gt; T | Dataset:\n    \"\"\"Return a Timeseries or a new Dataset filtered by station, sensor,\n    and/or variable.\n\n    Parameters:\n        location (Optional[str]): The location name.\n        variable (Optional[str]): The variable being measured.\n        unit (Optional[str]): Unit of the measurement.\n        **kwargs (dict): Attributes of subclassed timeseries used for filtering\n            (e.g., sensor, method).\n\n    Returns:\n        Timeseries | Dataset: A single Timeseries if exactly one match is found,\n                               or a new Dataset if multiple matches are found.\n    \"\"\"\n\n    def matches(ts: T, attr: str, value: dict[str, str | list]) -&gt; bool | None:\n        \"\"\"Check if the Timeseries object has the attribute and if it matches the value.\"\"\"\n        if not hasattr(ts, attr):\n            message = f\"'{ts.__class__.__name__}' object has no attribute '{attr}'\"\n            raise AttributeError(message)\n        return getattr(ts, attr) in value\n\n    if isinstance(location, str):\n        location = [location]\n    if isinstance(variable, str):\n        variable = [variable]\n    if isinstance(unit, str):\n        unit = [unit]\n    for key, value in kwargs.items():\n        if isinstance(value, str):\n            kwargs[key] = [value]\n\n    matching_timeseries = [\n        ts\n        for ts in self.timeseries\n        if ts is not None\n        and (location is None or ts.location in location)\n        and (variable is None or ts.variable in variable)\n        and (unit is None or ts.unit in unit)\n        and all(matches(ts, attr, value) for attr, value in kwargs.items())\n    ]\n\n    if not matching_timeseries:\n        return Dataset()\n\n    if len(matching_timeseries) == 1:\n        return matching_timeseries[0].model_copy(deep=True)\n\n    return self.model_copy(update={\"timeseries\": matching_timeseries})\n</code></pre>"},{"location":"modules/#gensor.Dataset.get_locations","title":"<code>get_locations()</code>","text":"<p>List all unique locations in the dataset.</p> Source code in <code>gensor/core/dataset.py</code> <pre><code>def get_locations(self) -&gt; list:\n    \"\"\"List all unique locations in the dataset.\"\"\"\n    return [ts.location for ts in self.timeseries if ts is not None]\n</code></pre>"},{"location":"modules/#gensor.Dataset.plot","title":"<code>plot(include_outliers=False, plot_kwargs=None, legend_kwargs=None)</code>","text":"<p>Plots the timeseries data, grouping by variable type.</p> <p>Parameters:</p> Name Type Description Default <code>include_outliers</code> <code>bool</code> <p>Whether to include outliers in the plot.</p> <code>False</code> <code>plot_kwargs</code> <code>dict[str, Any] | None</code> <p>kwargs passed to matplotlib.axes.Axes.plot() method to customize the plot.</p> <code>None</code> <code>legend_kwargs</code> <code>dict[str, Any] | None</code> <p>kwargs passed to matplotlib.axes.Axes.legend() to customize the legend.</p> <code>None</code> <p>Returns:</p> Type Description <code>(fig, ax)</code> <p>Matplotlib figure and axes to allow further customization.</p> Source code in <code>gensor/core/dataset.py</code> <pre><code>def plot(\n    self,\n    include_outliers: bool = False,\n    plot_kwargs: dict[str, Any] | None = None,\n    legend_kwargs: dict[str, Any] | None = None,\n) -&gt; tuple[Figure, Axes]:\n    \"\"\"Plots the timeseries data, grouping by variable type.\n\n    Parameters:\n        include_outliers (bool): Whether to include outliers in the plot.\n        plot_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.plot() method to customize the plot.\n        legend_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.legend() to customize the legend.\n\n    Returns:\n        (fig, ax): Matplotlib figure and axes to allow further customization.\n    \"\"\"\n\n    grouped_ts = defaultdict(list)\n\n    for ts in self.timeseries:\n        if ts:\n            grouped_ts[ts.variable].append(ts)\n\n    num_variables = len(grouped_ts)\n\n    fig, axes = plt.subplots(\n        num_variables, 1, figsize=(10, 5 * num_variables), sharex=True\n    )\n\n    if num_variables == 1:\n        axes = [axes]\n\n    for ax, (variable, ts_list) in zip(axes, grouped_ts.items(), strict=False):\n        for ts in ts_list:\n            ts.plot(\n                include_outliers=include_outliers,\n                ax=ax,\n                plot_kwargs=plot_kwargs,\n                legend_kwargs=legend_kwargs,\n            )\n\n        ax.set_title(f\"Timeseries for {variable.capitalize()}\")\n        ax.set_xlabel(\"Time\")\n\n    fig.tight_layout()\n    return fig, axes\n</code></pre>"},{"location":"modules/#gensor.Dataset.to_sql","title":"<code>to_sql(db)</code>","text":"<p>Save the entire timeseries to a SQLite database.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>DatabaseConnection</code> <p>SQLite database connection object.</p> required Source code in <code>gensor/core/dataset.py</code> <pre><code>def to_sql(self, db: DatabaseConnection) -&gt; None:\n    \"\"\"Save the entire timeseries to a SQLite database.\n\n    Parameters:\n        db (DatabaseConnection): SQLite database connection object.\n    \"\"\"\n    for ts in self.timeseries:\n        if ts:\n            ts.to_sql(db)\n    return\n</code></pre>"},{"location":"modules/#gensor.Timeseries","title":"<code>Timeseries</code>","text":"<p>               Bases: <code>BaseTimeseries</code></p> <p>Timeseries of groundwater sensor data.</p> <p>Attributes:</p> Name Type Description <code>ts</code> <code>Series</code> <p>The timeseries data.</p> <code>variable</code> <code>Literal['temperature', 'pressure', 'conductivity', 'flux']</code> <p>The type of the measurement.</p> <code>unit</code> <code>Literal['degC', 'mmH2O', 'mS/cm', 'm/s']</code> <p>The unit of the measurement.</p> <code>sensor</code> <code>str</code> <p>The serial number of the sensor.</p> <code>sensor_alt</code> <code>float</code> <p>Altitude of the sensor (ncessary to compute groundwater levels).</p> Source code in <code>gensor/core/timeseries.py</code> <pre><code>class Timeseries(BaseTimeseries):\n    \"\"\"Timeseries of groundwater sensor data.\n\n    Attributes:\n        ts (pd.Series): The timeseries data.\n        variable (Literal['temperature', 'pressure', 'conductivity', 'flux']):\n            The type of the measurement.\n        unit (Literal['degC', 'mmH2O', 'mS/cm', 'm/s']): The unit of\n            the measurement.\n        sensor (str): The serial number of the sensor.\n        sensor_alt (float): Altitude of the sensor (ncessary to compute groundwater levels).\n    \"\"\"\n\n    model_config = pyd.ConfigDict(\n        arbitrary_types_allowed=True, validate_assignment=True\n    )\n\n    sensor: str | None = None\n    sensor_alt: float | None = None\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Check equality based on location, sensor, variable, unit and sensor_alt.\"\"\"\n        if not isinstance(other, Timeseries):\n            return NotImplemented\n\n        if not super().__eq__(other):\n            return False\n\n        return self.sensor == other.sensor and self.sensor_alt == other.sensor_alt\n\n    def plot(\n        self,\n        include_outliers: bool = False,\n        ax: Axes | None = None,\n        plot_kwargs: dict[str, Any] | None = None,\n        legend_kwargs: dict[str, Any] | None = None,\n    ) -&gt; tuple[Figure, Axes]:\n        \"\"\"Plots the timeseries data.\n\n        Parameters:\n            include_outliers (bool): Whether to include outliers in the plot.\n            ax (matplotlib.axes.Axes, optional): Matplotlib axes object to plot on.\n                If None, a new figure and axes are created.\n            plot_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.plot() method to customize the plot.\n            legend_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.legend() to customize the legend.\n\n        Returns:\n            (fig, ax): Matplotlib figure and axes to allow further customization.\n        \"\"\"\n        fig, ax = super().plot(\n            include_outliers=include_outliers,\n            ax=ax,\n            plot_kwargs=plot_kwargs,\n            legend_kwargs=legend_kwargs,\n        )\n\n        ax.set_title(f\"{self.variable.capitalize()} at {self.location} ({self.sensor})\")\n\n        return fig, ax\n</code></pre>"},{"location":"modules/#gensor.Timeseries.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check equality based on location, sensor, variable, unit and sensor_alt.</p> Source code in <code>gensor/core/timeseries.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"Check equality based on location, sensor, variable, unit and sensor_alt.\"\"\"\n    if not isinstance(other, Timeseries):\n        return NotImplemented\n\n    if not super().__eq__(other):\n        return False\n\n    return self.sensor == other.sensor and self.sensor_alt == other.sensor_alt\n</code></pre>"},{"location":"modules/#gensor.Timeseries.plot","title":"<code>plot(include_outliers=False, ax=None, plot_kwargs=None, legend_kwargs=None)</code>","text":"<p>Plots the timeseries data.</p> <p>Parameters:</p> Name Type Description Default <code>include_outliers</code> <code>bool</code> <p>Whether to include outliers in the plot.</p> <code>False</code> <code>ax</code> <code>Axes</code> <p>Matplotlib axes object to plot on. If None, a new figure and axes are created.</p> <code>None</code> <code>plot_kwargs</code> <code>dict[str, Any] | None</code> <p>kwargs passed to matplotlib.axes.Axes.plot() method to customize the plot.</p> <code>None</code> <code>legend_kwargs</code> <code>dict[str, Any] | None</code> <p>kwargs passed to matplotlib.axes.Axes.legend() to customize the legend.</p> <code>None</code> <p>Returns:</p> Type Description <code>(fig, ax)</code> <p>Matplotlib figure and axes to allow further customization.</p> Source code in <code>gensor/core/timeseries.py</code> <pre><code>def plot(\n    self,\n    include_outliers: bool = False,\n    ax: Axes | None = None,\n    plot_kwargs: dict[str, Any] | None = None,\n    legend_kwargs: dict[str, Any] | None = None,\n) -&gt; tuple[Figure, Axes]:\n    \"\"\"Plots the timeseries data.\n\n    Parameters:\n        include_outliers (bool): Whether to include outliers in the plot.\n        ax (matplotlib.axes.Axes, optional): Matplotlib axes object to plot on.\n            If None, a new figure and axes are created.\n        plot_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.plot() method to customize the plot.\n        legend_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.legend() to customize the legend.\n\n    Returns:\n        (fig, ax): Matplotlib figure and axes to allow further customization.\n    \"\"\"\n    fig, ax = super().plot(\n        include_outliers=include_outliers,\n        ax=ax,\n        plot_kwargs=plot_kwargs,\n        legend_kwargs=legend_kwargs,\n    )\n\n    ax.set_title(f\"{self.variable.capitalize()} at {self.location} ({self.sensor})\")\n\n    return fig, ax\n</code></pre>"},{"location":"modules/#gensor.compensate","title":"<code>compensate(raw, barometric, alignment_period='h', threshold_wc=None, fieldwork_dates=None, interpolate_method=None)</code>","text":"<p>Constructor for the Comensator object.</p> <p>Parameters:</p> Name Type Description Default <code>raw</code> <code>Timeseries | Dataset</code> <p>Raw sensor timeseries</p> required <code>barometric</code> <code>Timeseries | float</code> <p>Barometric pressure timeseries or a single float value. If a float value is provided, it is assumed to be in cmH2O.</p> required <code>alignment_period</code> <code>Literal['D', 'ME', 'SME', 'MS', 'YE', 'YS', 'h', 'min', 's']</code> <p>The alignment period for the timeseries. Default is 'h'. See pandas offset aliases for definitinos.</p> <code>'h'</code> <code>threshold_wc</code> <code>float</code> <p>The threshold for the absolute water column. If it is provided, the records below that threshold are dropped.</p> <code>None</code> <code>fieldwork_dates</code> <code>Dict[str, list]</code> <p>Dictionary of location name and a list of fieldwork days. All records on the fieldwork day are set to None.</p> <code>None</code> <code>interpolate_method</code> <code>str</code> <p>String representing the interpolate method as in pd.Series.interpolate() method.</p> <code>None</code> Source code in <code>gensor/processing/compensation.py</code> <pre><code>def compensate(\n    raw: Timeseries | Dataset,\n    barometric: Timeseries | float,\n    alignment_period: Literal[\n        \"D\", \"ME\", \"SME\", \"MS\", \"YE\", \"YS\", \"h\", \"min\", \"s\"\n    ] = \"h\",\n    threshold_wc: float | None = None,\n    fieldwork_dates: dict | None = None,\n    interpolate_method: str | None = None,\n) -&gt; Timeseries | Dataset | None:\n    \"\"\"Constructor for the Comensator object.\n\n    Parameters:\n        raw (Timeseries | Dataset): Raw sensor timeseries\n        barometric (Timeseries | float): Barometric pressure timeseries or a single\n            float value. If a float value is provided, it is assumed to be in cmH2O.\n        alignment_period (Literal['D', 'ME', 'SME', 'MS', 'YE', 'YS', 'h', 'min', 's']): The alignment period for the timeseries.\n            Default is 'h'. See pandas offset aliases for definitinos.\n        threshold_wc (float): The threshold for the absolute water column. If it is\n            provided, the records below that threshold are dropped.\n        fieldwork_dates (Dict[str, list]): Dictionary of location name and a list of\n            fieldwork days. All records on the fieldwork day are set to None.\n        interpolate_method (str): String representing the interpolate method as in\n            pd.Series.interpolate() method.\n    \"\"\"\n    if fieldwork_dates is None:\n        fieldwork_dates = {}\n\n    def _compensate_one(\n        raw: Timeseries, fieldwork_dates: list | None\n    ) -&gt; Timeseries | None:\n        comp = Compensator(ts=raw, barometric=barometric)\n        compensated = comp.compensate(\n            alignment_period=alignment_period,\n            threshold_wc=threshold_wc,\n            fieldwork_dates=fieldwork_dates,\n        )\n        if compensated is not None and interpolate_method:\n            # .interpolate() called on Timeseries object is wrapped to return a\n            # Timeseries object from the original pandas.Series.interpolate().\n            return compensated.interpolate(method=interpolate_method)  # type: ignore[no-any-return]\n\n        else:\n            return compensated\n\n    if isinstance(raw, Timeseries):\n        dates = fieldwork_dates.get(raw.location)\n        return _compensate_one(raw, dates)\n\n    elif isinstance(raw, Dataset):\n        compensated_series = []\n        for item in raw:\n            dates = fieldwork_dates.get(item.location)\n            compensated_series.append(_compensate_one(item, dates))\n\n        return raw.model_copy(update={\"timeseries\": compensated_series}, deep=True)\n</code></pre>"},{"location":"modules/#gensor.read_from_csv","title":"<code>read_from_csv(path, file_format='vanessen', **kwargs)</code>","text":"<p>Loads the data from csv files with given file_format and returns a list of Timeseries objects.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the file or directory containing the files.</p> required <code>**kwargs</code> <code>dict</code> <p>Optional keyword arguments passed to the parsers: * serial_number_pattern (str): The regex pattern to extract the serial number from the file. * location_pattern (str): The regex pattern to extract the station from the file. * col_names (list): The column names for the dataframe. * location (str): Name of the location of the timeseries. * sensor (str): Sensor serial number.</p> <code>{}</code> Source code in <code>gensor/io/read.py</code> <pre><code>def read_from_csv(\n    path: Path, file_format: Literal[\"vanessen\", \"plain\"] = \"vanessen\", **kwargs: Any\n) -&gt; Dataset | Timeseries:\n    \"\"\"Loads the data from csv files with given file_format and returns a list of Timeseries objects.\n\n    Parameters:\n        path (Path): The path to the file or directory containing the files.\n        **kwargs (dict): Optional keyword arguments passed to the parsers:\n            * serial_number_pattern (str): The regex pattern to extract the serial number from the file.\n            * location_pattern (str): The regex pattern to extract the station from the file.\n            * col_names (list): The column names for the dataframe.\n            * location (str): Name of the location of the timeseries.\n            * sensor (str): Sensor serial number.\n    \"\"\"\n\n    parsers = {\n        \"vanessen\": parse_vanessen_csv,\n        \"plain\": parse_plain,\n        # more parser to be implemented\n    }\n\n    if not isinstance(path, Path):\n        message = \"The path argument must be a Path object.\"\n        raise TypeError(message)\n\n    if path.is_dir() and not any(\n        file.is_file() and file.suffix.lower() == \".csv\" for file in path.iterdir()\n    ):\n        logger.info(\"No CSV files found. Operation skipped.\")\n        return Dataset()\n\n    files = (\n        [\n            file\n            for file in path.iterdir()\n            if file.is_file() and file.suffix.lower() == \".csv\"\n        ]\n        if path.is_dir()\n        else [path]\n        if path.suffix.lower() == \".csv\"\n        else []\n    )\n\n    if not files:\n        logger.info(\"No CSV files found. Operation skipped.\")\n        return Dataset()\n\n    parser = parsers[file_format]\n\n    ds: Dataset = Dataset()\n\n    for f in files:\n        logger.info(f\"Loading file: {f}\")\n        ts_in_file = parser(f, **kwargs)\n        ds.add(ts_in_file)\n\n    # If there is only one Timeseries in Dataset (as in the condition), ds[0] will always\n    # be a Timeseries; so the line below does not introduce potential None in the return\n    return ds[0] if len(ds) == 1 else ds  # type: ignore[return-value]\n</code></pre>"},{"location":"modules/#gensor.read_from_sql","title":"<code>read_from_sql(db, load_all=True, location=None, variable=None, unit=None, timestamp_start=None, timestamp_stop=None, **kwargs)</code>","text":"<p>Returns the timeseries or a dataset from a SQL database.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>DatabaseConnection</code> <p>The database connection object.</p> required <code>load_all</code> <code>bool</code> <p>Whether to load all timeseries from the database.</p> <code>True</code> <code>location</code> <code>str</code> <p>The station name.</p> <code>None</code> <code>variable</code> <code>str</code> <p>The measurement type.</p> <code>None</code> <code>unit</code> <code>str</code> <p>The unit of the measurement.</p> <code>None</code> <code>timestamp_start</code> <code>Timestamp</code> <p>Start timestamp filter.</p> <code>None</code> <code>timestamp_stop</code> <code>Timestamp</code> <p>End timestamp filter.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Any additional filters matching attributes of the particular timeseries.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Timeseries | Dataset</code> <p>Dataset with retrieved objects or an empty Dataset.</p> Source code in <code>gensor/io/read.py</code> <pre><code>def read_from_sql(\n    db: DatabaseConnection,\n    load_all: bool = True,\n    location: str | None = None,\n    variable: str | None = None,\n    unit: str | None = None,\n    timestamp_start: pd.Timestamp | None = None,\n    timestamp_stop: pd.Timestamp | None = None,\n    **kwargs: dict,\n) -&gt; Timeseries | Dataset:\n    \"\"\"Returns the timeseries or a dataset from a SQL database.\n\n    Parameters:\n        db (DatabaseConnection): The database connection object.\n        load_all (bool): Whether to load all timeseries from the database.\n        location (str): The station name.\n        variable (str): The measurement type.\n        unit (str): The unit of the measurement.\n        timestamp_start (pd.Timestamp, optional): Start timestamp filter.\n        timestamp_stop (pd.Timestamp, optional): End timestamp filter.\n        **kwargs (dict): Any additional filters matching attributes of the particular\n            timeseries.\n\n    Returns:\n        Dataset: Dataset with retrieved objects or an empty Dataset.\n    \"\"\"\n\n    def _read_data_from_schema(schema_name: str) -&gt; Any:\n        \"\"\"Read data from the table and apply the timestamp filter.\n\n        Parameters:\n            schema_name (str): name of the schema in SQLite database.\n\n        Returns:\n            pd.Series: results of the query or an empty pd.Series if none are found.\n        \"\"\"\n        with db as con:\n            schema = db.metadata.tables[schema_name]\n            data_query = select(schema)\n\n            if timestamp_start or timestamp_stop:\n                if timestamp_start:\n                    data_query = data_query.where(schema.c.timestamp &gt;= timestamp_start)\n                if timestamp_stop:\n                    data_query = data_query.where(schema.c.timestamp &lt;= timestamp_stop)\n\n            ts = pd.read_sql(\n                data_query,\n                con=con,\n                parse_dates={\"timestamp\": \"%Y-%m-%dT%H:%M:%S%z\"},\n                index_col=\"timestamp\",\n            ).squeeze()\n\n        if ts.empty:\n            message = f\"No data found in table {schema_name}\"\n            logger.warning(message)\n\n        return ts\n\n    def _create_object(data: pd.Series, metadata: dict) -&gt; Any:\n        \"\"\"Create the appropriate object for timeseries.\"\"\"\n\n        core_metadata = {\n            \"location\": metadata[\"location\"],\n            \"variable\": metadata[\"variable\"],\n            \"unit\": metadata[\"unit\"],\n        }\n\n        extra_metadata = metadata.get(\"extra\", {})\n\n        ts_metadata = {**core_metadata, **extra_metadata}\n\n        cls = metadata[\"cls\"]\n        module_name, class_name = cls.rsplit(\".\", 1)\n        module = import_module(module_name)\n\n        TimeseriesClass = getattr(module, class_name)\n        ts_object = TimeseriesClass(ts=data, **ts_metadata)\n\n        return ts_object\n\n    metadata_df = (\n        db.get_timeseries_metadata(\n            location=location, variable=variable, unit=unit, **kwargs\n        )\n        if not load_all\n        else db.get_timeseries_metadata()\n    )\n\n    if metadata_df.empty:\n        message = \"No schemas matched the specified filters.\"\n        raise ValueError(message)\n\n    timeseries_list = []\n\n    for row in metadata_df.to_dict(orient=\"records\"):\n        try:\n            schema_name = row.pop(\"table_name\")\n            data = _read_data_from_schema(schema_name)\n            timeseries_obj = _create_object(data, row)\n            timeseries_list.append(timeseries_obj)\n        except (ValueError, TypeError):\n            logger.exception(f\"Skipping schema {schema_name} due to error.\")\n\n    return Dataset(timeseries=timeseries_list) if timeseries_list else Dataset()\n</code></pre>"},{"location":"modules/#gensor.set_log_level","title":"<code>set_log_level(level)</code>","text":"<p>Set the logging level for the package.</p> Source code in <code>gensor/log.py</code> <pre><code>def set_log_level(level: str) -&gt; None:\n    \"\"\"Set the logging level for the package.\"\"\"\n    logger = logging.getLogger(\"gensor\")\n    logger.setLevel(level.upper())\n</code></pre>"},{"location":"modules/#gensor.analysis","title":"<code>analysis</code>","text":""},{"location":"modules/#gensor.analysis.outliers","title":"<code>outliers</code>","text":""},{"location":"modules/#gensor.analysis.outliers.OutlierDetection","title":"<code>OutlierDetection</code>","text":"<p>Detecting outliers in groundwater timeseries data.</p> <p>Each method in this class returns a pandas.Series containing predicted outliers in the dataset.</p> <p>Methods:</p> Name Description <code>iqr</code> <p>Use interquartile range (IQR).</p> <code>zscore</code> <p>Use the z-score method.</p> <code>isolation_forest</code> <p>Using the isolation forest algorithm.</p> <code>lof</code> <p>Using the local outlier factor (LOF) method.</p> Source code in <code>gensor/analysis/outliers.py</code> <pre><code>class OutlierDetection:\n    \"\"\"Detecting outliers in groundwater timeseries data.\n\n    Each method in this class returns a pandas.Series containing predicted outliers in\n    the dataset.\n\n    Methods:\n        iqr: Use interquartile range (IQR).\n        zscore: Use the z-score method.\n        isolation_forest: Using the isolation forest algorithm.\n        lof: Using the local outlier factor (LOF) method.\n    \"\"\"\n\n    def __init__(\n        self,\n        data: Series,\n        method: Literal[\"iqr\", \"zscore\", \"isolation_forest\", \"lof\"],\n        rolling: bool,\n        window: int,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Find outliers in a time series using the specified method, with an option for rolling window.\"\"\"\n\n        FUNCS: dict[str, Callable] = {\n            \"iqr\": self.iqr,\n            \"zscore\": self.zscore,\n            \"isolation_forest\": self.isolation_forest,\n            \"lof\": self.lof,\n        }\n\n        method_func = FUNCS[method]\n\n        if method in [\"iqr\", \"zscore\"]:\n            # For 'iqr' and 'zscore' methods\n            y = (\n                kwargs.get(\"k\", 1.5)\n                if method == \"iqr\"\n                else kwargs.get(\"threshold\", 3.0)\n            )\n            if rolling:\n                roll = data.rolling(window=window)\n                mask = roll.apply(lambda x: method_func(x, y, rolling=True), raw=True)\n            else:\n                mask = method_func(data.to_numpy(), y, rolling=False)\n\n            bool_mask = mask.astype(bool)\n            bool_mask_series = Series(bool_mask, index=data.index)\n            self.outliers = data[bool_mask_series]\n\n        else:\n            # For 'isolation_forest' and 'lof' methods\n            self.outliers = method_func(data, **kwargs)\n\n    @staticmethod\n    def iqr(data: np.ndarray, k: float, rolling: bool) -&gt; np.ndarray:\n        \"\"\"Use interquartile range (IQR).\n\n        Parameters:\n            data (pandas.Series): The time series data.\n\n        Keyword Args:\n            k (float): The multiplier for the IQR to define the range. Defaults to 1.5.\n\n        Returns:\n            np.ndarray: Binary mask representing the outliers as 1.\n        \"\"\"\n\n        Q1 = np.percentile(data, 0.25)\n        Q3 = np.percentile(data, 0.75)\n        IQR = Q3 - Q1\n\n        lower_bound = Q1 - k * IQR\n        upper_bound = Q3 + k * IQR\n\n        if rolling:\n            return (\n                np.array([1])\n                if (data[-1] &lt; lower_bound or data[-1] &gt; upper_bound)\n                else np.array([0])\n            )\n\n        return np.where((data &lt; lower_bound) | (data &gt; upper_bound), 1, 0)\n\n    @staticmethod\n    def zscore(data: np.ndarray, threshold: float, rolling: bool) -&gt; np.ndarray:\n        \"\"\"Use the z-score method.\n\n        Parameters:\n            data (pandas.Series): The time series data.\n\n        Keyword Args:\n            threshold (float): The threshold for the z-score method. Defaults to 3.0.\n\n        Returns:\n            pandas.Series: Binary mask representing outliers.\n        \"\"\"\n\n        mean = np.mean(data)\n        std_dev = np.std(data)\n\n        z_scores = np.abs((data - mean) / std_dev)\n\n        if rolling:\n            return np.array([1]) if z_scores[-1] &gt; threshold else np.array([0])\n        return np.where(z_scores &gt; threshold, 1, 0)\n\n    def isolation_forest(self, data: Series, **kwargs: Any) -&gt; Series:\n        \"\"\"Using the isolation forest algorithm.\n\n        Parameters:\n            data (pandas.Series): The time series data.\n\n        Keyword Args:\n            n_estimators (int): The number of base estimators in the ensemble. Defaults to 100.\n            max_samples (int | 'auto' | float): The number of samples to draw from X to train each base estimator. Defaults to 'auto'.\n            contamination (float): The proportion of outliers in the data. Defaults to 0.01.\n            max_features (int | float): The number of features to draw from X to train each base estimator. Defaults to 1.0.\n            bootstrap (bool): Whether to use bootstrapping when sampling the data. Defaults to False.\n            n_jobs (int): The number of jobs to run in parallel. Defaults to 1.\n            random_state (int | RandomState | None): The random state to use. Defaults to None.\n            verbose (int): The verbosity level. Defaults to 0.\n            warm_start (bool): Whether to reuse the solution of the previous call to fit and add more estimators to the ensemble. Defaults to False.\n\n        Note:\n            For details on kwargs see: sklearn.ensemble.IsolationForest.\n        \"\"\"\n\n        X = data.to_numpy().reshape(-1, 1)\n\n        clf = IsolationForest(**kwargs)\n        clf.fit(X)\n\n        is_outlier = clf.predict(X)\n        outliers: Series = data[is_outlier == -1]\n\n        return outliers\n\n    def lof(self, data: Series, **kwargs: Any) -&gt; Series:\n        \"\"\"Using the local outlier factor (LOF) method.\n\n        Parameters:\n            data (pandas.Series): The time series data.\n\n        Keyword Args:\n            n_neighbors (int): The number of neighbors to consider for each sample. Defaults to 20.\n            algorithm (str): The algorithm to use. Either 'auto', 'ball_tree', 'kd_tree' or 'brute'. Defaults to 'auto'.\n            leaf_size (int): The leaf size of the tree. Defaults to 30.\n            metric (str): The distance metric to use. Defaults to 'minkowski'.\n            p (int): The power parameter for the Minkowski metric. Defaults to 2.\n            contamination (float): The proportion of outliers in the data. Defaults to 0.01.\n            novelty (bool): Whether to consider the samples as normal or outliers. Defaults to False.\n            n_jobs (int): The number of jobs to run in parallel. Defaults to 1.\n        Note:\n            For details on kwargs see: sklearn.neighbors.LocalOutlierFactor.\n        \"\"\"\n\n        X = data.to_numpy().reshape(-1, 1)\n\n        clf = LocalOutlierFactor(**kwargs)\n\n        is_outlier = clf.fit_predict(X)\n        outliers: Series = data[is_outlier == -1]\n\n        return outliers\n</code></pre>"},{"location":"modules/#gensor.analysis.outliers.OutlierDetection.__init__","title":"<code>__init__(data, method, rolling, window, **kwargs)</code>","text":"<p>Find outliers in a time series using the specified method, with an option for rolling window.</p> Source code in <code>gensor/analysis/outliers.py</code> <pre><code>def __init__(\n    self,\n    data: Series,\n    method: Literal[\"iqr\", \"zscore\", \"isolation_forest\", \"lof\"],\n    rolling: bool,\n    window: int,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Find outliers in a time series using the specified method, with an option for rolling window.\"\"\"\n\n    FUNCS: dict[str, Callable] = {\n        \"iqr\": self.iqr,\n        \"zscore\": self.zscore,\n        \"isolation_forest\": self.isolation_forest,\n        \"lof\": self.lof,\n    }\n\n    method_func = FUNCS[method]\n\n    if method in [\"iqr\", \"zscore\"]:\n        # For 'iqr' and 'zscore' methods\n        y = (\n            kwargs.get(\"k\", 1.5)\n            if method == \"iqr\"\n            else kwargs.get(\"threshold\", 3.0)\n        )\n        if rolling:\n            roll = data.rolling(window=window)\n            mask = roll.apply(lambda x: method_func(x, y, rolling=True), raw=True)\n        else:\n            mask = method_func(data.to_numpy(), y, rolling=False)\n\n        bool_mask = mask.astype(bool)\n        bool_mask_series = Series(bool_mask, index=data.index)\n        self.outliers = data[bool_mask_series]\n\n    else:\n        # For 'isolation_forest' and 'lof' methods\n        self.outliers = method_func(data, **kwargs)\n</code></pre>"},{"location":"modules/#gensor.analysis.outliers.OutlierDetection.iqr","title":"<code>iqr(data, k, rolling)</code>  <code>staticmethod</code>","text":"<p>Use interquartile range (IQR).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series</code> <p>The time series data.</p> required <p>Other Parameters:</p> Name Type Description <code>k</code> <code>float</code> <p>The multiplier for the IQR to define the range. Defaults to 1.5.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Binary mask representing the outliers as 1.</p> Source code in <code>gensor/analysis/outliers.py</code> <pre><code>@staticmethod\ndef iqr(data: np.ndarray, k: float, rolling: bool) -&gt; np.ndarray:\n    \"\"\"Use interquartile range (IQR).\n\n    Parameters:\n        data (pandas.Series): The time series data.\n\n    Keyword Args:\n        k (float): The multiplier for the IQR to define the range. Defaults to 1.5.\n\n    Returns:\n        np.ndarray: Binary mask representing the outliers as 1.\n    \"\"\"\n\n    Q1 = np.percentile(data, 0.25)\n    Q3 = np.percentile(data, 0.75)\n    IQR = Q3 - Q1\n\n    lower_bound = Q1 - k * IQR\n    upper_bound = Q3 + k * IQR\n\n    if rolling:\n        return (\n            np.array([1])\n            if (data[-1] &lt; lower_bound or data[-1] &gt; upper_bound)\n            else np.array([0])\n        )\n\n    return np.where((data &lt; lower_bound) | (data &gt; upper_bound), 1, 0)\n</code></pre>"},{"location":"modules/#gensor.analysis.outliers.OutlierDetection.isolation_forest","title":"<code>isolation_forest(data, **kwargs)</code>","text":"<p>Using the isolation forest algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series</code> <p>The time series data.</p> required <p>Other Parameters:</p> Name Type Description <code>n_estimators</code> <code>int</code> <p>The number of base estimators in the ensemble. Defaults to 100.</p> <code>max_samples</code> <code>int | auto | float</code> <p>The number of samples to draw from X to train each base estimator. Defaults to 'auto'.</p> <code>contamination</code> <code>float</code> <p>The proportion of outliers in the data. Defaults to 0.01.</p> <code>max_features</code> <code>int | float</code> <p>The number of features to draw from X to train each base estimator. Defaults to 1.0.</p> <code>bootstrap</code> <code>bool</code> <p>Whether to use bootstrapping when sampling the data. Defaults to False.</p> <code>n_jobs</code> <code>int</code> <p>The number of jobs to run in parallel. Defaults to 1.</p> <code>random_state</code> <code>int | RandomState | None</code> <p>The random state to use. Defaults to None.</p> <code>verbose</code> <code>int</code> <p>The verbosity level. Defaults to 0.</p> <code>warm_start</code> <code>bool</code> <p>Whether to reuse the solution of the previous call to fit and add more estimators to the ensemble. Defaults to False.</p> Note <p>For details on kwargs see: sklearn.ensemble.IsolationForest.</p> Source code in <code>gensor/analysis/outliers.py</code> <pre><code>def isolation_forest(self, data: Series, **kwargs: Any) -&gt; Series:\n    \"\"\"Using the isolation forest algorithm.\n\n    Parameters:\n        data (pandas.Series): The time series data.\n\n    Keyword Args:\n        n_estimators (int): The number of base estimators in the ensemble. Defaults to 100.\n        max_samples (int | 'auto' | float): The number of samples to draw from X to train each base estimator. Defaults to 'auto'.\n        contamination (float): The proportion of outliers in the data. Defaults to 0.01.\n        max_features (int | float): The number of features to draw from X to train each base estimator. Defaults to 1.0.\n        bootstrap (bool): Whether to use bootstrapping when sampling the data. Defaults to False.\n        n_jobs (int): The number of jobs to run in parallel. Defaults to 1.\n        random_state (int | RandomState | None): The random state to use. Defaults to None.\n        verbose (int): The verbosity level. Defaults to 0.\n        warm_start (bool): Whether to reuse the solution of the previous call to fit and add more estimators to the ensemble. Defaults to False.\n\n    Note:\n        For details on kwargs see: sklearn.ensemble.IsolationForest.\n    \"\"\"\n\n    X = data.to_numpy().reshape(-1, 1)\n\n    clf = IsolationForest(**kwargs)\n    clf.fit(X)\n\n    is_outlier = clf.predict(X)\n    outliers: Series = data[is_outlier == -1]\n\n    return outliers\n</code></pre>"},{"location":"modules/#gensor.analysis.outliers.OutlierDetection.lof","title":"<code>lof(data, **kwargs)</code>","text":"<p>Using the local outlier factor (LOF) method.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series</code> <p>The time series data.</p> required <p>Other Parameters:</p> Name Type Description <code>n_neighbors</code> <code>int</code> <p>The number of neighbors to consider for each sample. Defaults to 20.</p> <code>algorithm</code> <code>str</code> <p>The algorithm to use. Either 'auto', 'ball_tree', 'kd_tree' or 'brute'. Defaults to 'auto'.</p> <code>leaf_size</code> <code>int</code> <p>The leaf size of the tree. Defaults to 30.</p> <code>metric</code> <code>str</code> <p>The distance metric to use. Defaults to 'minkowski'.</p> <code>p</code> <code>int</code> <p>The power parameter for the Minkowski metric. Defaults to 2.</p> <code>contamination</code> <code>float</code> <p>The proportion of outliers in the data. Defaults to 0.01.</p> <code>novelty</code> <code>bool</code> <p>Whether to consider the samples as normal or outliers. Defaults to False.</p> <code>n_jobs</code> <code>int</code> <p>The number of jobs to run in parallel. Defaults to 1.</p> <p>Note:     For details on kwargs see: sklearn.neighbors.LocalOutlierFactor.</p> Source code in <code>gensor/analysis/outliers.py</code> <pre><code>def lof(self, data: Series, **kwargs: Any) -&gt; Series:\n    \"\"\"Using the local outlier factor (LOF) method.\n\n    Parameters:\n        data (pandas.Series): The time series data.\n\n    Keyword Args:\n        n_neighbors (int): The number of neighbors to consider for each sample. Defaults to 20.\n        algorithm (str): The algorithm to use. Either 'auto', 'ball_tree', 'kd_tree' or 'brute'. Defaults to 'auto'.\n        leaf_size (int): The leaf size of the tree. Defaults to 30.\n        metric (str): The distance metric to use. Defaults to 'minkowski'.\n        p (int): The power parameter for the Minkowski metric. Defaults to 2.\n        contamination (float): The proportion of outliers in the data. Defaults to 0.01.\n        novelty (bool): Whether to consider the samples as normal or outliers. Defaults to False.\n        n_jobs (int): The number of jobs to run in parallel. Defaults to 1.\n    Note:\n        For details on kwargs see: sklearn.neighbors.LocalOutlierFactor.\n    \"\"\"\n\n    X = data.to_numpy().reshape(-1, 1)\n\n    clf = LocalOutlierFactor(**kwargs)\n\n    is_outlier = clf.fit_predict(X)\n    outliers: Series = data[is_outlier == -1]\n\n    return outliers\n</code></pre>"},{"location":"modules/#gensor.analysis.outliers.OutlierDetection.zscore","title":"<code>zscore(data, threshold, rolling)</code>  <code>staticmethod</code>","text":"<p>Use the z-score method.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series</code> <p>The time series data.</p> required <p>Other Parameters:</p> Name Type Description <code>threshold</code> <code>float</code> <p>The threshold for the z-score method. Defaults to 3.0.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>pandas.Series: Binary mask representing outliers.</p> Source code in <code>gensor/analysis/outliers.py</code> <pre><code>@staticmethod\ndef zscore(data: np.ndarray, threshold: float, rolling: bool) -&gt; np.ndarray:\n    \"\"\"Use the z-score method.\n\n    Parameters:\n        data (pandas.Series): The time series data.\n\n    Keyword Args:\n        threshold (float): The threshold for the z-score method. Defaults to 3.0.\n\n    Returns:\n        pandas.Series: Binary mask representing outliers.\n    \"\"\"\n\n    mean = np.mean(data)\n    std_dev = np.std(data)\n\n    z_scores = np.abs((data - mean) / std_dev)\n\n    if rolling:\n        return np.array([1]) if z_scores[-1] &gt; threshold else np.array([0])\n    return np.where(z_scores &gt; threshold, 1, 0)\n</code></pre>"},{"location":"modules/#gensor.analysis.stats","title":"<code>stats</code>","text":"<p>Module to compute timeseries statistics, similar to pastas.stats.signatures module and following Heudorfer et al. 2019</p> <p>To be implemented:</p> <ul> <li>Structure</li> <li>Flashiness</li> <li>Distribution</li> <li>Modality</li> <li>Density</li> <li>Shape</li> <li>Scale</li> <li>Slope</li> </ul>"},{"location":"modules/#gensor.config","title":"<code>config</code>","text":"<p>Warning</p> <p>Whenever Timeseries objects are created via read_from_csv and use a parser (e.g., 'vanessen'), the timestamps are localized and converted to UTC. Therefore, if the user creates his own timeseries outside the read_from_csv, they should ensure that the timestamps are in UTC format.</p>"},{"location":"modules/#gensor.core","title":"<code>core</code>","text":""},{"location":"modules/#gensor.core.base","title":"<code>base</code>","text":""},{"location":"modules/#gensor.core.base.BaseTimeseries","title":"<code>BaseTimeseries</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Generic base class for timeseries with metadata.</p> <p>Timeseries is a series of measurements of a single variable, in the same unit, from a single location with unique timestamps.</p> <p>Attributes:</p> Name Type Description <code>ts</code> <code>Series</code> <p>The timeseries data.</p> <code>variable</code> <code>Literal['temperature', 'pressure', 'conductivity', 'flux']</code> <p>The type of the measurement.</p> <code>unit</code> <code>Literal['degC', 'mmH2O', 'mS/cm', 'm/s']</code> <p>The unit of the measurement.</p> <code>outliers</code> <code>Series</code> <p>Measurements marked as outliers.</p> <code>transformation</code> <code>Any</code> <p>Metadata of transformation the timeseries undergone.</p> <p>Methods:</p> Name Description <code>validate_ts</code> <p>if the pd.Series is not exactly what is required, coerce.</p> Source code in <code>gensor/core/base.py</code> <pre><code>class BaseTimeseries(pyd.BaseModel):\n    \"\"\"Generic base class for timeseries with metadata.\n\n    Timeseries is a series of measurements of a single variable, in the same unit, from a\n    single location with unique timestamps.\n\n    Attributes:\n        ts (pd.Series): The timeseries data.\n        variable (Literal['temperature', 'pressure', 'conductivity', 'flux']):\n            The type of the measurement.\n        unit (Literal['degC', 'mmH2O', 'mS/cm', 'm/s']): The unit of\n            the measurement.\n        outliers (pd.Series): Measurements marked as outliers.\n        transformation (Any): Metadata of transformation the timeseries undergone.\n\n    Methods:\n        validate_ts: if the pd.Series is not exactly what is required, coerce.\n    \"\"\"\n\n    model_config = pyd.ConfigDict(\n        arbitrary_types_allowed=True, validate_assignment=True\n    )\n\n    ts: pd.Series = pyd.Field(repr=False, exclude=True)\n    variable: Literal[\n        \"temperature\", \"pressure\", \"conductivity\", \"flux\", \"head\", \"depth\"\n    ]\n    unit: Literal[\"degc\", \"cmh2o\", \"ms/cm\", \"m/s\", \"m asl\", \"m\"]\n    location: str | None = None\n    outliers: pd.Series | None = pyd.Field(default=None, repr=False, exclude=True)\n    transformation: Any = pyd.Field(default=None, repr=False, exclude=True)\n\n    @pyd.computed_field()  # type: ignore[prop-decorator]\n    @property\n    def start(self) -&gt; pd.Timestamp | Any:\n        return self.ts.index.min()\n\n    @pyd.computed_field()  # type: ignore[prop-decorator]\n    @property\n    def end(self) -&gt; pd.Timestamp | Any:\n        return self.ts.index.max()\n\n    @pyd.field_serializer(\"start\", \"end\")\n    def serialize_timestamps(self, value: pd.Timestamp | None) -&gt; str | None:\n        \"\"\"Serialize `pd.Timestamp` to ISO format.\"\"\"\n        return value.strftime(\"%Y%m%d%H%M%S\") if value is not None else None\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Check equality based on location, sensor, variable, unit and sensor_alt.\"\"\"\n        if not isinstance(other, BaseTimeseries):\n            return NotImplemented\n\n        return (\n            self.variable == other.variable\n            and self.unit == other.unit\n            and self.location == other.location\n        )\n\n    def __getattr__(self, attr: Any) -&gt; Any:\n        \"\"\"Delegate attribute access to the underlying pandas Series if it exists.\n\n        Special handling is implemented for pandas indexer.\n        \"\"\"\n        if attr == \"loc\":\n            return TimeseriesIndexer(self, self.ts.loc)\n\n        if attr == \"iloc\":\n            return TimeseriesIndexer(self, self.ts.iloc)\n\n        error_message = f\"'{self.__class__.__name__}' object has no attribute '{attr}'\"\n\n        if hasattr(self.ts, attr):\n            # Return a function to call on the `ts` if it's a method, otherwise return the attribute\n            ts_attr = getattr(self.ts, attr)\n            if callable(ts_attr):\n\n                def wrapper(*args: Any, **kwargs: Any) -&gt; Any:\n                    result = ts_attr(*args, **kwargs)\n                    # If the result is a Series, return a new Timeseries; otherwise, return the result\n                    if isinstance(result, pd.Series):\n                        return self.model_copy(update={\"ts\": result}, deep=True)\n\n                    return result\n\n                return wrapper\n            else:\n                return ts_attr\n        raise AttributeError(error_message)\n\n    @pyd.field_validator(\"ts\")\n    def validate_ts(cls, v: pd.Series) -&gt; pd.Series:\n        validated_ts = ts_schema.validate(v)\n\n        return validated_ts\n\n    @pyd.field_validator(\"outliers\")\n    def validate_outliers(cls, v: pd.Series) -&gt; pd.Series:\n        if v is not None:\n            return ts_schema.validate(v)\n        return v\n\n    def concatenate(self: T, other: T) -&gt; T:\n        \"\"\"Concatenate two Timeseries objects if they are considered equal.\"\"\"\n        if not isinstance(other, type(self)):\n            return NotImplemented\n\n        if self == other:\n            combined_ts = pd.concat([self.ts, other.ts]).sort_index()\n            combined_ts = combined_ts[~combined_ts.index.duplicated(keep=\"first\")]\n\n            return self.model_copy(update={\"ts\": combined_ts})\n        else:\n            raise TimeseriesUnequal()\n\n    def resample(\n        self: T,\n        freq: Any,\n        agg_func: Any = pd.Series.mean,\n        **resample_kwargs: Any,\n    ) -&gt; T:\n        \"\"\"Resample the timeseries to a new frequency with a specified\n        aggregation function.\n\n        Parameters:\n            freq (Any): The offset string or object representing target conversion\n                (e.g., 'D' for daily, 'W' for weekly).\n            agg_func (Any): The aggregation function to apply\n                after resampling. Defaults to pd.Series.mean.\n            **resample_kwargs: Additional keyword arguments passed to the\n                pandas.Series.resample method.\n\n        Returns:\n            Updated deep copy of the Timeseries object with the\n                resampled timeseries data.\n        \"\"\"\n        resampled_ts = self.ts.resample(freq, **resample_kwargs).apply(agg_func)\n\n        return self.model_copy(update={\"ts\": resampled_ts}, deep=True)\n\n    def transform(\n        self: T,\n        method: Literal[\n            \"difference\",\n            \"log\",\n            \"square_root\",\n            \"box_cox\",\n            \"standard_scaler\",\n            \"minmax_scaler\",\n            \"robust_scaler\",\n            \"maxabs_scaler\",\n        ],\n        **transformer_kwargs: Any,\n    ) -&gt; T:\n        \"\"\"Transforms the timeseries using the specified method.\n\n        Parameters:\n            method (str): The method to use for transformation ('minmax',\n                'standard', 'robust').\n            transformer_kwargs: Additional keyword arguments passed to the\n                transformer definition. See gensor.preprocessing.\n\n        Returns:\n            Updated deep copy of the Timeseries object with the\n                transformed timeseries data.\n        \"\"\"\n\n        data, transformation = Transformation(\n            self.ts, method, **transformer_kwargs\n        ).get_transformation()\n\n        return self.model_copy(\n            update={\"ts\": data, \"transformation\": transformation}, deep=True\n        )\n\n    def detect_outliers(\n        self: T,\n        method: Literal[\"iqr\", \"zscore\", \"isolation_forest\", \"lof\"],\n        rolling: bool = False,\n        window: int = 6,\n        remove: bool = True,\n        **kwargs: Any,\n    ) -&gt; T:\n        \"\"\"Detects outliers in the timeseries using the specified method.\n\n        Parameters:\n            method (Literal['iqr', 'zscore', 'isolation_forest', 'lof']): The\n                method to use for outlier detection.\n            **kwargs: Additional kewword arguments for OutlierDetection.\n\n        Returns:\n            Updated deep copy of the Timeseries object with outliers,\n            optionally removed from the original timeseries.\n        \"\"\"\n        self.outliers = OutlierDetection(\n            self.ts, method, rolling, window, **kwargs\n        ).outliers\n\n        if remove:\n            filtered_ts = self.ts.drop(self.outliers.index)\n            return self.model_copy(update={\"ts\": filtered_ts}, deep=True)\n\n        else:\n            return self\n\n    def mask_with(\n        self: T, other: T | pd.Series, mode: Literal[\"keep\", \"remove\"] = \"remove\"\n    ) -&gt; T:\n        \"\"\"\n        Removes records not present in 'other' by index.\n\n        Parameters:\n            other (Timeseries): Another Timeseries whose indices are used to mask the current one.\n            mode (Literal['keep', 'remove']):\n                - 'keep': Retains only the overlapping data.\n                - 'remove': Removes the overlapping data.\n\n        Returns:\n            Timeseries: A new Timeseries object with the filtered data.\n        \"\"\"\n        if isinstance(other, pd.Series):\n            mask = other\n        elif isinstance(other, BaseTimeseries):\n            mask = other.ts\n\n        if mode == \"keep\":\n            masked_data = self.ts[self.ts.index.isin(mask.index)]\n        elif mode == \"remove\":\n            masked_data = self.ts[~self.ts.index.isin(mask.index)]\n        else:\n            message = f\"Invalid mode: {mode}. Use 'keep' or 'remove'.\"\n            raise ValueError(message)\n\n        return self.model_copy(update={\"ts\": masked_data}, deep=True)\n\n    def to_sql(self: T, db: DatabaseConnection) -&gt; str:\n        \"\"\"Converts the timeseries to a list of dictionaries and uploads it to the database.\n\n        The Timeseries data is uploaded to the SQL database by using the pandas\n        `to_sql` method. Additionally, metadata about the timeseries is stored in the\n        'timeseries_metadata' table.\n\n        Parameters:\n            db (DatabaseConnection): The database connection object.\n\n        Returns:\n            str: A message indicating the number of rows inserted into the database.\n        \"\"\"\n\n        def separate_metadata() -&gt; tuple:\n            _core_metadata_fields = {\"location\", \"variable\", \"unit\", \"start\", \"end\"}\n\n            core_metadata = self.model_dump(include=_core_metadata_fields)\n            core_metadata.update({\n                \"cls\": f\"{self.__module__}.{self.__class__.__name__}\"\n            })\n\n            extra_metadata = self.model_dump(exclude=_core_metadata_fields)\n\n            return core_metadata, extra_metadata\n\n        timestamp_start_fmt = self.start.strftime(\"%Y%m%d%H%M%S\")\n        timestamp_end_fmt = self.end.strftime(\"%Y%m%d%H%M%S\")\n\n        # Ensure the index is a pandas DatetimeIndex\n        if isinstance(self.ts.index, pd.DatetimeIndex):\n            utc_index = (\n                self.ts.index.tz_convert(\"UTC\")\n                if self.ts.index.tz is not None\n                else self.ts.index\n            )\n        else:\n            message = \"The index is not a DatetimeIndex and cannot be converted to UTC.\"\n            raise TypeError(message)\n\n        series_as_records = list(\n            zip(utc_index.strftime(\"%Y-%m-%dT%H:%M:%S%z\"), self.ts, strict=False)\n        )\n\n        # Extra metadata are attributes additional to BaseTimeseries\n        core_metadata, extra_metadata = separate_metadata()\n\n        metadata_entry = {\n            **core_metadata,\n            \"extra\": extra_metadata,\n        }\n\n        created_table = db.get_timeseries_metadata(\n            location=self.location,\n            variable=self.variable,\n            unit=self.unit,\n            **extra_metadata,\n        )\n\n        with db as con:\n            if created_table.empty:\n                schema_name = f\"{self.location}_{self.variable}_{self.unit}\".lower()\n                unique_hash = str(uuid.uuid4())[:5]\n                schema_name = schema_name + f\"_{unique_hash}\"\n\n                # Newly created data schema\n                schema = db.create_table(schema_name, self.variable)\n            else:\n                # Existing data schema\n                schema_name = created_table[\"table_name\"].iloc[0]\n                schema = db.metadata.tables[schema_name]\n\n            metadata_schema = db.metadata.tables[\"__timeseries_metadata__\"]\n            metadata_entry.update({\"table_name\": schema_name})\n\n            if isinstance(schema, Table):\n                stmt = sqlite_insert(schema).values(series_as_records)\n                stmt = stmt.on_conflict_do_nothing(index_elements=[\"timestamp\"])\n                con.execute(stmt)\n\n                metadata_stmt = sqlite_insert(metadata_schema).values(metadata_entry)\n                metadata_stmt = metadata_stmt.on_conflict_do_update(\n                    index_elements=[\"table_name\"],\n                    set_={\n                        \"start\": timestamp_start_fmt,\n                        \"end\": timestamp_end_fmt,\n                    },\n                )\n                con.execute(metadata_stmt)\n\n            # Commit all changes at once\n            con.commit()\n\n        return f\"{schema_name} table and metadata updated.\"\n\n    def plot(\n        self: T,\n        include_outliers: bool = False,\n        ax: Axes | None = None,\n        plot_kwargs: dict[str, Any] | None = None,\n        legend_kwargs: dict[str, Any] | None = None,\n    ) -&gt; tuple[Figure, Axes]:\n        \"\"\"Plots the timeseries data.\n\n        Parameters:\n            include_outliers (bool): Whether to include outliers in the plot.\n            ax (matplotlib.axes.Axes, optional): Matplotlib axes object to plot on.\n                If None, a new figure and axes are created.\n            plot_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.plot() method to customize the plot.\n            legend_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.legend() to customize the legend.\n\n        Returns:\n            (fig, ax): Matplotlib figure and axes to allow further customization.\n        \"\"\"\n\n        plot_kwargs = plot_kwargs or {}\n        legend_kwargs = legend_kwargs or {}\n\n        if ax is None:\n            fig, ax = plt.subplots(figsize=(10, 5))\n        else:\n            # mypy complained that the get_figure() can return None, but there is no\n            # situation here in which this could be the case.\n            fig = ax.get_figure()  # type: ignore [assignment]\n\n        ax.plot(\n            self.ts.index,\n            self.ts,\n            label=f\"{self.location}\",\n            **plot_kwargs,\n        )\n\n        if include_outliers and self.outliers is not None:\n            ax.scatter(\n                self.outliers.index, self.outliers, color=\"red\", label=\"Outliers\"\n            )\n        for label in ax.get_xticklabels():\n            label.set_rotation(45)\n\n        ax.set_xlabel(\"Time\")\n        ax.set_ylabel(f\"{self.variable} ({self.unit})\")\n        ax.set_title(f\"{self.variable.capitalize()} at {self.location}\")\n\n        ax.legend(**legend_kwargs)\n\n        return fig, ax\n</code></pre>"},{"location":"modules/#gensor.core.base.BaseTimeseries.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check equality based on location, sensor, variable, unit and sensor_alt.</p> Source code in <code>gensor/core/base.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"Check equality based on location, sensor, variable, unit and sensor_alt.\"\"\"\n    if not isinstance(other, BaseTimeseries):\n        return NotImplemented\n\n    return (\n        self.variable == other.variable\n        and self.unit == other.unit\n        and self.location == other.location\n    )\n</code></pre>"},{"location":"modules/#gensor.core.base.BaseTimeseries.__getattr__","title":"<code>__getattr__(attr)</code>","text":"<p>Delegate attribute access to the underlying pandas Series if it exists.</p> <p>Special handling is implemented for pandas indexer.</p> Source code in <code>gensor/core/base.py</code> <pre><code>def __getattr__(self, attr: Any) -&gt; Any:\n    \"\"\"Delegate attribute access to the underlying pandas Series if it exists.\n\n    Special handling is implemented for pandas indexer.\n    \"\"\"\n    if attr == \"loc\":\n        return TimeseriesIndexer(self, self.ts.loc)\n\n    if attr == \"iloc\":\n        return TimeseriesIndexer(self, self.ts.iloc)\n\n    error_message = f\"'{self.__class__.__name__}' object has no attribute '{attr}'\"\n\n    if hasattr(self.ts, attr):\n        # Return a function to call on the `ts` if it's a method, otherwise return the attribute\n        ts_attr = getattr(self.ts, attr)\n        if callable(ts_attr):\n\n            def wrapper(*args: Any, **kwargs: Any) -&gt; Any:\n                result = ts_attr(*args, **kwargs)\n                # If the result is a Series, return a new Timeseries; otherwise, return the result\n                if isinstance(result, pd.Series):\n                    return self.model_copy(update={\"ts\": result}, deep=True)\n\n                return result\n\n            return wrapper\n        else:\n            return ts_attr\n    raise AttributeError(error_message)\n</code></pre>"},{"location":"modules/#gensor.core.base.BaseTimeseries.concatenate","title":"<code>concatenate(other)</code>","text":"<p>Concatenate two Timeseries objects if they are considered equal.</p> Source code in <code>gensor/core/base.py</code> <pre><code>def concatenate(self: T, other: T) -&gt; T:\n    \"\"\"Concatenate two Timeseries objects if they are considered equal.\"\"\"\n    if not isinstance(other, type(self)):\n        return NotImplemented\n\n    if self == other:\n        combined_ts = pd.concat([self.ts, other.ts]).sort_index()\n        combined_ts = combined_ts[~combined_ts.index.duplicated(keep=\"first\")]\n\n        return self.model_copy(update={\"ts\": combined_ts})\n    else:\n        raise TimeseriesUnequal()\n</code></pre>"},{"location":"modules/#gensor.core.base.BaseTimeseries.detect_outliers","title":"<code>detect_outliers(method, rolling=False, window=6, remove=True, **kwargs)</code>","text":"<p>Detects outliers in the timeseries using the specified method.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Literal['iqr', 'zscore', 'isolation_forest', 'lof']</code> <p>The method to use for outlier detection.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional kewword arguments for OutlierDetection.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>Updated deep copy of the Timeseries object with outliers,</p> <code>T</code> <p>optionally removed from the original timeseries.</p> Source code in <code>gensor/core/base.py</code> <pre><code>def detect_outliers(\n    self: T,\n    method: Literal[\"iqr\", \"zscore\", \"isolation_forest\", \"lof\"],\n    rolling: bool = False,\n    window: int = 6,\n    remove: bool = True,\n    **kwargs: Any,\n) -&gt; T:\n    \"\"\"Detects outliers in the timeseries using the specified method.\n\n    Parameters:\n        method (Literal['iqr', 'zscore', 'isolation_forest', 'lof']): The\n            method to use for outlier detection.\n        **kwargs: Additional kewword arguments for OutlierDetection.\n\n    Returns:\n        Updated deep copy of the Timeseries object with outliers,\n        optionally removed from the original timeseries.\n    \"\"\"\n    self.outliers = OutlierDetection(\n        self.ts, method, rolling, window, **kwargs\n    ).outliers\n\n    if remove:\n        filtered_ts = self.ts.drop(self.outliers.index)\n        return self.model_copy(update={\"ts\": filtered_ts}, deep=True)\n\n    else:\n        return self\n</code></pre>"},{"location":"modules/#gensor.core.base.BaseTimeseries.mask_with","title":"<code>mask_with(other, mode='remove')</code>","text":"<p>Removes records not present in 'other' by index.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Timeseries</code> <p>Another Timeseries whose indices are used to mask the current one.</p> required <code>mode</code> <code>Literal['keep', 'remove']</code> <ul> <li>'keep': Retains only the overlapping data.</li> <li>'remove': Removes the overlapping data.</li> </ul> <code>'remove'</code> <p>Returns:</p> Name Type Description <code>Timeseries</code> <code>T</code> <p>A new Timeseries object with the filtered data.</p> Source code in <code>gensor/core/base.py</code> <pre><code>def mask_with(\n    self: T, other: T | pd.Series, mode: Literal[\"keep\", \"remove\"] = \"remove\"\n) -&gt; T:\n    \"\"\"\n    Removes records not present in 'other' by index.\n\n    Parameters:\n        other (Timeseries): Another Timeseries whose indices are used to mask the current one.\n        mode (Literal['keep', 'remove']):\n            - 'keep': Retains only the overlapping data.\n            - 'remove': Removes the overlapping data.\n\n    Returns:\n        Timeseries: A new Timeseries object with the filtered data.\n    \"\"\"\n    if isinstance(other, pd.Series):\n        mask = other\n    elif isinstance(other, BaseTimeseries):\n        mask = other.ts\n\n    if mode == \"keep\":\n        masked_data = self.ts[self.ts.index.isin(mask.index)]\n    elif mode == \"remove\":\n        masked_data = self.ts[~self.ts.index.isin(mask.index)]\n    else:\n        message = f\"Invalid mode: {mode}. Use 'keep' or 'remove'.\"\n        raise ValueError(message)\n\n    return self.model_copy(update={\"ts\": masked_data}, deep=True)\n</code></pre>"},{"location":"modules/#gensor.core.base.BaseTimeseries.plot","title":"<code>plot(include_outliers=False, ax=None, plot_kwargs=None, legend_kwargs=None)</code>","text":"<p>Plots the timeseries data.</p> <p>Parameters:</p> Name Type Description Default <code>include_outliers</code> <code>bool</code> <p>Whether to include outliers in the plot.</p> <code>False</code> <code>ax</code> <code>Axes</code> <p>Matplotlib axes object to plot on. If None, a new figure and axes are created.</p> <code>None</code> <code>plot_kwargs</code> <code>dict[str, Any] | None</code> <p>kwargs passed to matplotlib.axes.Axes.plot() method to customize the plot.</p> <code>None</code> <code>legend_kwargs</code> <code>dict[str, Any] | None</code> <p>kwargs passed to matplotlib.axes.Axes.legend() to customize the legend.</p> <code>None</code> <p>Returns:</p> Type Description <code>(fig, ax)</code> <p>Matplotlib figure and axes to allow further customization.</p> Source code in <code>gensor/core/base.py</code> <pre><code>def plot(\n    self: T,\n    include_outliers: bool = False,\n    ax: Axes | None = None,\n    plot_kwargs: dict[str, Any] | None = None,\n    legend_kwargs: dict[str, Any] | None = None,\n) -&gt; tuple[Figure, Axes]:\n    \"\"\"Plots the timeseries data.\n\n    Parameters:\n        include_outliers (bool): Whether to include outliers in the plot.\n        ax (matplotlib.axes.Axes, optional): Matplotlib axes object to plot on.\n            If None, a new figure and axes are created.\n        plot_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.plot() method to customize the plot.\n        legend_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.legend() to customize the legend.\n\n    Returns:\n        (fig, ax): Matplotlib figure and axes to allow further customization.\n    \"\"\"\n\n    plot_kwargs = plot_kwargs or {}\n    legend_kwargs = legend_kwargs or {}\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(10, 5))\n    else:\n        # mypy complained that the get_figure() can return None, but there is no\n        # situation here in which this could be the case.\n        fig = ax.get_figure()  # type: ignore [assignment]\n\n    ax.plot(\n        self.ts.index,\n        self.ts,\n        label=f\"{self.location}\",\n        **plot_kwargs,\n    )\n\n    if include_outliers and self.outliers is not None:\n        ax.scatter(\n            self.outliers.index, self.outliers, color=\"red\", label=\"Outliers\"\n        )\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)\n\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(f\"{self.variable} ({self.unit})\")\n    ax.set_title(f\"{self.variable.capitalize()} at {self.location}\")\n\n    ax.legend(**legend_kwargs)\n\n    return fig, ax\n</code></pre>"},{"location":"modules/#gensor.core.base.BaseTimeseries.resample","title":"<code>resample(freq, agg_func=pd.Series.mean, **resample_kwargs)</code>","text":"<p>Resample the timeseries to a new frequency with a specified aggregation function.</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>Any</code> <p>The offset string or object representing target conversion (e.g., 'D' for daily, 'W' for weekly).</p> required <code>agg_func</code> <code>Any</code> <p>The aggregation function to apply after resampling. Defaults to pd.Series.mean.</p> <code>mean</code> <code>**resample_kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the pandas.Series.resample method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>Updated deep copy of the Timeseries object with the resampled timeseries data.</p> Source code in <code>gensor/core/base.py</code> <pre><code>def resample(\n    self: T,\n    freq: Any,\n    agg_func: Any = pd.Series.mean,\n    **resample_kwargs: Any,\n) -&gt; T:\n    \"\"\"Resample the timeseries to a new frequency with a specified\n    aggregation function.\n\n    Parameters:\n        freq (Any): The offset string or object representing target conversion\n            (e.g., 'D' for daily, 'W' for weekly).\n        agg_func (Any): The aggregation function to apply\n            after resampling. Defaults to pd.Series.mean.\n        **resample_kwargs: Additional keyword arguments passed to the\n            pandas.Series.resample method.\n\n    Returns:\n        Updated deep copy of the Timeseries object with the\n            resampled timeseries data.\n    \"\"\"\n    resampled_ts = self.ts.resample(freq, **resample_kwargs).apply(agg_func)\n\n    return self.model_copy(update={\"ts\": resampled_ts}, deep=True)\n</code></pre>"},{"location":"modules/#gensor.core.base.BaseTimeseries.serialize_timestamps","title":"<code>serialize_timestamps(value)</code>","text":"<p>Serialize <code>pd.Timestamp</code> to ISO format.</p> Source code in <code>gensor/core/base.py</code> <pre><code>@pyd.field_serializer(\"start\", \"end\")\ndef serialize_timestamps(self, value: pd.Timestamp | None) -&gt; str | None:\n    \"\"\"Serialize `pd.Timestamp` to ISO format.\"\"\"\n    return value.strftime(\"%Y%m%d%H%M%S\") if value is not None else None\n</code></pre>"},{"location":"modules/#gensor.core.base.BaseTimeseries.to_sql","title":"<code>to_sql(db)</code>","text":"<p>Converts the timeseries to a list of dictionaries and uploads it to the database.</p> <p>The Timeseries data is uploaded to the SQL database by using the pandas <code>to_sql</code> method. Additionally, metadata about the timeseries is stored in the 'timeseries_metadata' table.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>DatabaseConnection</code> <p>The database connection object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A message indicating the number of rows inserted into the database.</p> Source code in <code>gensor/core/base.py</code> <pre><code>def to_sql(self: T, db: DatabaseConnection) -&gt; str:\n    \"\"\"Converts the timeseries to a list of dictionaries and uploads it to the database.\n\n    The Timeseries data is uploaded to the SQL database by using the pandas\n    `to_sql` method. Additionally, metadata about the timeseries is stored in the\n    'timeseries_metadata' table.\n\n    Parameters:\n        db (DatabaseConnection): The database connection object.\n\n    Returns:\n        str: A message indicating the number of rows inserted into the database.\n    \"\"\"\n\n    def separate_metadata() -&gt; tuple:\n        _core_metadata_fields = {\"location\", \"variable\", \"unit\", \"start\", \"end\"}\n\n        core_metadata = self.model_dump(include=_core_metadata_fields)\n        core_metadata.update({\n            \"cls\": f\"{self.__module__}.{self.__class__.__name__}\"\n        })\n\n        extra_metadata = self.model_dump(exclude=_core_metadata_fields)\n\n        return core_metadata, extra_metadata\n\n    timestamp_start_fmt = self.start.strftime(\"%Y%m%d%H%M%S\")\n    timestamp_end_fmt = self.end.strftime(\"%Y%m%d%H%M%S\")\n\n    # Ensure the index is a pandas DatetimeIndex\n    if isinstance(self.ts.index, pd.DatetimeIndex):\n        utc_index = (\n            self.ts.index.tz_convert(\"UTC\")\n            if self.ts.index.tz is not None\n            else self.ts.index\n        )\n    else:\n        message = \"The index is not a DatetimeIndex and cannot be converted to UTC.\"\n        raise TypeError(message)\n\n    series_as_records = list(\n        zip(utc_index.strftime(\"%Y-%m-%dT%H:%M:%S%z\"), self.ts, strict=False)\n    )\n\n    # Extra metadata are attributes additional to BaseTimeseries\n    core_metadata, extra_metadata = separate_metadata()\n\n    metadata_entry = {\n        **core_metadata,\n        \"extra\": extra_metadata,\n    }\n\n    created_table = db.get_timeseries_metadata(\n        location=self.location,\n        variable=self.variable,\n        unit=self.unit,\n        **extra_metadata,\n    )\n\n    with db as con:\n        if created_table.empty:\n            schema_name = f\"{self.location}_{self.variable}_{self.unit}\".lower()\n            unique_hash = str(uuid.uuid4())[:5]\n            schema_name = schema_name + f\"_{unique_hash}\"\n\n            # Newly created data schema\n            schema = db.create_table(schema_name, self.variable)\n        else:\n            # Existing data schema\n            schema_name = created_table[\"table_name\"].iloc[0]\n            schema = db.metadata.tables[schema_name]\n\n        metadata_schema = db.metadata.tables[\"__timeseries_metadata__\"]\n        metadata_entry.update({\"table_name\": schema_name})\n\n        if isinstance(schema, Table):\n            stmt = sqlite_insert(schema).values(series_as_records)\n            stmt = stmt.on_conflict_do_nothing(index_elements=[\"timestamp\"])\n            con.execute(stmt)\n\n            metadata_stmt = sqlite_insert(metadata_schema).values(metadata_entry)\n            metadata_stmt = metadata_stmt.on_conflict_do_update(\n                index_elements=[\"table_name\"],\n                set_={\n                    \"start\": timestamp_start_fmt,\n                    \"end\": timestamp_end_fmt,\n                },\n            )\n            con.execute(metadata_stmt)\n\n        # Commit all changes at once\n        con.commit()\n\n    return f\"{schema_name} table and metadata updated.\"\n</code></pre>"},{"location":"modules/#gensor.core.base.BaseTimeseries.transform","title":"<code>transform(method, **transformer_kwargs)</code>","text":"<p>Transforms the timeseries using the specified method.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The method to use for transformation ('minmax', 'standard', 'robust').</p> required <code>transformer_kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the transformer definition. See gensor.preprocessing.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>Updated deep copy of the Timeseries object with the transformed timeseries data.</p> Source code in <code>gensor/core/base.py</code> <pre><code>def transform(\n    self: T,\n    method: Literal[\n        \"difference\",\n        \"log\",\n        \"square_root\",\n        \"box_cox\",\n        \"standard_scaler\",\n        \"minmax_scaler\",\n        \"robust_scaler\",\n        \"maxabs_scaler\",\n    ],\n    **transformer_kwargs: Any,\n) -&gt; T:\n    \"\"\"Transforms the timeseries using the specified method.\n\n    Parameters:\n        method (str): The method to use for transformation ('minmax',\n            'standard', 'robust').\n        transformer_kwargs: Additional keyword arguments passed to the\n            transformer definition. See gensor.preprocessing.\n\n    Returns:\n        Updated deep copy of the Timeseries object with the\n            transformed timeseries data.\n    \"\"\"\n\n    data, transformation = Transformation(\n        self.ts, method, **transformer_kwargs\n    ).get_transformation()\n\n    return self.model_copy(\n        update={\"ts\": data, \"transformation\": transformation}, deep=True\n    )\n</code></pre>"},{"location":"modules/#gensor.core.dataset","title":"<code>dataset</code>","text":""},{"location":"modules/#gensor.core.dataset.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[T]</code></p> <p>Store and operate on a collection of Timeseries.</p> <p>Attributes:</p> Name Type Description <code>timeseries</code> <code>list[Timeseries]</code> <p>A list of Timeseries objects.</p> Source code in <code>gensor/core/dataset.py</code> <pre><code>class Dataset(pyd.BaseModel, Generic[T]):\n    \"\"\"Store and operate on a collection of Timeseries.\n\n    Attributes:\n        timeseries (list[Timeseries]): A list of Timeseries objects.\n    \"\"\"\n\n    timeseries: list[T | None] = pyd.Field(default_factory=list)\n\n    def __iter__(self) -&gt; Any:\n        \"\"\"Allows to iterate directly over the dataset.\"\"\"\n        return iter(self.timeseries)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Gives the number of timeseries in the Dataset.\"\"\"\n        return len(self.timeseries)\n\n    def __repr__(self) -&gt; str:\n        return f\"Dataset({len(self)})\"\n\n    def __getitem__(self, index: int) -&gt; T | None:\n        \"\"\"Retrieve a Timeseries object by its index in the dataset.\n\n        Parameters:\n            index (int): The index of the Timeseries to retrieve.\n\n        Returns:\n            Timeseries: The Timeseries object at the specified index.\n\n        Raises:\n            IndexError: If the index is out of range.\n        \"\"\"\n        try:\n            return self.timeseries[index].model_copy(deep=True)\n        except IndexError:\n            raise IndexOutOfRangeError(index, len(self)) from None\n\n    def get_locations(self) -&gt; list:\n        \"\"\"List all unique locations in the dataset.\"\"\"\n        return [ts.location for ts in self.timeseries if ts is not None]\n\n    def add(self, other: T | list[T] | Dataset) -&gt; Dataset:\n        \"\"\"Appends new Timeseries to the Dataset.\n\n        If an equal Timeseries already exists, merge the new data into the existing\n        Timeseries, dropping duplicate timestamps.\n\n        Parameters:\n            other (Timeseries): The Timeseries object to add.\n        \"\"\"\n\n        # I need to check for BaseTimeseries instance in the add() method, but also\n        # type hint VarType T.\n        if isinstance(other, list | Dataset):\n            for ts in other:\n                if isinstance(ts, BaseTimeseries):\n                    self._add_single_timeseries(ts)  # type: ignore[arg-type]\n\n        elif isinstance(other, BaseTimeseries):\n            self._add_single_timeseries(other)\n\n        return self\n\n    def _add_single_timeseries(self, ts: T) -&gt; None:\n        \"\"\"Adds a single Timeseries to the Dataset or merges if an equal one exists.\"\"\"\n        for i, existing_ts in enumerate(self.timeseries):\n            if existing_ts == ts:\n                self.timeseries[i] = existing_ts.concatenate(ts)\n                return\n\n        self.timeseries.append(ts)\n\n        return\n\n    def filter(\n        self,\n        location: str | list | None = None,\n        variable: str | list | None = None,\n        unit: str | list | None = None,\n        **kwargs: dict[str, str | list],\n    ) -&gt; T | Dataset:\n        \"\"\"Return a Timeseries or a new Dataset filtered by station, sensor,\n        and/or variable.\n\n        Parameters:\n            location (Optional[str]): The location name.\n            variable (Optional[str]): The variable being measured.\n            unit (Optional[str]): Unit of the measurement.\n            **kwargs (dict): Attributes of subclassed timeseries used for filtering\n                (e.g., sensor, method).\n\n        Returns:\n            Timeseries | Dataset: A single Timeseries if exactly one match is found,\n                                   or a new Dataset if multiple matches are found.\n        \"\"\"\n\n        def matches(ts: T, attr: str, value: dict[str, str | list]) -&gt; bool | None:\n            \"\"\"Check if the Timeseries object has the attribute and if it matches the value.\"\"\"\n            if not hasattr(ts, attr):\n                message = f\"'{ts.__class__.__name__}' object has no attribute '{attr}'\"\n                raise AttributeError(message)\n            return getattr(ts, attr) in value\n\n        if isinstance(location, str):\n            location = [location]\n        if isinstance(variable, str):\n            variable = [variable]\n        if isinstance(unit, str):\n            unit = [unit]\n        for key, value in kwargs.items():\n            if isinstance(value, str):\n                kwargs[key] = [value]\n\n        matching_timeseries = [\n            ts\n            for ts in self.timeseries\n            if ts is not None\n            and (location is None or ts.location in location)\n            and (variable is None or ts.variable in variable)\n            and (unit is None or ts.unit in unit)\n            and all(matches(ts, attr, value) for attr, value in kwargs.items())\n        ]\n\n        if not matching_timeseries:\n            return Dataset()\n\n        if len(matching_timeseries) == 1:\n            return matching_timeseries[0].model_copy(deep=True)\n\n        return self.model_copy(update={\"timeseries\": matching_timeseries})\n\n    def to_sql(self, db: DatabaseConnection) -&gt; None:\n        \"\"\"Save the entire timeseries to a SQLite database.\n\n        Parameters:\n            db (DatabaseConnection): SQLite database connection object.\n        \"\"\"\n        for ts in self.timeseries:\n            if ts:\n                ts.to_sql(db)\n        return\n\n    def plot(\n        self,\n        include_outliers: bool = False,\n        plot_kwargs: dict[str, Any] | None = None,\n        legend_kwargs: dict[str, Any] | None = None,\n    ) -&gt; tuple[Figure, Axes]:\n        \"\"\"Plots the timeseries data, grouping by variable type.\n\n        Parameters:\n            include_outliers (bool): Whether to include outliers in the plot.\n            plot_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.plot() method to customize the plot.\n            legend_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.legend() to customize the legend.\n\n        Returns:\n            (fig, ax): Matplotlib figure and axes to allow further customization.\n        \"\"\"\n\n        grouped_ts = defaultdict(list)\n\n        for ts in self.timeseries:\n            if ts:\n                grouped_ts[ts.variable].append(ts)\n\n        num_variables = len(grouped_ts)\n\n        fig, axes = plt.subplots(\n            num_variables, 1, figsize=(10, 5 * num_variables), sharex=True\n        )\n\n        if num_variables == 1:\n            axes = [axes]\n\n        for ax, (variable, ts_list) in zip(axes, grouped_ts.items(), strict=False):\n            for ts in ts_list:\n                ts.plot(\n                    include_outliers=include_outliers,\n                    ax=ax,\n                    plot_kwargs=plot_kwargs,\n                    legend_kwargs=legend_kwargs,\n                )\n\n            ax.set_title(f\"Timeseries for {variable.capitalize()}\")\n            ax.set_xlabel(\"Time\")\n\n        fig.tight_layout()\n        return fig, axes\n</code></pre>"},{"location":"modules/#gensor.core.dataset.Dataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Retrieve a Timeseries object by its index in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the Timeseries to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Timeseries</code> <code>T | None</code> <p>The Timeseries object at the specified index.</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If the index is out of range.</p> Source code in <code>gensor/core/dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; T | None:\n    \"\"\"Retrieve a Timeseries object by its index in the dataset.\n\n    Parameters:\n        index (int): The index of the Timeseries to retrieve.\n\n    Returns:\n        Timeseries: The Timeseries object at the specified index.\n\n    Raises:\n        IndexError: If the index is out of range.\n    \"\"\"\n    try:\n        return self.timeseries[index].model_copy(deep=True)\n    except IndexError:\n        raise IndexOutOfRangeError(index, len(self)) from None\n</code></pre>"},{"location":"modules/#gensor.core.dataset.Dataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Allows to iterate directly over the dataset.</p> Source code in <code>gensor/core/dataset.py</code> <pre><code>def __iter__(self) -&gt; Any:\n    \"\"\"Allows to iterate directly over the dataset.\"\"\"\n    return iter(self.timeseries)\n</code></pre>"},{"location":"modules/#gensor.core.dataset.Dataset.__len__","title":"<code>__len__()</code>","text":"<p>Gives the number of timeseries in the Dataset.</p> Source code in <code>gensor/core/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Gives the number of timeseries in the Dataset.\"\"\"\n    return len(self.timeseries)\n</code></pre>"},{"location":"modules/#gensor.core.dataset.Dataset.add","title":"<code>add(other)</code>","text":"<p>Appends new Timeseries to the Dataset.</p> <p>If an equal Timeseries already exists, merge the new data into the existing Timeseries, dropping duplicate timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Timeseries</code> <p>The Timeseries object to add.</p> required Source code in <code>gensor/core/dataset.py</code> <pre><code>def add(self, other: T | list[T] | Dataset) -&gt; Dataset:\n    \"\"\"Appends new Timeseries to the Dataset.\n\n    If an equal Timeseries already exists, merge the new data into the existing\n    Timeseries, dropping duplicate timestamps.\n\n    Parameters:\n        other (Timeseries): The Timeseries object to add.\n    \"\"\"\n\n    # I need to check for BaseTimeseries instance in the add() method, but also\n    # type hint VarType T.\n    if isinstance(other, list | Dataset):\n        for ts in other:\n            if isinstance(ts, BaseTimeseries):\n                self._add_single_timeseries(ts)  # type: ignore[arg-type]\n\n    elif isinstance(other, BaseTimeseries):\n        self._add_single_timeseries(other)\n\n    return self\n</code></pre>"},{"location":"modules/#gensor.core.dataset.Dataset.filter","title":"<code>filter(location=None, variable=None, unit=None, **kwargs)</code>","text":"<p>Return a Timeseries or a new Dataset filtered by station, sensor, and/or variable.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>Optional[str]</code> <p>The location name.</p> <code>None</code> <code>variable</code> <code>Optional[str]</code> <p>The variable being measured.</p> <code>None</code> <code>unit</code> <code>Optional[str]</code> <p>Unit of the measurement.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Attributes of subclassed timeseries used for filtering (e.g., sensor, method).</p> <code>{}</code> <p>Returns:</p> Type Description <code>T | Dataset</code> <p>Timeseries | Dataset: A single Timeseries if exactly one match is found,                    or a new Dataset if multiple matches are found.</p> Source code in <code>gensor/core/dataset.py</code> <pre><code>def filter(\n    self,\n    location: str | list | None = None,\n    variable: str | list | None = None,\n    unit: str | list | None = None,\n    **kwargs: dict[str, str | list],\n) -&gt; T | Dataset:\n    \"\"\"Return a Timeseries or a new Dataset filtered by station, sensor,\n    and/or variable.\n\n    Parameters:\n        location (Optional[str]): The location name.\n        variable (Optional[str]): The variable being measured.\n        unit (Optional[str]): Unit of the measurement.\n        **kwargs (dict): Attributes of subclassed timeseries used for filtering\n            (e.g., sensor, method).\n\n    Returns:\n        Timeseries | Dataset: A single Timeseries if exactly one match is found,\n                               or a new Dataset if multiple matches are found.\n    \"\"\"\n\n    def matches(ts: T, attr: str, value: dict[str, str | list]) -&gt; bool | None:\n        \"\"\"Check if the Timeseries object has the attribute and if it matches the value.\"\"\"\n        if not hasattr(ts, attr):\n            message = f\"'{ts.__class__.__name__}' object has no attribute '{attr}'\"\n            raise AttributeError(message)\n        return getattr(ts, attr) in value\n\n    if isinstance(location, str):\n        location = [location]\n    if isinstance(variable, str):\n        variable = [variable]\n    if isinstance(unit, str):\n        unit = [unit]\n    for key, value in kwargs.items():\n        if isinstance(value, str):\n            kwargs[key] = [value]\n\n    matching_timeseries = [\n        ts\n        for ts in self.timeseries\n        if ts is not None\n        and (location is None or ts.location in location)\n        and (variable is None or ts.variable in variable)\n        and (unit is None or ts.unit in unit)\n        and all(matches(ts, attr, value) for attr, value in kwargs.items())\n    ]\n\n    if not matching_timeseries:\n        return Dataset()\n\n    if len(matching_timeseries) == 1:\n        return matching_timeseries[0].model_copy(deep=True)\n\n    return self.model_copy(update={\"timeseries\": matching_timeseries})\n</code></pre>"},{"location":"modules/#gensor.core.dataset.Dataset.get_locations","title":"<code>get_locations()</code>","text":"<p>List all unique locations in the dataset.</p> Source code in <code>gensor/core/dataset.py</code> <pre><code>def get_locations(self) -&gt; list:\n    \"\"\"List all unique locations in the dataset.\"\"\"\n    return [ts.location for ts in self.timeseries if ts is not None]\n</code></pre>"},{"location":"modules/#gensor.core.dataset.Dataset.plot","title":"<code>plot(include_outliers=False, plot_kwargs=None, legend_kwargs=None)</code>","text":"<p>Plots the timeseries data, grouping by variable type.</p> <p>Parameters:</p> Name Type Description Default <code>include_outliers</code> <code>bool</code> <p>Whether to include outliers in the plot.</p> <code>False</code> <code>plot_kwargs</code> <code>dict[str, Any] | None</code> <p>kwargs passed to matplotlib.axes.Axes.plot() method to customize the plot.</p> <code>None</code> <code>legend_kwargs</code> <code>dict[str, Any] | None</code> <p>kwargs passed to matplotlib.axes.Axes.legend() to customize the legend.</p> <code>None</code> <p>Returns:</p> Type Description <code>(fig, ax)</code> <p>Matplotlib figure and axes to allow further customization.</p> Source code in <code>gensor/core/dataset.py</code> <pre><code>def plot(\n    self,\n    include_outliers: bool = False,\n    plot_kwargs: dict[str, Any] | None = None,\n    legend_kwargs: dict[str, Any] | None = None,\n) -&gt; tuple[Figure, Axes]:\n    \"\"\"Plots the timeseries data, grouping by variable type.\n\n    Parameters:\n        include_outliers (bool): Whether to include outliers in the plot.\n        plot_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.plot() method to customize the plot.\n        legend_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.legend() to customize the legend.\n\n    Returns:\n        (fig, ax): Matplotlib figure and axes to allow further customization.\n    \"\"\"\n\n    grouped_ts = defaultdict(list)\n\n    for ts in self.timeseries:\n        if ts:\n            grouped_ts[ts.variable].append(ts)\n\n    num_variables = len(grouped_ts)\n\n    fig, axes = plt.subplots(\n        num_variables, 1, figsize=(10, 5 * num_variables), sharex=True\n    )\n\n    if num_variables == 1:\n        axes = [axes]\n\n    for ax, (variable, ts_list) in zip(axes, grouped_ts.items(), strict=False):\n        for ts in ts_list:\n            ts.plot(\n                include_outliers=include_outliers,\n                ax=ax,\n                plot_kwargs=plot_kwargs,\n                legend_kwargs=legend_kwargs,\n            )\n\n        ax.set_title(f\"Timeseries for {variable.capitalize()}\")\n        ax.set_xlabel(\"Time\")\n\n    fig.tight_layout()\n    return fig, axes\n</code></pre>"},{"location":"modules/#gensor.core.dataset.Dataset.to_sql","title":"<code>to_sql(db)</code>","text":"<p>Save the entire timeseries to a SQLite database.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>DatabaseConnection</code> <p>SQLite database connection object.</p> required Source code in <code>gensor/core/dataset.py</code> <pre><code>def to_sql(self, db: DatabaseConnection) -&gt; None:\n    \"\"\"Save the entire timeseries to a SQLite database.\n\n    Parameters:\n        db (DatabaseConnection): SQLite database connection object.\n    \"\"\"\n    for ts in self.timeseries:\n        if ts:\n            ts.to_sql(db)\n    return\n</code></pre>"},{"location":"modules/#gensor.core.indexer","title":"<code>indexer</code>","text":""},{"location":"modules/#gensor.core.indexer.TimeseriesIndexer","title":"<code>TimeseriesIndexer</code>","text":"<p>A wrapper for the Pandas indexers (e.g., loc, iloc) to return Timeseries objects.</p> Source code in <code>gensor/core/indexer.py</code> <pre><code>class TimeseriesIndexer:\n    \"\"\"A wrapper for the Pandas indexers (e.g., loc, iloc) to return Timeseries objects.\"\"\"\n\n    # marked indexer as Any to silence mypy. BaseIndexer is normally not indexable:\n    # the same for the `parent`. It should by always type Timeseries, but I don't want\n    # to deal with circular imports just for type hints for the devs...\n\n    def __init__(self, parent: Any, indexer: Any):\n        self.parent = parent\n        self.indexer = indexer\n\n    def __getitem__(self, key: str) -&gt; Any:\n        \"\"\"Allows using the indexer (e.g., loc) and wraps the result in a Timeseries.\"\"\"\n\n        result = self.indexer[key]\n\n        if isinstance(result, pd.Series):\n            return self.parent.model_copy(update={\"ts\": result}, deep=True)\n\n        if isinstance(result, (int | float | str | pd.Timestamp | np.float64)):\n            return result\n\n        message = f\"Expected pd.Series, but got {type(result)} instead.\"\n        raise TypeError(message)\n\n    def __setitem__(self, key: str, value: Any) -&gt; None:\n        \"\"\"Allows setting values directly using the indexer (e.g., loc, iloc).\"\"\"\n\n        self.indexer[key] = value\n</code></pre>"},{"location":"modules/#gensor.core.indexer.TimeseriesIndexer.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Allows using the indexer (e.g., loc) and wraps the result in a Timeseries.</p> Source code in <code>gensor/core/indexer.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Any:\n    \"\"\"Allows using the indexer (e.g., loc) and wraps the result in a Timeseries.\"\"\"\n\n    result = self.indexer[key]\n\n    if isinstance(result, pd.Series):\n        return self.parent.model_copy(update={\"ts\": result}, deep=True)\n\n    if isinstance(result, (int | float | str | pd.Timestamp | np.float64)):\n        return result\n\n    message = f\"Expected pd.Series, but got {type(result)} instead.\"\n    raise TypeError(message)\n</code></pre>"},{"location":"modules/#gensor.core.indexer.TimeseriesIndexer.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Allows setting values directly using the indexer (e.g., loc, iloc).</p> Source code in <code>gensor/core/indexer.py</code> <pre><code>def __setitem__(self, key: str, value: Any) -&gt; None:\n    \"\"\"Allows setting values directly using the indexer (e.g., loc, iloc).\"\"\"\n\n    self.indexer[key] = value\n</code></pre>"},{"location":"modules/#gensor.core.timeseries","title":"<code>timeseries</code>","text":""},{"location":"modules/#gensor.core.timeseries.Timeseries","title":"<code>Timeseries</code>","text":"<p>               Bases: <code>BaseTimeseries</code></p> <p>Timeseries of groundwater sensor data.</p> <p>Attributes:</p> Name Type Description <code>ts</code> <code>Series</code> <p>The timeseries data.</p> <code>variable</code> <code>Literal['temperature', 'pressure', 'conductivity', 'flux']</code> <p>The type of the measurement.</p> <code>unit</code> <code>Literal['degC', 'mmH2O', 'mS/cm', 'm/s']</code> <p>The unit of the measurement.</p> <code>sensor</code> <code>str</code> <p>The serial number of the sensor.</p> <code>sensor_alt</code> <code>float</code> <p>Altitude of the sensor (ncessary to compute groundwater levels).</p> Source code in <code>gensor/core/timeseries.py</code> <pre><code>class Timeseries(BaseTimeseries):\n    \"\"\"Timeseries of groundwater sensor data.\n\n    Attributes:\n        ts (pd.Series): The timeseries data.\n        variable (Literal['temperature', 'pressure', 'conductivity', 'flux']):\n            The type of the measurement.\n        unit (Literal['degC', 'mmH2O', 'mS/cm', 'm/s']): The unit of\n            the measurement.\n        sensor (str): The serial number of the sensor.\n        sensor_alt (float): Altitude of the sensor (ncessary to compute groundwater levels).\n    \"\"\"\n\n    model_config = pyd.ConfigDict(\n        arbitrary_types_allowed=True, validate_assignment=True\n    )\n\n    sensor: str | None = None\n    sensor_alt: float | None = None\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Check equality based on location, sensor, variable, unit and sensor_alt.\"\"\"\n        if not isinstance(other, Timeseries):\n            return NotImplemented\n\n        if not super().__eq__(other):\n            return False\n\n        return self.sensor == other.sensor and self.sensor_alt == other.sensor_alt\n\n    def plot(\n        self,\n        include_outliers: bool = False,\n        ax: Axes | None = None,\n        plot_kwargs: dict[str, Any] | None = None,\n        legend_kwargs: dict[str, Any] | None = None,\n    ) -&gt; tuple[Figure, Axes]:\n        \"\"\"Plots the timeseries data.\n\n        Parameters:\n            include_outliers (bool): Whether to include outliers in the plot.\n            ax (matplotlib.axes.Axes, optional): Matplotlib axes object to plot on.\n                If None, a new figure and axes are created.\n            plot_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.plot() method to customize the plot.\n            legend_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.legend() to customize the legend.\n\n        Returns:\n            (fig, ax): Matplotlib figure and axes to allow further customization.\n        \"\"\"\n        fig, ax = super().plot(\n            include_outliers=include_outliers,\n            ax=ax,\n            plot_kwargs=plot_kwargs,\n            legend_kwargs=legend_kwargs,\n        )\n\n        ax.set_title(f\"{self.variable.capitalize()} at {self.location} ({self.sensor})\")\n\n        return fig, ax\n</code></pre>"},{"location":"modules/#gensor.core.timeseries.Timeseries.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check equality based on location, sensor, variable, unit and sensor_alt.</p> Source code in <code>gensor/core/timeseries.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"Check equality based on location, sensor, variable, unit and sensor_alt.\"\"\"\n    if not isinstance(other, Timeseries):\n        return NotImplemented\n\n    if not super().__eq__(other):\n        return False\n\n    return self.sensor == other.sensor and self.sensor_alt == other.sensor_alt\n</code></pre>"},{"location":"modules/#gensor.core.timeseries.Timeseries.plot","title":"<code>plot(include_outliers=False, ax=None, plot_kwargs=None, legend_kwargs=None)</code>","text":"<p>Plots the timeseries data.</p> <p>Parameters:</p> Name Type Description Default <code>include_outliers</code> <code>bool</code> <p>Whether to include outliers in the plot.</p> <code>False</code> <code>ax</code> <code>Axes</code> <p>Matplotlib axes object to plot on. If None, a new figure and axes are created.</p> <code>None</code> <code>plot_kwargs</code> <code>dict[str, Any] | None</code> <p>kwargs passed to matplotlib.axes.Axes.plot() method to customize the plot.</p> <code>None</code> <code>legend_kwargs</code> <code>dict[str, Any] | None</code> <p>kwargs passed to matplotlib.axes.Axes.legend() to customize the legend.</p> <code>None</code> <p>Returns:</p> Type Description <code>(fig, ax)</code> <p>Matplotlib figure and axes to allow further customization.</p> Source code in <code>gensor/core/timeseries.py</code> <pre><code>def plot(\n    self,\n    include_outliers: bool = False,\n    ax: Axes | None = None,\n    plot_kwargs: dict[str, Any] | None = None,\n    legend_kwargs: dict[str, Any] | None = None,\n) -&gt; tuple[Figure, Axes]:\n    \"\"\"Plots the timeseries data.\n\n    Parameters:\n        include_outliers (bool): Whether to include outliers in the plot.\n        ax (matplotlib.axes.Axes, optional): Matplotlib axes object to plot on.\n            If None, a new figure and axes are created.\n        plot_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.plot() method to customize the plot.\n        legend_kwargs (dict[str, Any] | None): kwargs passed to matplotlib.axes.Axes.legend() to customize the legend.\n\n    Returns:\n        (fig, ax): Matplotlib figure and axes to allow further customization.\n    \"\"\"\n    fig, ax = super().plot(\n        include_outliers=include_outliers,\n        ax=ax,\n        plot_kwargs=plot_kwargs,\n        legend_kwargs=legend_kwargs,\n    )\n\n    ax.set_title(f\"{self.variable.capitalize()} at {self.location} ({self.sensor})\")\n\n    return fig, ax\n</code></pre>"},{"location":"modules/#gensor.db","title":"<code>db</code>","text":""},{"location":"modules/#gensor.db--db","title":"DB","text":"<p>Module handling database connection in case saving and loading from SQLite database is used.</p> <p>Modules:</p> <pre><code>connection.py\n</code></pre>"},{"location":"modules/#gensor.db.DatabaseConnection","title":"<code>DatabaseConnection</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Database connection object. If no database exists at the specified path, it will be created. If no database is specified, an in-memory database will be used.</p> <p>Attributes     metadata (MetaData): SQLAlchemy metadata object.     db_directory (Path): Path to the database to connect to.     db_name (str): Name for the database to connect to.     engine (Engine | None): SQLAlchemy Engine instance.</p> Source code in <code>gensor/db/connection.py</code> <pre><code>class DatabaseConnection(pyd.BaseModel):\n    \"\"\"Database connection object.\n    If no database exists at the specified path, it will be created.\n    If no database is specified, an in-memory database will be used.\n\n    Attributes\n        metadata (MetaData): SQLAlchemy metadata object.\n        db_directory (Path): Path to the database to connect to.\n        db_name (str): Name for the database to connect to.\n        engine (Engine | None): SQLAlchemy Engine instance.\n    \"\"\"\n\n    model_config = pyd.ConfigDict(\n        arbitrary_types_allowed=True, validate_assignment=True\n    )\n\n    metadata: MetaData = MetaData()\n    db_directory: Path = Path.cwd()\n    db_name: str = \"gensor.db\"\n    engine: Engine | None = None\n\n    def _verify_path(self) -&gt; str:\n        \"\"\"Verify database path.\"\"\"\n\n        if not self.db_directory.exists():\n            raise DatabaseNotFound()\n        return f\"sqlite:///{self.db_directory}/{self.db_name}\"\n\n    def connect(self) -&gt; Connection:\n        \"\"\"Connect to the database and initialize the engine.\n        If engine is None &gt; create it with verified path &gt; reflect.\n        After connecting, ensure the timeseries_metadata table is present.\n        \"\"\"\n        if self.engine is None:\n            sqlite_path = self._verify_path()\n            self.engine = create_engine(sqlite_path)\n\n        connection = self.engine.connect()\n\n        self.create_metadata()\n\n        return connection\n\n    def dispose(self) -&gt; None:\n        \"\"\"Dispose of the engine, closing all connections.\"\"\"\n        if self.metadata:\n            self.metadata.clear()\n        if self.engine:\n            self.engine.dispose()\n\n    def __enter__(self) -&gt; Connection:\n        \"\"\"Enable usage in a `with` block by returning the engine.\"\"\"\n        con = self.connect()\n        if self.engine:\n            self.metadata.reflect(bind=self.engine)\n        return con\n\n    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -&gt; None:\n        \"\"\"Dispose of the engine when exiting the `with` block.\"\"\"\n        self.dispose()\n\n    def get_timeseries_metadata(\n        self,\n        location: str | None = None,\n        variable: str | None = None,\n        unit: str | None = None,\n        **kwargs: dict,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        List timeseries available in the database.\n\n        Parameters:\n            location (str): Location attribute to match.\n            variable (str): Variable attribute to match.\n            unit (str): Unit attribute to match.\n            **kwargs: Additional filters. Must match the attributes of the\n                Timeseries instance user is trying to retrieve.\n\n        Returns:\n            pd.DataFrame: The name of the matching table or None if no table is found.\n        \"\"\"\n        with self as con:\n            if \"__timeseries_metadata__\" not in self.metadata.tables:\n                logger.info(\"The metadata table does not exist in this database.\")\n                return pd.DataFrame()\n\n            metadata_table = self.metadata.tables[\"__timeseries_metadata__\"]\n\n            base_filters = []\n\n            if location is not None:\n                base_filters.append(metadata_table.c.location.ilike(location))\n            if variable is not None:\n                base_filters.append(metadata_table.c.variable.ilike(variable))\n            if unit is not None:\n                base_filters.append(metadata_table.c.unit.ilike(unit))\n\n            extra_filters = [\n                func.json_extract(metadata_table.c.extra, f\"$.{k}\").ilike(v)\n                for k, v in kwargs.items()\n                if v is not None\n            ]\n\n            # True in and_(True, *arg) fixis FutureWarning of dissallowing empty\n            # filters in the future.\n            query = metadata_table.select().where(\n                and_(True, *base_filters, *extra_filters)\n            )\n\n            result = con.execute(query).fetchall()\n\n            return pd.DataFrame(result).set_index(\"id\") if result else pd.DataFrame()\n\n    def create_metadata(self) -&gt; Table | None:\n        \"\"\"Create a metadata table if it doesn't exist yet and store ts metadata.\"\"\"\n\n        metadata_table = Table(\n            \"__timeseries_metadata__\",\n            self.metadata,\n            Column(\"id\", Integer, primary_key=True),\n            Column(\"table_name\", String, unique=True),\n            Column(\"location\", String),\n            Column(\"variable\", String),\n            Column(\"unit\", String),\n            Column(\"start\", String, nullable=True),\n            Column(\"end\", String, nullable=True),\n            Column(\"extra\", JSON, nullable=True),\n            Column(\"cls\", String, nullable=False),\n        )\n\n        if self.engine:\n            metadata_table.create(self.engine, checkfirst=True)\n            self.metadata.reflect(bind=self.engine)\n            return metadata_table\n        else:\n            logger.info(\"Engine does not exist.\")\n            return None\n\n    def create_table(self, schema_name: str, column_name: str) -&gt; Table | None:\n        \"\"\"Create a table in the database.\n\n        Schema name is a string representing the location, sensor, variable measured and\n        unit of measurement. This is a way of preserving the metadata of the Timeseries.\n        The index is always `timestamp` and the column name is dynamicly create from\n        the measured variable.\n        \"\"\"\n\n        if schema_name in self.metadata.tables:\n            return self.metadata.tables[schema_name]\n\n        ts_table = Table(\n            schema_name,\n            self.metadata,\n            Column(\"timestamp\", String, primary_key=True),\n            Column(column_name, Float),\n            info={},\n        )\n\n        if self.engine:\n            ts_table.create(self.engine, checkfirst=True)\n            self.metadata.reflect(bind=self.engine)\n            return ts_table\n        else:\n            logger.info(\"Engine does not exist.\")\n            return None\n</code></pre>"},{"location":"modules/#gensor.db.DatabaseConnection.__enter__","title":"<code>__enter__()</code>","text":"<p>Enable usage in a <code>with</code> block by returning the engine.</p> Source code in <code>gensor/db/connection.py</code> <pre><code>def __enter__(self) -&gt; Connection:\n    \"\"\"Enable usage in a `with` block by returning the engine.\"\"\"\n    con = self.connect()\n    if self.engine:\n        self.metadata.reflect(bind=self.engine)\n    return con\n</code></pre>"},{"location":"modules/#gensor.db.DatabaseConnection.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Dispose of the engine when exiting the <code>with</code> block.</p> Source code in <code>gensor/db/connection.py</code> <pre><code>def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -&gt; None:\n    \"\"\"Dispose of the engine when exiting the `with` block.\"\"\"\n    self.dispose()\n</code></pre>"},{"location":"modules/#gensor.db.DatabaseConnection.connect","title":"<code>connect()</code>","text":"<p>Connect to the database and initialize the engine. If engine is None &gt; create it with verified path &gt; reflect. After connecting, ensure the timeseries_metadata table is present.</p> Source code in <code>gensor/db/connection.py</code> <pre><code>def connect(self) -&gt; Connection:\n    \"\"\"Connect to the database and initialize the engine.\n    If engine is None &gt; create it with verified path &gt; reflect.\n    After connecting, ensure the timeseries_metadata table is present.\n    \"\"\"\n    if self.engine is None:\n        sqlite_path = self._verify_path()\n        self.engine = create_engine(sqlite_path)\n\n    connection = self.engine.connect()\n\n    self.create_metadata()\n\n    return connection\n</code></pre>"},{"location":"modules/#gensor.db.DatabaseConnection.create_metadata","title":"<code>create_metadata()</code>","text":"<p>Create a metadata table if it doesn't exist yet and store ts metadata.</p> Source code in <code>gensor/db/connection.py</code> <pre><code>def create_metadata(self) -&gt; Table | None:\n    \"\"\"Create a metadata table if it doesn't exist yet and store ts metadata.\"\"\"\n\n    metadata_table = Table(\n        \"__timeseries_metadata__\",\n        self.metadata,\n        Column(\"id\", Integer, primary_key=True),\n        Column(\"table_name\", String, unique=True),\n        Column(\"location\", String),\n        Column(\"variable\", String),\n        Column(\"unit\", String),\n        Column(\"start\", String, nullable=True),\n        Column(\"end\", String, nullable=True),\n        Column(\"extra\", JSON, nullable=True),\n        Column(\"cls\", String, nullable=False),\n    )\n\n    if self.engine:\n        metadata_table.create(self.engine, checkfirst=True)\n        self.metadata.reflect(bind=self.engine)\n        return metadata_table\n    else:\n        logger.info(\"Engine does not exist.\")\n        return None\n</code></pre>"},{"location":"modules/#gensor.db.DatabaseConnection.create_table","title":"<code>create_table(schema_name, column_name)</code>","text":"<p>Create a table in the database.</p> <p>Schema name is a string representing the location, sensor, variable measured and unit of measurement. This is a way of preserving the metadata of the Timeseries. The index is always <code>timestamp</code> and the column name is dynamicly create from the measured variable.</p> Source code in <code>gensor/db/connection.py</code> <pre><code>def create_table(self, schema_name: str, column_name: str) -&gt; Table | None:\n    \"\"\"Create a table in the database.\n\n    Schema name is a string representing the location, sensor, variable measured and\n    unit of measurement. This is a way of preserving the metadata of the Timeseries.\n    The index is always `timestamp` and the column name is dynamicly create from\n    the measured variable.\n    \"\"\"\n\n    if schema_name in self.metadata.tables:\n        return self.metadata.tables[schema_name]\n\n    ts_table = Table(\n        schema_name,\n        self.metadata,\n        Column(\"timestamp\", String, primary_key=True),\n        Column(column_name, Float),\n        info={},\n    )\n\n    if self.engine:\n        ts_table.create(self.engine, checkfirst=True)\n        self.metadata.reflect(bind=self.engine)\n        return ts_table\n    else:\n        logger.info(\"Engine does not exist.\")\n        return None\n</code></pre>"},{"location":"modules/#gensor.db.DatabaseConnection.dispose","title":"<code>dispose()</code>","text":"<p>Dispose of the engine, closing all connections.</p> Source code in <code>gensor/db/connection.py</code> <pre><code>def dispose(self) -&gt; None:\n    \"\"\"Dispose of the engine, closing all connections.\"\"\"\n    if self.metadata:\n        self.metadata.clear()\n    if self.engine:\n        self.engine.dispose()\n</code></pre>"},{"location":"modules/#gensor.db.DatabaseConnection.get_timeseries_metadata","title":"<code>get_timeseries_metadata(location=None, variable=None, unit=None, **kwargs)</code>","text":"<p>List timeseries available in the database.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>Location attribute to match.</p> <code>None</code> <code>variable</code> <code>str</code> <p>Variable attribute to match.</p> <code>None</code> <code>unit</code> <code>str</code> <p>Unit attribute to match.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional filters. Must match the attributes of the Timeseries instance user is trying to retrieve.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The name of the matching table or None if no table is found.</p> Source code in <code>gensor/db/connection.py</code> <pre><code>def get_timeseries_metadata(\n    self,\n    location: str | None = None,\n    variable: str | None = None,\n    unit: str | None = None,\n    **kwargs: dict,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    List timeseries available in the database.\n\n    Parameters:\n        location (str): Location attribute to match.\n        variable (str): Variable attribute to match.\n        unit (str): Unit attribute to match.\n        **kwargs: Additional filters. Must match the attributes of the\n            Timeseries instance user is trying to retrieve.\n\n    Returns:\n        pd.DataFrame: The name of the matching table or None if no table is found.\n    \"\"\"\n    with self as con:\n        if \"__timeseries_metadata__\" not in self.metadata.tables:\n            logger.info(\"The metadata table does not exist in this database.\")\n            return pd.DataFrame()\n\n        metadata_table = self.metadata.tables[\"__timeseries_metadata__\"]\n\n        base_filters = []\n\n        if location is not None:\n            base_filters.append(metadata_table.c.location.ilike(location))\n        if variable is not None:\n            base_filters.append(metadata_table.c.variable.ilike(variable))\n        if unit is not None:\n            base_filters.append(metadata_table.c.unit.ilike(unit))\n\n        extra_filters = [\n            func.json_extract(metadata_table.c.extra, f\"$.{k}\").ilike(v)\n            for k, v in kwargs.items()\n            if v is not None\n        ]\n\n        # True in and_(True, *arg) fixis FutureWarning of dissallowing empty\n        # filters in the future.\n        query = metadata_table.select().where(\n            and_(True, *base_filters, *extra_filters)\n        )\n\n        result = con.execute(query).fetchall()\n\n        return pd.DataFrame(result).set_index(\"id\") if result else pd.DataFrame()\n</code></pre>"},{"location":"modules/#gensor.db.connection","title":"<code>connection</code>","text":"<p>Module defining database connection object.</p> <p>Classes:</p> Name Description <code>DatabaseConnection</code> <p>Database connection object</p>"},{"location":"modules/#gensor.db.connection.DatabaseConnection","title":"<code>DatabaseConnection</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Database connection object. If no database exists at the specified path, it will be created. If no database is specified, an in-memory database will be used.</p> <p>Attributes     metadata (MetaData): SQLAlchemy metadata object.     db_directory (Path): Path to the database to connect to.     db_name (str): Name for the database to connect to.     engine (Engine | None): SQLAlchemy Engine instance.</p> Source code in <code>gensor/db/connection.py</code> <pre><code>class DatabaseConnection(pyd.BaseModel):\n    \"\"\"Database connection object.\n    If no database exists at the specified path, it will be created.\n    If no database is specified, an in-memory database will be used.\n\n    Attributes\n        metadata (MetaData): SQLAlchemy metadata object.\n        db_directory (Path): Path to the database to connect to.\n        db_name (str): Name for the database to connect to.\n        engine (Engine | None): SQLAlchemy Engine instance.\n    \"\"\"\n\n    model_config = pyd.ConfigDict(\n        arbitrary_types_allowed=True, validate_assignment=True\n    )\n\n    metadata: MetaData = MetaData()\n    db_directory: Path = Path.cwd()\n    db_name: str = \"gensor.db\"\n    engine: Engine | None = None\n\n    def _verify_path(self) -&gt; str:\n        \"\"\"Verify database path.\"\"\"\n\n        if not self.db_directory.exists():\n            raise DatabaseNotFound()\n        return f\"sqlite:///{self.db_directory}/{self.db_name}\"\n\n    def connect(self) -&gt; Connection:\n        \"\"\"Connect to the database and initialize the engine.\n        If engine is None &gt; create it with verified path &gt; reflect.\n        After connecting, ensure the timeseries_metadata table is present.\n        \"\"\"\n        if self.engine is None:\n            sqlite_path = self._verify_path()\n            self.engine = create_engine(sqlite_path)\n\n        connection = self.engine.connect()\n\n        self.create_metadata()\n\n        return connection\n\n    def dispose(self) -&gt; None:\n        \"\"\"Dispose of the engine, closing all connections.\"\"\"\n        if self.metadata:\n            self.metadata.clear()\n        if self.engine:\n            self.engine.dispose()\n\n    def __enter__(self) -&gt; Connection:\n        \"\"\"Enable usage in a `with` block by returning the engine.\"\"\"\n        con = self.connect()\n        if self.engine:\n            self.metadata.reflect(bind=self.engine)\n        return con\n\n    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -&gt; None:\n        \"\"\"Dispose of the engine when exiting the `with` block.\"\"\"\n        self.dispose()\n\n    def get_timeseries_metadata(\n        self,\n        location: str | None = None,\n        variable: str | None = None,\n        unit: str | None = None,\n        **kwargs: dict,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        List timeseries available in the database.\n\n        Parameters:\n            location (str): Location attribute to match.\n            variable (str): Variable attribute to match.\n            unit (str): Unit attribute to match.\n            **kwargs: Additional filters. Must match the attributes of the\n                Timeseries instance user is trying to retrieve.\n\n        Returns:\n            pd.DataFrame: The name of the matching table or None if no table is found.\n        \"\"\"\n        with self as con:\n            if \"__timeseries_metadata__\" not in self.metadata.tables:\n                logger.info(\"The metadata table does not exist in this database.\")\n                return pd.DataFrame()\n\n            metadata_table = self.metadata.tables[\"__timeseries_metadata__\"]\n\n            base_filters = []\n\n            if location is not None:\n                base_filters.append(metadata_table.c.location.ilike(location))\n            if variable is not None:\n                base_filters.append(metadata_table.c.variable.ilike(variable))\n            if unit is not None:\n                base_filters.append(metadata_table.c.unit.ilike(unit))\n\n            extra_filters = [\n                func.json_extract(metadata_table.c.extra, f\"$.{k}\").ilike(v)\n                for k, v in kwargs.items()\n                if v is not None\n            ]\n\n            # True in and_(True, *arg) fixis FutureWarning of dissallowing empty\n            # filters in the future.\n            query = metadata_table.select().where(\n                and_(True, *base_filters, *extra_filters)\n            )\n\n            result = con.execute(query).fetchall()\n\n            return pd.DataFrame(result).set_index(\"id\") if result else pd.DataFrame()\n\n    def create_metadata(self) -&gt; Table | None:\n        \"\"\"Create a metadata table if it doesn't exist yet and store ts metadata.\"\"\"\n\n        metadata_table = Table(\n            \"__timeseries_metadata__\",\n            self.metadata,\n            Column(\"id\", Integer, primary_key=True),\n            Column(\"table_name\", String, unique=True),\n            Column(\"location\", String),\n            Column(\"variable\", String),\n            Column(\"unit\", String),\n            Column(\"start\", String, nullable=True),\n            Column(\"end\", String, nullable=True),\n            Column(\"extra\", JSON, nullable=True),\n            Column(\"cls\", String, nullable=False),\n        )\n\n        if self.engine:\n            metadata_table.create(self.engine, checkfirst=True)\n            self.metadata.reflect(bind=self.engine)\n            return metadata_table\n        else:\n            logger.info(\"Engine does not exist.\")\n            return None\n\n    def create_table(self, schema_name: str, column_name: str) -&gt; Table | None:\n        \"\"\"Create a table in the database.\n\n        Schema name is a string representing the location, sensor, variable measured and\n        unit of measurement. This is a way of preserving the metadata of the Timeseries.\n        The index is always `timestamp` and the column name is dynamicly create from\n        the measured variable.\n        \"\"\"\n\n        if schema_name in self.metadata.tables:\n            return self.metadata.tables[schema_name]\n\n        ts_table = Table(\n            schema_name,\n            self.metadata,\n            Column(\"timestamp\", String, primary_key=True),\n            Column(column_name, Float),\n            info={},\n        )\n\n        if self.engine:\n            ts_table.create(self.engine, checkfirst=True)\n            self.metadata.reflect(bind=self.engine)\n            return ts_table\n        else:\n            logger.info(\"Engine does not exist.\")\n            return None\n</code></pre>"},{"location":"modules/#gensor.db.connection.DatabaseConnection.__enter__","title":"<code>__enter__()</code>","text":"<p>Enable usage in a <code>with</code> block by returning the engine.</p> Source code in <code>gensor/db/connection.py</code> <pre><code>def __enter__(self) -&gt; Connection:\n    \"\"\"Enable usage in a `with` block by returning the engine.\"\"\"\n    con = self.connect()\n    if self.engine:\n        self.metadata.reflect(bind=self.engine)\n    return con\n</code></pre>"},{"location":"modules/#gensor.db.connection.DatabaseConnection.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Dispose of the engine when exiting the <code>with</code> block.</p> Source code in <code>gensor/db/connection.py</code> <pre><code>def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -&gt; None:\n    \"\"\"Dispose of the engine when exiting the `with` block.\"\"\"\n    self.dispose()\n</code></pre>"},{"location":"modules/#gensor.db.connection.DatabaseConnection.connect","title":"<code>connect()</code>","text":"<p>Connect to the database and initialize the engine. If engine is None &gt; create it with verified path &gt; reflect. After connecting, ensure the timeseries_metadata table is present.</p> Source code in <code>gensor/db/connection.py</code> <pre><code>def connect(self) -&gt; Connection:\n    \"\"\"Connect to the database and initialize the engine.\n    If engine is None &gt; create it with verified path &gt; reflect.\n    After connecting, ensure the timeseries_metadata table is present.\n    \"\"\"\n    if self.engine is None:\n        sqlite_path = self._verify_path()\n        self.engine = create_engine(sqlite_path)\n\n    connection = self.engine.connect()\n\n    self.create_metadata()\n\n    return connection\n</code></pre>"},{"location":"modules/#gensor.db.connection.DatabaseConnection.create_metadata","title":"<code>create_metadata()</code>","text":"<p>Create a metadata table if it doesn't exist yet and store ts metadata.</p> Source code in <code>gensor/db/connection.py</code> <pre><code>def create_metadata(self) -&gt; Table | None:\n    \"\"\"Create a metadata table if it doesn't exist yet and store ts metadata.\"\"\"\n\n    metadata_table = Table(\n        \"__timeseries_metadata__\",\n        self.metadata,\n        Column(\"id\", Integer, primary_key=True),\n        Column(\"table_name\", String, unique=True),\n        Column(\"location\", String),\n        Column(\"variable\", String),\n        Column(\"unit\", String),\n        Column(\"start\", String, nullable=True),\n        Column(\"end\", String, nullable=True),\n        Column(\"extra\", JSON, nullable=True),\n        Column(\"cls\", String, nullable=False),\n    )\n\n    if self.engine:\n        metadata_table.create(self.engine, checkfirst=True)\n        self.metadata.reflect(bind=self.engine)\n        return metadata_table\n    else:\n        logger.info(\"Engine does not exist.\")\n        return None\n</code></pre>"},{"location":"modules/#gensor.db.connection.DatabaseConnection.create_table","title":"<code>create_table(schema_name, column_name)</code>","text":"<p>Create a table in the database.</p> <p>Schema name is a string representing the location, sensor, variable measured and unit of measurement. This is a way of preserving the metadata of the Timeseries. The index is always <code>timestamp</code> and the column name is dynamicly create from the measured variable.</p> Source code in <code>gensor/db/connection.py</code> <pre><code>def create_table(self, schema_name: str, column_name: str) -&gt; Table | None:\n    \"\"\"Create a table in the database.\n\n    Schema name is a string representing the location, sensor, variable measured and\n    unit of measurement. This is a way of preserving the metadata of the Timeseries.\n    The index is always `timestamp` and the column name is dynamicly create from\n    the measured variable.\n    \"\"\"\n\n    if schema_name in self.metadata.tables:\n        return self.metadata.tables[schema_name]\n\n    ts_table = Table(\n        schema_name,\n        self.metadata,\n        Column(\"timestamp\", String, primary_key=True),\n        Column(column_name, Float),\n        info={},\n    )\n\n    if self.engine:\n        ts_table.create(self.engine, checkfirst=True)\n        self.metadata.reflect(bind=self.engine)\n        return ts_table\n    else:\n        logger.info(\"Engine does not exist.\")\n        return None\n</code></pre>"},{"location":"modules/#gensor.db.connection.DatabaseConnection.dispose","title":"<code>dispose()</code>","text":"<p>Dispose of the engine, closing all connections.</p> Source code in <code>gensor/db/connection.py</code> <pre><code>def dispose(self) -&gt; None:\n    \"\"\"Dispose of the engine, closing all connections.\"\"\"\n    if self.metadata:\n        self.metadata.clear()\n    if self.engine:\n        self.engine.dispose()\n</code></pre>"},{"location":"modules/#gensor.db.connection.DatabaseConnection.get_timeseries_metadata","title":"<code>get_timeseries_metadata(location=None, variable=None, unit=None, **kwargs)</code>","text":"<p>List timeseries available in the database.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>Location attribute to match.</p> <code>None</code> <code>variable</code> <code>str</code> <p>Variable attribute to match.</p> <code>None</code> <code>unit</code> <code>str</code> <p>Unit attribute to match.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional filters. Must match the attributes of the Timeseries instance user is trying to retrieve.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The name of the matching table or None if no table is found.</p> Source code in <code>gensor/db/connection.py</code> <pre><code>def get_timeseries_metadata(\n    self,\n    location: str | None = None,\n    variable: str | None = None,\n    unit: str | None = None,\n    **kwargs: dict,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    List timeseries available in the database.\n\n    Parameters:\n        location (str): Location attribute to match.\n        variable (str): Variable attribute to match.\n        unit (str): Unit attribute to match.\n        **kwargs: Additional filters. Must match the attributes of the\n            Timeseries instance user is trying to retrieve.\n\n    Returns:\n        pd.DataFrame: The name of the matching table or None if no table is found.\n    \"\"\"\n    with self as con:\n        if \"__timeseries_metadata__\" not in self.metadata.tables:\n            logger.info(\"The metadata table does not exist in this database.\")\n            return pd.DataFrame()\n\n        metadata_table = self.metadata.tables[\"__timeseries_metadata__\"]\n\n        base_filters = []\n\n        if location is not None:\n            base_filters.append(metadata_table.c.location.ilike(location))\n        if variable is not None:\n            base_filters.append(metadata_table.c.variable.ilike(variable))\n        if unit is not None:\n            base_filters.append(metadata_table.c.unit.ilike(unit))\n\n        extra_filters = [\n            func.json_extract(metadata_table.c.extra, f\"$.{k}\").ilike(v)\n            for k, v in kwargs.items()\n            if v is not None\n        ]\n\n        # True in and_(True, *arg) fixis FutureWarning of dissallowing empty\n        # filters in the future.\n        query = metadata_table.select().where(\n            and_(True, *base_filters, *extra_filters)\n        )\n\n        result = con.execute(query).fetchall()\n\n        return pd.DataFrame(result).set_index(\"id\") if result else pd.DataFrame()\n</code></pre>"},{"location":"modules/#gensor.exceptions","title":"<code>exceptions</code>","text":""},{"location":"modules/#gensor.exceptions.IndexOutOfRangeError","title":"<code>IndexOutOfRangeError</code>","text":"<p>               Bases: <code>IndexError</code></p> <p>Custom exception raised when an index is out of range in the dataset.</p> Source code in <code>gensor/exceptions.py</code> <pre><code>class IndexOutOfRangeError(IndexError):\n    \"\"\"Custom exception raised when an index is out of range in the dataset.\"\"\"\n\n    def __init__(self, index: int, dataset_size: int) -&gt; None:\n        super().__init__(\n            f\"Index {index} is out of range for the dataset with {dataset_size} timeseries.\"\n        )\n</code></pre>"},{"location":"modules/#gensor.exceptions.InvalidMeasurementTypeError","title":"<code>InvalidMeasurementTypeError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Raised when a timeseries of a wrong measurement type is operated upon.</p> Source code in <code>gensor/exceptions.py</code> <pre><code>class InvalidMeasurementTypeError(ValueError):\n    \"\"\"Raised when a timeseries of a wrong measurement type is operated upon.\"\"\"\n\n    def __init__(self, expected_type: str = \"pressure\") -&gt; None:\n        self.expected_type = expected_type\n        message = f\"Timeseries must be of measurement type '{self.expected_type}'.\"\n        super().__init__(message)\n</code></pre>"},{"location":"modules/#gensor.exceptions.MissingInputError","title":"<code>MissingInputError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Raised when a required input is missing.</p> Source code in <code>gensor/exceptions.py</code> <pre><code>class MissingInputError(ValueError):\n    \"\"\"Raised when a required input is missing.\"\"\"\n\n    def __init__(self, input_name: str, message: str | None = None) -&gt; None:\n        self.input_name = input_name\n        if message is None:\n            message = f\"Missing required input: '{self.input_name}'.\"\n        super().__init__(message)\n</code></pre>"},{"location":"modules/#gensor.exceptions.TimeseriesUnequal","title":"<code>TimeseriesUnequal</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Raised when Timeseries objects are compared and are unequal.</p> Source code in <code>gensor/exceptions.py</code> <pre><code>class TimeseriesUnequal(ValueError):\n    \"\"\"Raised when Timeseries objects are compared and are unequal.\"\"\"\n\n    def __init__(self, *args: object, message: str | None = None) -&gt; None:\n        message = (\n            \"Timeseries objects must have the same location, sensor, variable, and \\\n        unit to be added together.\"\n        )\n        super().__init__(message, *args)\n</code></pre>"},{"location":"modules/#gensor.io","title":"<code>io</code>","text":""},{"location":"modules/#gensor.io.read","title":"<code>read</code>","text":"<p>Fetching the data from various sources.</p> <p>TODO: Fix up the read_from_sql() function to actually work properly.</p>"},{"location":"modules/#gensor.io.read.read_from_api","title":"<code>read_from_api()</code>","text":"<p>Fetch data from the API.</p> Source code in <code>gensor/io/read.py</code> <pre><code>def read_from_api() -&gt; Dataset:\n    \"\"\"Fetch data from the API.\"\"\"\n    return NotImplemented\n</code></pre>"},{"location":"modules/#gensor.io.read.read_from_csv","title":"<code>read_from_csv(path, file_format='vanessen', **kwargs)</code>","text":"<p>Loads the data from csv files with given file_format and returns a list of Timeseries objects.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the file or directory containing the files.</p> required <code>**kwargs</code> <code>dict</code> <p>Optional keyword arguments passed to the parsers: * serial_number_pattern (str): The regex pattern to extract the serial number from the file. * location_pattern (str): The regex pattern to extract the station from the file. * col_names (list): The column names for the dataframe. * location (str): Name of the location of the timeseries. * sensor (str): Sensor serial number.</p> <code>{}</code> Source code in <code>gensor/io/read.py</code> <pre><code>def read_from_csv(\n    path: Path, file_format: Literal[\"vanessen\", \"plain\"] = \"vanessen\", **kwargs: Any\n) -&gt; Dataset | Timeseries:\n    \"\"\"Loads the data from csv files with given file_format and returns a list of Timeseries objects.\n\n    Parameters:\n        path (Path): The path to the file or directory containing the files.\n        **kwargs (dict): Optional keyword arguments passed to the parsers:\n            * serial_number_pattern (str): The regex pattern to extract the serial number from the file.\n            * location_pattern (str): The regex pattern to extract the station from the file.\n            * col_names (list): The column names for the dataframe.\n            * location (str): Name of the location of the timeseries.\n            * sensor (str): Sensor serial number.\n    \"\"\"\n\n    parsers = {\n        \"vanessen\": parse_vanessen_csv,\n        \"plain\": parse_plain,\n        # more parser to be implemented\n    }\n\n    if not isinstance(path, Path):\n        message = \"The path argument must be a Path object.\"\n        raise TypeError(message)\n\n    if path.is_dir() and not any(\n        file.is_file() and file.suffix.lower() == \".csv\" for file in path.iterdir()\n    ):\n        logger.info(\"No CSV files found. Operation skipped.\")\n        return Dataset()\n\n    files = (\n        [\n            file\n            for file in path.iterdir()\n            if file.is_file() and file.suffix.lower() == \".csv\"\n        ]\n        if path.is_dir()\n        else [path]\n        if path.suffix.lower() == \".csv\"\n        else []\n    )\n\n    if not files:\n        logger.info(\"No CSV files found. Operation skipped.\")\n        return Dataset()\n\n    parser = parsers[file_format]\n\n    ds: Dataset = Dataset()\n\n    for f in files:\n        logger.info(f\"Loading file: {f}\")\n        ts_in_file = parser(f, **kwargs)\n        ds.add(ts_in_file)\n\n    # If there is only one Timeseries in Dataset (as in the condition), ds[0] will always\n    # be a Timeseries; so the line below does not introduce potential None in the return\n    return ds[0] if len(ds) == 1 else ds  # type: ignore[return-value]\n</code></pre>"},{"location":"modules/#gensor.io.read.read_from_sql","title":"<code>read_from_sql(db, load_all=True, location=None, variable=None, unit=None, timestamp_start=None, timestamp_stop=None, **kwargs)</code>","text":"<p>Returns the timeseries or a dataset from a SQL database.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>DatabaseConnection</code> <p>The database connection object.</p> required <code>load_all</code> <code>bool</code> <p>Whether to load all timeseries from the database.</p> <code>True</code> <code>location</code> <code>str</code> <p>The station name.</p> <code>None</code> <code>variable</code> <code>str</code> <p>The measurement type.</p> <code>None</code> <code>unit</code> <code>str</code> <p>The unit of the measurement.</p> <code>None</code> <code>timestamp_start</code> <code>Timestamp</code> <p>Start timestamp filter.</p> <code>None</code> <code>timestamp_stop</code> <code>Timestamp</code> <p>End timestamp filter.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Any additional filters matching attributes of the particular timeseries.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Timeseries | Dataset</code> <p>Dataset with retrieved objects or an empty Dataset.</p> Source code in <code>gensor/io/read.py</code> <pre><code>def read_from_sql(\n    db: DatabaseConnection,\n    load_all: bool = True,\n    location: str | None = None,\n    variable: str | None = None,\n    unit: str | None = None,\n    timestamp_start: pd.Timestamp | None = None,\n    timestamp_stop: pd.Timestamp | None = None,\n    **kwargs: dict,\n) -&gt; Timeseries | Dataset:\n    \"\"\"Returns the timeseries or a dataset from a SQL database.\n\n    Parameters:\n        db (DatabaseConnection): The database connection object.\n        load_all (bool): Whether to load all timeseries from the database.\n        location (str): The station name.\n        variable (str): The measurement type.\n        unit (str): The unit of the measurement.\n        timestamp_start (pd.Timestamp, optional): Start timestamp filter.\n        timestamp_stop (pd.Timestamp, optional): End timestamp filter.\n        **kwargs (dict): Any additional filters matching attributes of the particular\n            timeseries.\n\n    Returns:\n        Dataset: Dataset with retrieved objects or an empty Dataset.\n    \"\"\"\n\n    def _read_data_from_schema(schema_name: str) -&gt; Any:\n        \"\"\"Read data from the table and apply the timestamp filter.\n\n        Parameters:\n            schema_name (str): name of the schema in SQLite database.\n\n        Returns:\n            pd.Series: results of the query or an empty pd.Series if none are found.\n        \"\"\"\n        with db as con:\n            schema = db.metadata.tables[schema_name]\n            data_query = select(schema)\n\n            if timestamp_start or timestamp_stop:\n                if timestamp_start:\n                    data_query = data_query.where(schema.c.timestamp &gt;= timestamp_start)\n                if timestamp_stop:\n                    data_query = data_query.where(schema.c.timestamp &lt;= timestamp_stop)\n\n            ts = pd.read_sql(\n                data_query,\n                con=con,\n                parse_dates={\"timestamp\": \"%Y-%m-%dT%H:%M:%S%z\"},\n                index_col=\"timestamp\",\n            ).squeeze()\n\n        if ts.empty:\n            message = f\"No data found in table {schema_name}\"\n            logger.warning(message)\n\n        return ts\n\n    def _create_object(data: pd.Series, metadata: dict) -&gt; Any:\n        \"\"\"Create the appropriate object for timeseries.\"\"\"\n\n        core_metadata = {\n            \"location\": metadata[\"location\"],\n            \"variable\": metadata[\"variable\"],\n            \"unit\": metadata[\"unit\"],\n        }\n\n        extra_metadata = metadata.get(\"extra\", {})\n\n        ts_metadata = {**core_metadata, **extra_metadata}\n\n        cls = metadata[\"cls\"]\n        module_name, class_name = cls.rsplit(\".\", 1)\n        module = import_module(module_name)\n\n        TimeseriesClass = getattr(module, class_name)\n        ts_object = TimeseriesClass(ts=data, **ts_metadata)\n\n        return ts_object\n\n    metadata_df = (\n        db.get_timeseries_metadata(\n            location=location, variable=variable, unit=unit, **kwargs\n        )\n        if not load_all\n        else db.get_timeseries_metadata()\n    )\n\n    if metadata_df.empty:\n        message = \"No schemas matched the specified filters.\"\n        raise ValueError(message)\n\n    timeseries_list = []\n\n    for row in metadata_df.to_dict(orient=\"records\"):\n        try:\n            schema_name = row.pop(\"table_name\")\n            data = _read_data_from_schema(schema_name)\n            timeseries_obj = _create_object(data, row)\n            timeseries_list.append(timeseries_obj)\n        except (ValueError, TypeError):\n            logger.exception(f\"Skipping schema {schema_name} due to error.\")\n\n    return Dataset(timeseries=timeseries_list) if timeseries_list else Dataset()\n</code></pre>"},{"location":"modules/#gensor.log","title":"<code>log</code>","text":""},{"location":"modules/#gensor.log.set_log_level","title":"<code>set_log_level(level)</code>","text":"<p>Set the logging level for the package.</p> Source code in <code>gensor/log.py</code> <pre><code>def set_log_level(level: str) -&gt; None:\n    \"\"\"Set the logging level for the package.\"\"\"\n    logger = logging.getLogger(\"gensor\")\n    logger.setLevel(level.upper())\n</code></pre>"},{"location":"modules/#gensor.parse","title":"<code>parse</code>","text":""},{"location":"modules/#gensor.parse.parse_plain","title":"<code>parse_plain(path, **kwargs)</code>","text":"<p>Parse a simple csv without metadata header, just columns with variables</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the file.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[Timeseries]</code> <p>A list of Timeseries objects.</p> Source code in <code>gensor/parse/plain.py</code> <pre><code>def parse_plain(path: Path, **kwargs: Any) -&gt; list[Timeseries]:\n    \"\"\"Parse a simple csv without metadata header, just columns with variables\n\n    Parameters:\n        path (Path): The path to the file.\n\n    Returns:\n        list: A list of Timeseries objects.\n    \"\"\"\n\n    column_names = kwargs.get(\"col_names\", [\"timestamp\", \"pressure\", \"temperature\"])\n\n    encoding = detect_encoding(path, num_bytes=10_000)\n\n    df = read_csv(\n        path,\n        encoding=encoding,\n        skipfooter=1,\n        skip_blank_lines=True,\n        header=None,\n        skiprows=1,\n        index_col=\"timestamp\",\n        names=column_names,\n        engine=\"python\",\n    )\n\n    df = handle_timestamps(df, kwargs.get(\"timezone\", \"UTC\"))\n\n    ts_list = []\n\n    for col in df.columns:\n        if col in VARIABLE_TYPES_AND_UNITS:\n            unit = VARIABLE_TYPES_AND_UNITS[col][0]\n            ts_list.append(\n                Timeseries(\n                    ts=df[col],\n                    # Validation will be done in Pydantic\n                    variable=col,  # type: ignore[arg-type]\n                    location=kwargs[\"location\"],\n                    sensor=kwargs[\"sensor\"],\n                    # Validation will be done in Pydantic\n                    unit=unit,  # type: ignore[arg-type]\n                )\n            )\n        else:\n            message = (\n                \"Unsupported variable: {col}. Please provide a valid variable type.\"\n            )\n            raise ValueError(message)\n\n    return ts_list\n</code></pre>"},{"location":"modules/#gensor.parse.parse_vanessen_csv","title":"<code>parse_vanessen_csv(path, **kwargs)</code>","text":"<p>Parses a van Essen csv file and returns a list of Timeseries objects. At this point it does not matter whether the file is a barometric or piezometric logger file.</p> <p>The function will use regex patterns to extract the serial number and station from the file. It is important to use the appropriate regex patterns, particularily for the station. If the default patterns are not working (whihc most likely will be the case), the user should provide their own patterns. The patterns can be provided as keyword arguments to the function and it is possible to use OR (|) in the regex pattern.</p> <p>Warning</p> <p>A better check for the variable type and units has to be implemented.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the file.</p> required <p>Other Parameters:</p> Name Type Description <code>serial_number_pattern</code> <code>str</code> <p>The regex pattern to extract the serial number from the file.</p> <code>location_pattern</code> <code>str</code> <p>The regex pattern to extract the station from the file.</p> <code>col_names</code> <code>list</code> <p>The column names for the dataframe.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list[Timeseries]</code> <p>A list of Timeseries objects.</p> Source code in <code>gensor/parse/vanessen.py</code> <pre><code>def parse_vanessen_csv(path: Path, **kwargs: Any) -&gt; list[Timeseries]:\n    \"\"\"Parses a van Essen csv file and returns a list of Timeseries objects. At this point it\n    does not matter whether the file is a barometric or piezometric logger file.\n\n    The function will use regex patterns to extract the serial number and station from the file. It is\n    important to use the appropriate regex patterns, particularily for the station. If the default patterns\n    are not working (whihc most likely will be the case), the user should provide their own patterns. The patterns\n    can be provided as keyword arguments to the function and it is possible to use OR (|) in the regex pattern.\n\n    !!! warning\n\n        A better check for the variable type and units has to be implemented.\n\n    Parameters:\n        path (Path): The path to the file.\n\n    Other Parameters:\n        serial_number_pattern (str): The regex pattern to extract the serial number from the file.\n        location_pattern (str): The regex pattern to extract the station from the file.\n        col_names (list): The column names for the dataframe.\n\n    Returns:\n        list: A list of Timeseries objects.\n    \"\"\"\n\n    patterns = {\n        \"sensor\": kwargs.get(\"serial_number_pattern\", r\"[A-Za-z]{2}\\d{3,4}\"),\n        \"location\": kwargs.get(\n            \"location_pattern\", r\"[A-Za-z]{2}\\d{2}[A-Za-z]{1}|Barodiver\"\n        ),\n        \"timezone\": kwargs.get(\"timezone_pattern\", r\"UTC[+-]?\\d+\"),\n    }\n\n    column_names = kwargs.get(\"col_names\", [\"timestamp\", \"pressure\", \"temperature\"])\n\n    encoding = detect_encoding(path, num_bytes=10_000)\n\n    with path.open(mode=\"r\", encoding=encoding) as f:\n        text = f.read()\n\n        metadata = get_metadata(text, patterns)\n\n        if not metadata:\n            logger.info(f\"Skipping file {path} due to missing metadata.\")\n            return []\n\n        data_start = \"Date/time\"\n        data_end = \"END OF DATA FILE\"\n\n        df = get_data(text, data_start, data_end, column_names)\n\n        df = handle_timestamps(df, metadata.get(\"timezone\", \"UTC\"))\n\n        ts_list = []\n\n        for col in df.columns:\n            if col in VARIABLE_TYPES_AND_UNITS:\n                unit = VARIABLE_TYPES_AND_UNITS[col][0]\n                ts_list.append(\n                    Timeseries(\n                        ts=df[col],\n                        # Validation will be done in Pydantic\n                        variable=col,  # type: ignore[arg-type]\n                        location=metadata.get(\"location\"),\n                        sensor=metadata.get(\"sensor\"),\n                        # Validation will be done in Pydantic\n                        unit=unit,  # type: ignore[arg-type]\n                    )\n                )\n            else:\n                message = (\n                    \"Unsupported variable: {col}. Please provide a valid variable type.\"\n                )\n                raise ValueError(message)\n\n    return ts_list\n</code></pre>"},{"location":"modules/#gensor.parse.plain","title":"<code>plain</code>","text":""},{"location":"modules/#gensor.parse.plain.parse_plain","title":"<code>parse_plain(path, **kwargs)</code>","text":"<p>Parse a simple csv without metadata header, just columns with variables</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the file.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[Timeseries]</code> <p>A list of Timeseries objects.</p> Source code in <code>gensor/parse/plain.py</code> <pre><code>def parse_plain(path: Path, **kwargs: Any) -&gt; list[Timeseries]:\n    \"\"\"Parse a simple csv without metadata header, just columns with variables\n\n    Parameters:\n        path (Path): The path to the file.\n\n    Returns:\n        list: A list of Timeseries objects.\n    \"\"\"\n\n    column_names = kwargs.get(\"col_names\", [\"timestamp\", \"pressure\", \"temperature\"])\n\n    encoding = detect_encoding(path, num_bytes=10_000)\n\n    df = read_csv(\n        path,\n        encoding=encoding,\n        skipfooter=1,\n        skip_blank_lines=True,\n        header=None,\n        skiprows=1,\n        index_col=\"timestamp\",\n        names=column_names,\n        engine=\"python\",\n    )\n\n    df = handle_timestamps(df, kwargs.get(\"timezone\", \"UTC\"))\n\n    ts_list = []\n\n    for col in df.columns:\n        if col in VARIABLE_TYPES_AND_UNITS:\n            unit = VARIABLE_TYPES_AND_UNITS[col][0]\n            ts_list.append(\n                Timeseries(\n                    ts=df[col],\n                    # Validation will be done in Pydantic\n                    variable=col,  # type: ignore[arg-type]\n                    location=kwargs[\"location\"],\n                    sensor=kwargs[\"sensor\"],\n                    # Validation will be done in Pydantic\n                    unit=unit,  # type: ignore[arg-type]\n                )\n            )\n        else:\n            message = (\n                \"Unsupported variable: {col}. Please provide a valid variable type.\"\n            )\n            raise ValueError(message)\n\n    return ts_list\n</code></pre>"},{"location":"modules/#gensor.parse.utils","title":"<code>utils</code>","text":""},{"location":"modules/#gensor.parse.utils.detect_encoding","title":"<code>detect_encoding(path, num_bytes=1024)</code>","text":"<p>Detect the encoding of a file using chardet.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the file.</p> required <code>num_bytes</code> <code>int</code> <p>Number of bytes to read for encoding detection (default is 1024).</p> <code>1024</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The detected encoding of the file.</p> Source code in <code>gensor/parse/utils.py</code> <pre><code>def detect_encoding(path: Path, num_bytes: int = 1024) -&gt; str:\n    \"\"\"Detect the encoding of a file using chardet.\n\n    Parameters:\n        path (Path): The path to the file.\n        num_bytes (int): Number of bytes to read for encoding detection (default is 1024).\n\n    Returns:\n        str: The detected encoding of the file.\n    \"\"\"\n    with path.open(\"rb\") as f:\n        raw_data = f.read(num_bytes)\n    result = chardet.detect(raw_data)\n    return result[\"encoding\"] or \"utf-8\"\n</code></pre>"},{"location":"modules/#gensor.parse.utils.get_data","title":"<code>get_data(text, data_start, data_end, column_names)</code>","text":"<p>Search for data in the file.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>string obtained from the CSV file.</p> required <code>data_start</code> <code>str</code> <p>string at the first row of the data.</p> required <code>data_end</code> <code>str</code> <p>string at the last row of the data.</p> required <code>column_names</code> <code>list</code> <p>list of expected column names.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame</p> Source code in <code>gensor/parse/utils.py</code> <pre><code>def get_data(\n    text: str, data_start: str, data_end: str, column_names: list\n) -&gt; DataFrame:\n    \"\"\"Search for data in the file.\n\n    Parameters:\n        text (str): string obtained from the CSV file.\n        data_start (str): string at the first row of the data.\n        data_end (str): string at the last row of the data.\n        column_names (list): list of expected column names.\n\n    Returns:\n        pd.DataFrame\n    \"\"\"\n\n    data_io = StringIO(text[text.index(data_start) : text.index(data_end)])\n\n    df = read_csv(\n        data_io, skiprows=1, header=None, names=column_names, index_col=\"timestamp\"\n    )\n\n    return df\n</code></pre>"},{"location":"modules/#gensor.parse.utils.get_metadata","title":"<code>get_metadata(text, patterns)</code>","text":"<p>Search for metadata in the file header with given regex patterns.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>string obtained from the CSV file.</p> required <code>patterns</code> <code>dict</code> <p>regex patterns matching the location and sensor information.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>metadata of the timeseries.</p> Source code in <code>gensor/parse/utils.py</code> <pre><code>def get_metadata(text: str, patterns: dict) -&gt; dict:\n    \"\"\"Search for metadata in the file header with given regex patterns.\n\n    Parameters:\n        text (str): string obtained from the CSV file.\n        patterns (dict): regex patterns matching the location and sensor information.\n\n    Returns:\n        dict: metadata of the timeseries.\n    \"\"\"\n    metadata = {}\n\n    for k, v in patterns.items():\n        match = re.search(v, text)\n        metadata[k] = match.group() if match else None\n\n    if metadata[\"sensor\"] is None or metadata[\"location\"] is None:\n        return {}\n\n    return metadata\n</code></pre>"},{"location":"modules/#gensor.parse.utils.handle_timestamps","title":"<code>handle_timestamps(df, tz_string)</code>","text":"<p>Converts timestamps in the dataframe to the specified timezone (e.g., 'UTC+1').</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe with timestamps.</p> required <code>tz_string</code> <code>str</code> <p>A timezone string like 'UTC+1' or 'UTC-5'.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The dataframe with timestamps converted to UTC.</p> Source code in <code>gensor/parse/utils.py</code> <pre><code>def handle_timestamps(df: DataFrame, tz_string: str) -&gt; DataFrame:\n    \"\"\"Converts timestamps in the dataframe to the specified timezone (e.g., 'UTC+1').\n\n    Parameters:\n        df (pd.DataFrame): The dataframe with timestamps.\n        tz_string (str): A timezone string like 'UTC+1' or 'UTC-5'.\n\n    Returns:\n        pd.DataFrame: The dataframe with timestamps converted to UTC.\n    \"\"\"\n    timezone = tz.gettz(tz_string)\n\n    df.index = to_datetime(df.index).tz_localize(timezone)\n    df.index = df.index.tz_convert(\"UTC\")\n\n    return df\n</code></pre>"},{"location":"modules/#gensor.parse.vanessen","title":"<code>vanessen</code>","text":"<p>Logic parsing CSV files from van Essen Instruments Divers.</p>"},{"location":"modules/#gensor.parse.vanessen.parse_vanessen_csv","title":"<code>parse_vanessen_csv(path, **kwargs)</code>","text":"<p>Parses a van Essen csv file and returns a list of Timeseries objects. At this point it does not matter whether the file is a barometric or piezometric logger file.</p> <p>The function will use regex patterns to extract the serial number and station from the file. It is important to use the appropriate regex patterns, particularily for the station. If the default patterns are not working (whihc most likely will be the case), the user should provide their own patterns. The patterns can be provided as keyword arguments to the function and it is possible to use OR (|) in the regex pattern.</p> <p>Warning</p> <p>A better check for the variable type and units has to be implemented.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the file.</p> required <p>Other Parameters:</p> Name Type Description <code>serial_number_pattern</code> <code>str</code> <p>The regex pattern to extract the serial number from the file.</p> <code>location_pattern</code> <code>str</code> <p>The regex pattern to extract the station from the file.</p> <code>col_names</code> <code>list</code> <p>The column names for the dataframe.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list[Timeseries]</code> <p>A list of Timeseries objects.</p> Source code in <code>gensor/parse/vanessen.py</code> <pre><code>def parse_vanessen_csv(path: Path, **kwargs: Any) -&gt; list[Timeseries]:\n    \"\"\"Parses a van Essen csv file and returns a list of Timeseries objects. At this point it\n    does not matter whether the file is a barometric or piezometric logger file.\n\n    The function will use regex patterns to extract the serial number and station from the file. It is\n    important to use the appropriate regex patterns, particularily for the station. If the default patterns\n    are not working (whihc most likely will be the case), the user should provide their own patterns. The patterns\n    can be provided as keyword arguments to the function and it is possible to use OR (|) in the regex pattern.\n\n    !!! warning\n\n        A better check for the variable type and units has to be implemented.\n\n    Parameters:\n        path (Path): The path to the file.\n\n    Other Parameters:\n        serial_number_pattern (str): The regex pattern to extract the serial number from the file.\n        location_pattern (str): The regex pattern to extract the station from the file.\n        col_names (list): The column names for the dataframe.\n\n    Returns:\n        list: A list of Timeseries objects.\n    \"\"\"\n\n    patterns = {\n        \"sensor\": kwargs.get(\"serial_number_pattern\", r\"[A-Za-z]{2}\\d{3,4}\"),\n        \"location\": kwargs.get(\n            \"location_pattern\", r\"[A-Za-z]{2}\\d{2}[A-Za-z]{1}|Barodiver\"\n        ),\n        \"timezone\": kwargs.get(\"timezone_pattern\", r\"UTC[+-]?\\d+\"),\n    }\n\n    column_names = kwargs.get(\"col_names\", [\"timestamp\", \"pressure\", \"temperature\"])\n\n    encoding = detect_encoding(path, num_bytes=10_000)\n\n    with path.open(mode=\"r\", encoding=encoding) as f:\n        text = f.read()\n\n        metadata = get_metadata(text, patterns)\n\n        if not metadata:\n            logger.info(f\"Skipping file {path} due to missing metadata.\")\n            return []\n\n        data_start = \"Date/time\"\n        data_end = \"END OF DATA FILE\"\n\n        df = get_data(text, data_start, data_end, column_names)\n\n        df = handle_timestamps(df, metadata.get(\"timezone\", \"UTC\"))\n\n        ts_list = []\n\n        for col in df.columns:\n            if col in VARIABLE_TYPES_AND_UNITS:\n                unit = VARIABLE_TYPES_AND_UNITS[col][0]\n                ts_list.append(\n                    Timeseries(\n                        ts=df[col],\n                        # Validation will be done in Pydantic\n                        variable=col,  # type: ignore[arg-type]\n                        location=metadata.get(\"location\"),\n                        sensor=metadata.get(\"sensor\"),\n                        # Validation will be done in Pydantic\n                        unit=unit,  # type: ignore[arg-type]\n                    )\n                )\n            else:\n                message = (\n                    \"Unsupported variable: {col}. Please provide a valid variable type.\"\n                )\n                raise ValueError(message)\n\n    return ts_list\n</code></pre>"},{"location":"modules/#gensor.processing","title":"<code>processing</code>","text":""},{"location":"modules/#gensor.processing.compensation","title":"<code>compensation</code>","text":"<p>Compensating the raw data from the absolute pressure transducer to the actual water level using the barometric pressure data.</p> <p>Because van Essen Instrument divers are non-vented pressure transducers, to obtain the pressure resulting from the water column above the logger (i.e. the water level), the barometric pressure must be subtracted from the raw pressure measurements. In the first step the function aligns the two series to the same time step and then subtracts the barometric pressure from the raw pressure measurements. For short time periods (when for instance a slug test is performed) the barometric pressure can be provided as a single float value.</p> <p>Subsequently the function filters out all records where the absolute water column is less than or equal to the cutoff value. This is because when the logger is out of the water when the measurement is taken, the absolute water column is close to zero, producing erroneous results and spikes in the plots. The cutoff value is set to 5 cm by default, but can be adjusted using the cutoff_wc kwarg.</p> <p>Functions:</p> <pre><code>compensate: Compensate raw sensor pressure measurement with barometric pressure.\n</code></pre>"},{"location":"modules/#gensor.processing.compensation.Compensator","title":"<code>Compensator</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Compensate raw sensor pressure measurement with barometric pressure.</p> <p>Attributes:</p> Name Type Description <code>ts</code> <code>Timeseries</code> <p>Raw sensor timeseries</p> <code>barometric</code> <code>Timeseries | float</code> <p>Barometric pressure timeseries or a single float value. If a float value is provided, it is assumed to be in cmH2O.</p> Source code in <code>gensor/processing/compensation.py</code> <pre><code>class Compensator(pyd.BaseModel):\n    \"\"\"Compensate raw sensor pressure measurement with barometric pressure.\n\n    Attributes:\n        ts (Timeseries): Raw sensor timeseries\n        barometric (Timeseries | float): Barometric pressure timeseries or a single\n            float value. If a float value is provided, it is assumed to be in cmH2O.\n    \"\"\"\n\n    ts: Timeseries\n    barometric: Timeseries | float\n\n    @pyd.field_validator(\"ts\", \"barometric\", mode=\"before\")\n    def validate_timeseries_type(cls, v: Timeseries) -&gt; Timeseries:\n        if isinstance(v, Timeseries) and v.variable != \"pressure\":\n            raise InvalidMeasurementTypeError()\n        return v\n\n    @pyd.field_validator(\"ts\")\n    def validate_sensor_information(cls, v: Timeseries) -&gt; Timeseries:\n        if v.sensor is not None and not v.sensor_alt:\n            raise MissingInputError(\"sensor_alt\")\n        return v\n\n    def compensate(\n        self,\n        alignment_period: Literal[\"D\", \"ME\", \"SME\", \"MS\", \"YE\", \"YS\", \"h\", \"min\", \"s\"],\n        threshold_wc: float | None,\n        fieldwork_dates: list | None,\n    ) -&gt; Timeseries | None:\n        \"\"\"Perform compensation.\n\n        Parameters:\n            alignment_period Literal['D', 'ME', 'SME', 'MS', 'YE', 'YS', 'h', 'min', 's']: The alignment period for the timeseries.\n                Default is 'h'. See pandas offset aliases for definitinos.\n            threshold_wc (float): The threshold for the absolute water column.\n            fieldwork_dates (Optional[list]): List of dates when fieldwork was done. All\n                measurement from a fieldwork day will be set to None.\n\n        Returns:\n            Timeseries: A new Timeseries instance with the compensated data and updated unit and variable. Optionally removed outliers are included.\n        \"\"\"\n\n        resample_params = {\"freq\": alignment_period, \"agg_func\": pd.Series.mean}\n        resampled_ts = self.ts.resample(**resample_params)\n\n        if isinstance(self.barometric, Timeseries):\n            if self.ts == self.barometric:\n                print(\"Skipping compensation: both timeseries are the same.\")\n                return None\n            resampled_baro = self.barometric.resample(**resample_params).ts\n\n        elif isinstance(self.barometric, float):\n            resampled_baro = pd.Series(\n                [self.barometric] * len(resampled_ts.ts), index=resampled_ts.ts.index\n            )\n\n        # dividing by 100 to convert water column from cmH2O to mH2O\n        watercolumn_ts = resampled_ts.ts.sub(resampled_baro).divide(100).dropna()\n\n        if not isinstance(watercolumn_ts.index, pd.DatetimeIndex):\n            watercolumn_ts.index = pd.to_datetime(watercolumn_ts.index)\n\n        if fieldwork_dates:\n            fieldwork_timestamps = pd.to_datetime(fieldwork_dates).tz_localize(\n                watercolumn_ts.index.tz\n            )\n\n            watercolumn_ts.loc[\n                watercolumn_ts.index.normalize().isin(fieldwork_timestamps)\n            ] = None\n\n        if threshold_wc:\n            watercolumn_ts_filtered = watercolumn_ts[\n                watercolumn_ts.abs() &gt; threshold_wc\n            ]\n\n            dropped_outliers = watercolumn_ts[watercolumn_ts.abs() &lt;= threshold_wc]\n\n            print(\n                f\"{len(dropped_outliers)} records \\\n                    dropped due to low water column.\"\n            )\n            gwl = watercolumn_ts_filtered.add(float(resampled_ts.sensor_alt or 0))\n\n            compensated = resampled_ts.model_copy(\n                update={\n                    \"ts\": gwl,\n                    \"outliers\": dropped_outliers,\n                    \"unit\": \"m asl\",\n                    \"variable\": \"head\",\n                },\n                deep=True,\n            )\n        else:\n            gwl = watercolumn_ts.add(float(resampled_ts.sensor_alt or 0))\n\n            compensated = resampled_ts.model_copy(\n                update={\"ts\": gwl, \"unit\": \"m asl\", \"variable\": \"head\"}, deep=True\n            )\n\n        return compensated\n</code></pre>"},{"location":"modules/#gensor.processing.compensation.Compensator.compensate","title":"<code>compensate(alignment_period, threshold_wc, fieldwork_dates)</code>","text":"<p>Perform compensation.</p> <p>Parameters:</p> Name Type Description Default <code>alignment_period</code> <code>Literal['D', 'ME', 'SME', 'MS', 'YE', 'YS', 'h', 'min', 's']</code> <p>The alignment period for the timeseries. Default is 'h'. See pandas offset aliases for definitinos.</p> required <code>threshold_wc</code> <code>float</code> <p>The threshold for the absolute water column.</p> required <code>fieldwork_dates</code> <code>Optional[list]</code> <p>List of dates when fieldwork was done. All measurement from a fieldwork day will be set to None.</p> required <p>Returns:</p> Name Type Description <code>Timeseries</code> <code>Timeseries | None</code> <p>A new Timeseries instance with the compensated data and updated unit and variable. Optionally removed outliers are included.</p> Source code in <code>gensor/processing/compensation.py</code> <pre><code>def compensate(\n    self,\n    alignment_period: Literal[\"D\", \"ME\", \"SME\", \"MS\", \"YE\", \"YS\", \"h\", \"min\", \"s\"],\n    threshold_wc: float | None,\n    fieldwork_dates: list | None,\n) -&gt; Timeseries | None:\n    \"\"\"Perform compensation.\n\n    Parameters:\n        alignment_period Literal['D', 'ME', 'SME', 'MS', 'YE', 'YS', 'h', 'min', 's']: The alignment period for the timeseries.\n            Default is 'h'. See pandas offset aliases for definitinos.\n        threshold_wc (float): The threshold for the absolute water column.\n        fieldwork_dates (Optional[list]): List of dates when fieldwork was done. All\n            measurement from a fieldwork day will be set to None.\n\n    Returns:\n        Timeseries: A new Timeseries instance with the compensated data and updated unit and variable. Optionally removed outliers are included.\n    \"\"\"\n\n    resample_params = {\"freq\": alignment_period, \"agg_func\": pd.Series.mean}\n    resampled_ts = self.ts.resample(**resample_params)\n\n    if isinstance(self.barometric, Timeseries):\n        if self.ts == self.barometric:\n            print(\"Skipping compensation: both timeseries are the same.\")\n            return None\n        resampled_baro = self.barometric.resample(**resample_params).ts\n\n    elif isinstance(self.barometric, float):\n        resampled_baro = pd.Series(\n            [self.barometric] * len(resampled_ts.ts), index=resampled_ts.ts.index\n        )\n\n    # dividing by 100 to convert water column from cmH2O to mH2O\n    watercolumn_ts = resampled_ts.ts.sub(resampled_baro).divide(100).dropna()\n\n    if not isinstance(watercolumn_ts.index, pd.DatetimeIndex):\n        watercolumn_ts.index = pd.to_datetime(watercolumn_ts.index)\n\n    if fieldwork_dates:\n        fieldwork_timestamps = pd.to_datetime(fieldwork_dates).tz_localize(\n            watercolumn_ts.index.tz\n        )\n\n        watercolumn_ts.loc[\n            watercolumn_ts.index.normalize().isin(fieldwork_timestamps)\n        ] = None\n\n    if threshold_wc:\n        watercolumn_ts_filtered = watercolumn_ts[\n            watercolumn_ts.abs() &gt; threshold_wc\n        ]\n\n        dropped_outliers = watercolumn_ts[watercolumn_ts.abs() &lt;= threshold_wc]\n\n        print(\n            f\"{len(dropped_outliers)} records \\\n                dropped due to low water column.\"\n        )\n        gwl = watercolumn_ts_filtered.add(float(resampled_ts.sensor_alt or 0))\n\n        compensated = resampled_ts.model_copy(\n            update={\n                \"ts\": gwl,\n                \"outliers\": dropped_outliers,\n                \"unit\": \"m asl\",\n                \"variable\": \"head\",\n            },\n            deep=True,\n        )\n    else:\n        gwl = watercolumn_ts.add(float(resampled_ts.sensor_alt or 0))\n\n        compensated = resampled_ts.model_copy(\n            update={\"ts\": gwl, \"unit\": \"m asl\", \"variable\": \"head\"}, deep=True\n        )\n\n    return compensated\n</code></pre>"},{"location":"modules/#gensor.processing.compensation.compensate","title":"<code>compensate(raw, barometric, alignment_period='h', threshold_wc=None, fieldwork_dates=None, interpolate_method=None)</code>","text":"<p>Constructor for the Comensator object.</p> <p>Parameters:</p> Name Type Description Default <code>raw</code> <code>Timeseries | Dataset</code> <p>Raw sensor timeseries</p> required <code>barometric</code> <code>Timeseries | float</code> <p>Barometric pressure timeseries or a single float value. If a float value is provided, it is assumed to be in cmH2O.</p> required <code>alignment_period</code> <code>Literal['D', 'ME', 'SME', 'MS', 'YE', 'YS', 'h', 'min', 's']</code> <p>The alignment period for the timeseries. Default is 'h'. See pandas offset aliases for definitinos.</p> <code>'h'</code> <code>threshold_wc</code> <code>float</code> <p>The threshold for the absolute water column. If it is provided, the records below that threshold are dropped.</p> <code>None</code> <code>fieldwork_dates</code> <code>Dict[str, list]</code> <p>Dictionary of location name and a list of fieldwork days. All records on the fieldwork day are set to None.</p> <code>None</code> <code>interpolate_method</code> <code>str</code> <p>String representing the interpolate method as in pd.Series.interpolate() method.</p> <code>None</code> Source code in <code>gensor/processing/compensation.py</code> <pre><code>def compensate(\n    raw: Timeseries | Dataset,\n    barometric: Timeseries | float,\n    alignment_period: Literal[\n        \"D\", \"ME\", \"SME\", \"MS\", \"YE\", \"YS\", \"h\", \"min\", \"s\"\n    ] = \"h\",\n    threshold_wc: float | None = None,\n    fieldwork_dates: dict | None = None,\n    interpolate_method: str | None = None,\n) -&gt; Timeseries | Dataset | None:\n    \"\"\"Constructor for the Comensator object.\n\n    Parameters:\n        raw (Timeseries | Dataset): Raw sensor timeseries\n        barometric (Timeseries | float): Barometric pressure timeseries or a single\n            float value. If a float value is provided, it is assumed to be in cmH2O.\n        alignment_period (Literal['D', 'ME', 'SME', 'MS', 'YE', 'YS', 'h', 'min', 's']): The alignment period for the timeseries.\n            Default is 'h'. See pandas offset aliases for definitinos.\n        threshold_wc (float): The threshold for the absolute water column. If it is\n            provided, the records below that threshold are dropped.\n        fieldwork_dates (Dict[str, list]): Dictionary of location name and a list of\n            fieldwork days. All records on the fieldwork day are set to None.\n        interpolate_method (str): String representing the interpolate method as in\n            pd.Series.interpolate() method.\n    \"\"\"\n    if fieldwork_dates is None:\n        fieldwork_dates = {}\n\n    def _compensate_one(\n        raw: Timeseries, fieldwork_dates: list | None\n    ) -&gt; Timeseries | None:\n        comp = Compensator(ts=raw, barometric=barometric)\n        compensated = comp.compensate(\n            alignment_period=alignment_period,\n            threshold_wc=threshold_wc,\n            fieldwork_dates=fieldwork_dates,\n        )\n        if compensated is not None and interpolate_method:\n            # .interpolate() called on Timeseries object is wrapped to return a\n            # Timeseries object from the original pandas.Series.interpolate().\n            return compensated.interpolate(method=interpolate_method)  # type: ignore[no-any-return]\n\n        else:\n            return compensated\n\n    if isinstance(raw, Timeseries):\n        dates = fieldwork_dates.get(raw.location)\n        return _compensate_one(raw, dates)\n\n    elif isinstance(raw, Dataset):\n        compensated_series = []\n        for item in raw:\n            dates = fieldwork_dates.get(item.location)\n            compensated_series.append(_compensate_one(item, dates))\n\n        return raw.model_copy(update={\"timeseries\": compensated_series}, deep=True)\n</code></pre>"},{"location":"modules/#gensor.processing.smoothing","title":"<code>smoothing</code>","text":"<p>Tools for smoothing the data.</p>"},{"location":"modules/#gensor.processing.smoothing.smooth_data","title":"<code>smooth_data(data, window=5, method='rolling_mean', print_statistics=False, inplace=False, plot=False)</code>","text":"<p>Smooth a time series using a rolling mean or median.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series</code> <p>The time series data.</p> required <code>window</code> <code>int</code> <p>The size of the window for the rolling mean or median. Defaults to 5.</p> <code>5</code> <code>method</code> <code>str</code> <p>The method to use for smoothing. Either 'rolling_mean' or 'rolling_median'. Defaults to 'rolling_mean'.</p> <code>'rolling_mean'</code> <p>Returns:</p> Type Description <code>Series | None</code> <p>pandas.Series: The smoothed time series.</p> Source code in <code>gensor/processing/smoothing.py</code> <pre><code>def smooth_data(\n    data: Timeseries,\n    window: int = 5,\n    method: str = \"rolling_mean\",\n    print_statistics: bool = False,\n    inplace: bool = False,\n    plot: bool = False,\n) -&gt; Series | None:\n    \"\"\"Smooth a time series using a rolling mean or median.\n\n    Args:\n        data (pandas.Series): The time series data.\n        window (int): The size of the window for the rolling mean or median. Defaults to 5.\n        method (str): The method to use for smoothing. Either 'rolling_mean' or 'rolling_median'. Defaults to 'rolling_mean'.\n\n    Returns:\n        pandas.Series: The smoothed time series.\n    \"\"\"\n    if method == \"rolling_mean\":\n        smoothed_data = data.ts.rolling(window=window, center=True).mean()\n    elif method == \"rolling_median\":\n        smoothed_data = data.ts.rolling(window=window, center=True).median()\n    else:\n        raise NotImplementedError()\n\n    valid_indices = smoothed_data.notna()\n    original_data_aligned = data.ts[valid_indices]\n    smoothed_data_aligned = smoothed_data[valid_indices]\n\n    if print_statistics:\n        mse = root_mean_squared_error(original_data_aligned, smoothed_data_aligned)\n        print(f\"Mean Squared Error of {method}: {mse:.2f}\")\n\n    if plot:\n        plt.figure(figsize=(12, 6))\n        plt.plot(\n            data.timeseries.index, data.timeseries, label=\"Original Data\", color=\"black\"\n        )\n        plt.plot(\n            smoothed_data.index,\n            smoothed_data,\n            label=f\"Moving Average ({method})\",\n            color=\"green\",\n            linestyle=\"dotted\",\n        )\n\n        plt.legend()\n        plt.title(\"Groundwater Level with Moving Average\")\n        plt.xlabel(\"Date\")\n        plt.ylabel(\"Groundwater Level\")\n        plt.show()\n\n    if inplace:\n        data.ts = smoothed_data\n        return None\n    else:\n        return smoothed_data\n</code></pre>"},{"location":"modules/#gensor.processing.transform","title":"<code>transform</code>","text":""},{"location":"modules/#gensor.processing.transform.Transformation","title":"<code>Transformation</code>","text":"Source code in <code>gensor/processing/transform.py</code> <pre><code>class Transformation:\n    def __init__(\n        self,\n        data: Series,\n        method: Literal[\n            \"difference\",\n            \"log\",\n            \"square_root\",\n            \"box_cox\",\n            \"standard_scaler\",\n            \"minmax_scaler\",\n            \"robust_scaler\",\n            \"maxabs_scaler\",\n        ],\n        **kwargs: Any,\n    ) -&gt; None:\n        self.data = data\n\n        if method == \"difference\":\n            self.transformed_data, self.scaler = self.difference(**kwargs)\n        elif method == \"log\":\n            self.transformed_data, self.scaler = self.log()\n        elif method == \"square_root\":\n            self.transformed_data, self.scaler = self.square_root()\n        elif method == \"box_cox\":\n            self.transformed_data, self.scaler = self.box_cox(**kwargs)\n        elif method == \"standard_scaler\":\n            self.transformed_data, self.scaler = self.standard_scaler()\n        elif method == \"minmax_scaler\":\n            self.transformed_data, self.scaler = self.minmax_scaler()\n        elif method == \"robust_scaler\":\n            self.transformed_data, self.scaler = self.robust_scaler()\n        elif method == \"maxabs_scaler\":\n            self.transformed_data, self.scaler = self.maxabs_scaler()\n        else:\n            raise NotImplementedError()\n\n    def get_transformation(self) -&gt; tuple:\n        return self.transformed_data, self.scaler\n\n    def difference(self, **kwargs: int) -&gt; tuple[Series, str]:\n        \"\"\"Difference the time series data.\n\n        Keword Arguments:\n            periods (int): The number of periods to shift. Defaults to 1.\n\n        Returns:\n            pandas.Series: The differenced time series data.\n        \"\"\"\n        periods = kwargs.get(\"periods\", 1)\n        transformed = self.data.diff(periods=periods).dropna()\n\n        return (transformed, \"difference\")\n\n    def log(self) -&gt; tuple[Series, str]:\n        \"\"\"Take the natural logarithm of the time series data.\n\n        Returns:\n            pandas.Series: The natural logarithm of the time series data.\n        \"\"\"\n        transformed = self.data.apply(lambda x: x if x &lt;= 0 else np.log(x))\n        return (transformed, \"log\")\n\n    def square_root(self) -&gt; tuple[Series, str]:\n        \"\"\"Take the square root of the time series data.\n\n        Returns:\n            pandas.Series: The square root of the time series data.\n        \"\"\"\n        transformed = self.data.apply(lambda x: x if x &lt;= 0 else np.sqrt(x))\n        return (transformed, \"square_root\")\n\n    def box_cox(self, **kwargs: float) -&gt; tuple[Series, str]:\n        \"\"\"Apply the Box-Cox transformation to the time series data. Only works\n            for all positive datasets!\n\n        Keyword Arguments:\n            lmbda (float): The transformation parameter. If not provided, it is automatically estimated.\n\n        Returns:\n            pandas.Series: The Box-Cox transformed time series data.\n        \"\"\"\n        lmbda = kwargs.get(\"lmbda\")\n\n        if (self.data &lt;= 0).any():\n            message = (\n                \"Box-Cox transformation requires all values to be strictly positive.\"\n            )\n            raise ValueError(message)\n\n        # Box-Cox always returns a tuple: (transformed_data, lmbda)\n        if lmbda is not None:\n            transformed_data = stats.boxcox(self.data, lmbda=lmbda)\n        else:\n            transformed_data, lmbda = stats.boxcox(self.data, lmbda=lmbda)\n\n        # Return the transformed series and mark the method used\n        transformed_series = Series(transformed_data, index=self.data.index)\n        return transformed_series, f\"box-cox (lambda={lmbda})\"\n\n    def standard_scaler(self) -&gt; tuple[Series, Any]:\n        \"\"\"Normalize a pandas Series using StandardScaler.\"\"\"\n        scaler = StandardScaler()\n        scaled_values = scaler.fit_transform(\n            self.data.to_numpy().reshape(-1, 1)\n        ).flatten()\n        scaled_series = Series(scaled_values, index=self.data.index)\n        return scaled_series, scaler\n\n    def minmax_scaler(self) -&gt; tuple[Series, Any]:\n        \"\"\"Normalize a pandas Series using MinMaxScaler.\"\"\"\n        scaler = MinMaxScaler()\n        scaled_values = scaler.fit_transform(\n            self.data.to_numpy().reshape(-1, 1)\n        ).flatten()\n        scaled_series = Series(scaled_values, index=self.data.index)\n        return scaled_series, scaler\n\n    def robust_scaler(self) -&gt; tuple[Series, Any]:\n        \"\"\"Normalize a pandas Series using RobustScaler.\"\"\"\n        scaler = RobustScaler()\n        scaled_values = scaler.fit_transform(\n            self.data.to_numpy().reshape(-1, 1)\n        ).flatten()\n        scaled_series = Series(scaled_values, index=self.data.index)\n        return scaled_series, scaler\n\n    def maxabs_scaler(self) -&gt; tuple[Series, Any]:\n        \"\"\"Normalize a pandas Series using MaxAbsScaler.\"\"\"\n        scaler = MaxAbsScaler()\n        scaled_values = scaler.fit_transform(\n            self.data.to_numpy().reshape(-1, 1)\n        ).flatten()\n        scaled_series = Series(scaled_values, index=self.data.index)\n        return scaled_series, scaler\n</code></pre>"},{"location":"modules/#gensor.processing.transform.Transformation.box_cox","title":"<code>box_cox(**kwargs)</code>","text":"<p>Apply the Box-Cox transformation to the time series data. Only works     for all positive datasets!</p> <p>Other Parameters:</p> Name Type Description <code>lmbda</code> <code>float</code> <p>The transformation parameter. If not provided, it is automatically estimated.</p> <p>Returns:</p> Type Description <code>tuple[Series, str]</code> <p>pandas.Series: The Box-Cox transformed time series data.</p> Source code in <code>gensor/processing/transform.py</code> <pre><code>def box_cox(self, **kwargs: float) -&gt; tuple[Series, str]:\n    \"\"\"Apply the Box-Cox transformation to the time series data. Only works\n        for all positive datasets!\n\n    Keyword Arguments:\n        lmbda (float): The transformation parameter. If not provided, it is automatically estimated.\n\n    Returns:\n        pandas.Series: The Box-Cox transformed time series data.\n    \"\"\"\n    lmbda = kwargs.get(\"lmbda\")\n\n    if (self.data &lt;= 0).any():\n        message = (\n            \"Box-Cox transformation requires all values to be strictly positive.\"\n        )\n        raise ValueError(message)\n\n    # Box-Cox always returns a tuple: (transformed_data, lmbda)\n    if lmbda is not None:\n        transformed_data = stats.boxcox(self.data, lmbda=lmbda)\n    else:\n        transformed_data, lmbda = stats.boxcox(self.data, lmbda=lmbda)\n\n    # Return the transformed series and mark the method used\n    transformed_series = Series(transformed_data, index=self.data.index)\n    return transformed_series, f\"box-cox (lambda={lmbda})\"\n</code></pre>"},{"location":"modules/#gensor.processing.transform.Transformation.difference","title":"<code>difference(**kwargs)</code>","text":"<p>Difference the time series data.</p> Keword Arguments <p>periods (int): The number of periods to shift. Defaults to 1.</p> <p>Returns:</p> Type Description <code>tuple[Series, str]</code> <p>pandas.Series: The differenced time series data.</p> Source code in <code>gensor/processing/transform.py</code> <pre><code>def difference(self, **kwargs: int) -&gt; tuple[Series, str]:\n    \"\"\"Difference the time series data.\n\n    Keword Arguments:\n        periods (int): The number of periods to shift. Defaults to 1.\n\n    Returns:\n        pandas.Series: The differenced time series data.\n    \"\"\"\n    periods = kwargs.get(\"periods\", 1)\n    transformed = self.data.diff(periods=periods).dropna()\n\n    return (transformed, \"difference\")\n</code></pre>"},{"location":"modules/#gensor.processing.transform.Transformation.log","title":"<code>log()</code>","text":"<p>Take the natural logarithm of the time series data.</p> <p>Returns:</p> Type Description <code>tuple[Series, str]</code> <p>pandas.Series: The natural logarithm of the time series data.</p> Source code in <code>gensor/processing/transform.py</code> <pre><code>def log(self) -&gt; tuple[Series, str]:\n    \"\"\"Take the natural logarithm of the time series data.\n\n    Returns:\n        pandas.Series: The natural logarithm of the time series data.\n    \"\"\"\n    transformed = self.data.apply(lambda x: x if x &lt;= 0 else np.log(x))\n    return (transformed, \"log\")\n</code></pre>"},{"location":"modules/#gensor.processing.transform.Transformation.maxabs_scaler","title":"<code>maxabs_scaler()</code>","text":"<p>Normalize a pandas Series using MaxAbsScaler.</p> Source code in <code>gensor/processing/transform.py</code> <pre><code>def maxabs_scaler(self) -&gt; tuple[Series, Any]:\n    \"\"\"Normalize a pandas Series using MaxAbsScaler.\"\"\"\n    scaler = MaxAbsScaler()\n    scaled_values = scaler.fit_transform(\n        self.data.to_numpy().reshape(-1, 1)\n    ).flatten()\n    scaled_series = Series(scaled_values, index=self.data.index)\n    return scaled_series, scaler\n</code></pre>"},{"location":"modules/#gensor.processing.transform.Transformation.minmax_scaler","title":"<code>minmax_scaler()</code>","text":"<p>Normalize a pandas Series using MinMaxScaler.</p> Source code in <code>gensor/processing/transform.py</code> <pre><code>def minmax_scaler(self) -&gt; tuple[Series, Any]:\n    \"\"\"Normalize a pandas Series using MinMaxScaler.\"\"\"\n    scaler = MinMaxScaler()\n    scaled_values = scaler.fit_transform(\n        self.data.to_numpy().reshape(-1, 1)\n    ).flatten()\n    scaled_series = Series(scaled_values, index=self.data.index)\n    return scaled_series, scaler\n</code></pre>"},{"location":"modules/#gensor.processing.transform.Transformation.robust_scaler","title":"<code>robust_scaler()</code>","text":"<p>Normalize a pandas Series using RobustScaler.</p> Source code in <code>gensor/processing/transform.py</code> <pre><code>def robust_scaler(self) -&gt; tuple[Series, Any]:\n    \"\"\"Normalize a pandas Series using RobustScaler.\"\"\"\n    scaler = RobustScaler()\n    scaled_values = scaler.fit_transform(\n        self.data.to_numpy().reshape(-1, 1)\n    ).flatten()\n    scaled_series = Series(scaled_values, index=self.data.index)\n    return scaled_series, scaler\n</code></pre>"},{"location":"modules/#gensor.processing.transform.Transformation.square_root","title":"<code>square_root()</code>","text":"<p>Take the square root of the time series data.</p> <p>Returns:</p> Type Description <code>tuple[Series, str]</code> <p>pandas.Series: The square root of the time series data.</p> Source code in <code>gensor/processing/transform.py</code> <pre><code>def square_root(self) -&gt; tuple[Series, str]:\n    \"\"\"Take the square root of the time series data.\n\n    Returns:\n        pandas.Series: The square root of the time series data.\n    \"\"\"\n    transformed = self.data.apply(lambda x: x if x &lt;= 0 else np.sqrt(x))\n    return (transformed, \"square_root\")\n</code></pre>"},{"location":"modules/#gensor.processing.transform.Transformation.standard_scaler","title":"<code>standard_scaler()</code>","text":"<p>Normalize a pandas Series using StandardScaler.</p> Source code in <code>gensor/processing/transform.py</code> <pre><code>def standard_scaler(self) -&gt; tuple[Series, Any]:\n    \"\"\"Normalize a pandas Series using StandardScaler.\"\"\"\n    scaler = StandardScaler()\n    scaled_values = scaler.fit_transform(\n        self.data.to_numpy().reshape(-1, 1)\n    ).flatten()\n    scaled_series = Series(scaled_values, index=self.data.index)\n    return scaled_series, scaler\n</code></pre>"},{"location":"modules/#gensor.testdata","title":"<code>testdata</code>","text":"<p>Test data for Gensor package:</p> <p>Attributes:</p> <pre><code>all (Path): The whole directory of test groundwater sensor data.\nbaro (Path): Timeseries of barometric pressure measurements.\npb01a (Path): Timeseries of a submerged logger.\npb02a_plain (Path): Timeseries from PB02A with the metadata removed.\n</code></pre>"},{"location":"modules/#gensor.testdata.all_paths","title":"<code>all_paths: Traversable = resources.files(__name__)</code>  <code>module-attribute</code>","text":"<p>The whole directory of test groundwater sensor data.</p>"},{"location":"modules/#gensor.testdata.baro","title":"<code>baro: Traversable = all_paths / 'Barodiver_220427183008_BY222.csv'</code>  <code>module-attribute</code>","text":"<p>Timeseries of barometric pressure measurements.</p>"},{"location":"modules/#gensor.testdata.pb01a","title":"<code>pb01a: Traversable = all_paths / 'PB01A_moni_AV319_220427183019_AV319.csv'</code>  <code>module-attribute</code>","text":"<p>Timeseries of a submerged logger.</p>"},{"location":"modules/#gensor.testdata.pb02a_plain","title":"<code>pb02a_plain: Traversable = all_paths / 'PB02A_plain.csv'</code>  <code>module-attribute</code>","text":"<p>Timeseries from PB02A with the metadata removed.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Generally gensor's PyPi release is up to date with the github repository, so the recommended installation is via pip install:</p> <pre><code>pip install gensor\n</code></pre> <p>Consider also using poetry - a great tool for dependency management in your project:</p> <pre><code>poetry add gensor\n</code></pre> <p>You can upgrade gensor if there is a new release by typing</p> <pre><code>pip install gensor --upgrade\n</code></pre> <p>Follow the tutorials to learn how to use the package.</p>"},{"location":"notebooks/","title":"Examples","text":"<p>Below you can find examples of how to use Gensor package.</p> <ul> <li>Example 1: Basic usage: Timeseries &amp; Datasets</li> <li>Example 2: Transformation and outlier detection</li> <li>Example 3: SQL database integration</li> <li>Example 4: Advanced plotting</li> </ul> <p>Click on the links above to open the respective tutorial.</p> <p>More example will come as the functionality of the package expands.</p>"},{"location":"notebooks/000-basic-usage/","title":"Basic usage of the package","text":"<p>Welcome to the first tutorial of Gensor. Here we will explore the core features of the package which come with the base classes: <code>Timeseries</code> and <code>Dataset</code>.</p> <p>We start with importing the package and the test datasets. We will also set the logging level to \"WARNING\" to avoid getting all \"INFO\" messages.</p> In\u00a0[1]: Copied! <pre>import gensor as gs\nfrom gensor.testdata import all_paths, pb02a_plain\n\ngs.set_log_level(\"WARNING\")\n</pre> import gensor as gs from gensor.testdata import all_paths, pb02a_plain  gs.set_log_level(\"WARNING\") In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\n\ntimestamps = pd.date_range(start=\"2024-01-01 00:00\", periods=10, freq=\"h\", tz=\"UTC\")\n\nbarometric_pressure = np.random.uniform(low=980, high=1050, size=10)\n\npressure_series = pd.Series(\n    data=barometric_pressure, index=timestamps, name=\"Barometric Pressure\"\n)\n\nts = gs.Timeseries(\n    ts=pressure_series,\n    variable=\"pressure\",\n    unit=\"cmh2o\",\n    location=\"BH1\",\n    sensor=\"ABC123\",\n)\n</pre> import numpy as np import pandas as pd  timestamps = pd.date_range(start=\"2024-01-01 00:00\", periods=10, freq=\"h\", tz=\"UTC\")  barometric_pressure = np.random.uniform(low=980, high=1050, size=10)  pressure_series = pd.Series(     data=barometric_pressure, index=timestamps, name=\"Barometric Pressure\" )  ts = gs.Timeseries(     ts=pressure_series,     variable=\"pressure\",     unit=\"cmh2o\",     location=\"BH1\",     sensor=\"ABC123\", ) In\u00a0[3]: Copied! <pre>pattern = r\"[A-Za-z]{2}\\d{2}[A-Za-z]{1}|Barodiver\"\n\nds = gs.read_from_csv(path=all_paths, file_format=\"vanessen\", location_pattern=pattern)\n</pre> pattern = r\"[A-Za-z]{2}\\d{2}[A-Za-z]{1}|Barodiver\"  ds = gs.read_from_csv(path=all_paths, file_format=\"vanessen\", location_pattern=pattern) <p>By using the load_from_csv file supplied with a directory path, we looped over all the files and attempted to create Timeseries for each. However, one of the files was skipped because it did not contain the metadata in the headers.</p> <p>Now, let's check the data variable.</p> In\u00a0[4]: Copied! <pre>ds.get_locations()\n</pre> ds.get_locations() Out[4]: <pre>['Barodiver', 'Barodiver', 'PB01A', 'PB01A']</pre> <p>We obtained an object of type Dataset which has 4 <code>Timeseries</code> in it. Unless there is only 1 Timeseries in the file (measurements of just 1 variable), this is the default return object from the load_from_csv() function. Dataset is a collection of Timeseries and it has some functionality like merging timeseries from the same sensors and locations. Now, we will use the \"plain\" parser to load the other timeseries. This time we do not need any patterns, but we do need to provide the timeseries metadata manually.</p> In\u00a0[5]: Copied! <pre>ds2 = gs.read_from_csv(\n    path=pb02a_plain, file_format=\"plain\", location=\"PB02A\", sensor=\"AV336\"\n)\n</pre> ds2 = gs.read_from_csv(     path=pb02a_plain, file_format=\"plain\", location=\"PB02A\", sensor=\"AV336\" ) In\u00a0[6]: Copied! <pre>ds2\n</pre> ds2 Out[6]: <pre>Dataset(2)</pre> <p>So this time we passed a file path and not a directory. The read_from_csv function returned a <code>Dataset</code> with 2 <code>Timeseries</code> in it. What we can do now is to merge these two datasets.</p> In\u00a0[7]: Copied! <pre>ds.add(ds2)\n</pre> ds.add(ds2) Out[7]: <pre>Dataset(6)</pre> In\u00a0[8]: Copied! <pre>ds.plot()\n</pre> ds.plot() Out[8]: <pre>(&lt;Figure size 1000x1000 with 2 Axes&gt;,\n array([&lt;Axes: title={'center': 'Timeseries for Pressure'}, xlabel='Time', ylabel='pressure (cmh2o)'&gt;,\n        &lt;Axes: title={'center': 'Timeseries for Temperature'}, xlabel='Time', ylabel='temperature (degc)'&gt;],\n       dtype=object))</pre> <p>Voila, we now have a <code>Dataset</code> of 6 <code>Timeseries</code>.</p> <p>Note</p> <p>     If you would attempt to do this operation again, the code would only add timeseries that are not present in the `Dataset` already. If the `Timeseries` is already present in the `Dataset`, only missing records will be added to that timeseries. This helps to avoid duplication. But it also means that if you have separate sets of measurements from the same location but generated, for example in a slug test, you may want to create separate datasets for them.  </p> <p>Example:</p> <p>In Project Grow, we had absolute pressure transducers deployed across an agricultural field. We obtained all measurements for sensor altitude necessary for converting the raw measurments to groundwater levels in meters above sea level. However, over time, some piezometers had to be shortened, and the sensors were re-deployed at the same location with the same cable. Hence, we had to split the timeseries into pieces to account for chaning sensor altitude.The loggers we use in Project Grow record temerature and pressure, but there are also other products that can measure electricul conductivity as well. In the CSV file each timeseries is saved in a separate column. <code>load_from_csv()</code> function splits the columns into individual timeseries and creates a list of <code>Timeseries</code> objects.</p> <p>Note: In the vanessen file, the metadata of the diver and the station it is deployed to is stored in the header of the file. To extract those from all the files, I used regex. It is important that the user knows the naming patterns of the stations and replace the default pattern using a kwarg <code>location_pattern</code>. It is possible to add an OR (|) operator to try matching multiple patterns. For us, the following pattern matches all substrings that start with two letters, have two numbers in the middle and end with one letter or include a string 'Barodived' <code>pattern = r'[A-Za-z]{2}\\d{2}[A-Za-z]{1}|Barodiver'</code></p> In\u00a0[9]: Copied! <pre># identify the barometric timeseries in your dataset\nbaro = ds.filter(location=\"Barodiver\", variable=\"pressure\")\nbaro.plot()  # Timeseries have a .plot()\n</pre> # identify the barometric timeseries in your dataset baro = ds.filter(location=\"Barodiver\", variable=\"pressure\") baro.plot()  # Timeseries have a .plot() Out[9]: <pre>(&lt;Figure size 1000x500 with 1 Axes&gt;,\n &lt;Axes: title={'center': 'Pressure at Barodiver (BY222)'}, xlabel='Time', ylabel='pressure (cmh2o)'&gt;)</pre> In\u00a0[10]: Copied! <pre># Adding extra information to the submerged sensor timeseries. This could also\n# be imported from a json file or a csv and converted into a dictionary.\nalts = {\"PB01A\": 31.48, \"PB02A\": 31.68}\n\ngroundwater_ds = ds.filter(location=alts.keys(), variable=\"pressure\")\n\n# we will loop over the dataset and attach the sensor altitude to the timeseries\n# based on the location name.\nfor ts in groundwater_ds:\n    ts.sensor_alt = alts.get(ts.location)\n\ngroundwater_ds.plot()\n</pre> # Adding extra information to the submerged sensor timeseries. This could also # be imported from a json file or a csv and converted into a dictionary. alts = {\"PB01A\": 31.48, \"PB02A\": 31.68}  groundwater_ds = ds.filter(location=alts.keys(), variable=\"pressure\")  # we will loop over the dataset and attach the sensor altitude to the timeseries # based on the location name. for ts in groundwater_ds:     ts.sensor_alt = alts.get(ts.location)  groundwater_ds.plot() Out[10]: <pre>(&lt;Figure size 1000x500 with 1 Axes&gt;,\n [&lt;Axes: title={'center': 'Timeseries for Pressure'}, xlabel='Time', ylabel='pressure (cmh2o)'&gt;])</pre> <p>The sudden dips visible in the graph above are measurements taken by the diver when it was out of the water for some time or when pumping was done. The safest way to deal with this is to remove the measuremenst from the day of the fieldwork.</p> <p>The compensate function accepts either a <code>Dataset</code> or <code>Timeseries</code> objects. It also accepts a dictionary containing lists of fieldwork dates per station, which are then used by the function to mask the erroneous measuremsnt. Another option is to drop measuremsnt that have a low water column, but it puts you at risk of removing valid measurements as well.</p> In\u00a0[11]: Copied! <pre>fieldwork_days = {\n    \"PB01A\": [\n        \"2020-08-25\",\n        \"2020-10-20\",\n        \"2020-11-18\",\n        \"2020-12-03\",\n        \"2020-12-08\",\n        \"2021-02-17\",\n        \"2021-03-10\",\n        \"2021-04-14\",\n        \"2021-05-18\",\n        \"2021-05-27\",\n        \"2021-08-17\",\n        \"2021-09-16\",\n    ],\n    \"PB02A\": [\n        \"2020-08-25\",\n        \"2020-10-20\",\n        \"2020-11-18\",\n        \"2020-12-03\",\n        \"2020-12-08\",\n        \"2021-02-17\",\n        \"2021-03-10\",\n        \"2021-04-14\",\n        \"2021-05-18\",\n        \"2021-05-27\",\n        \"2021-08-17\",\n        \"2021-09-16\",\n        \"2021-04-26\",\n    ],\n}\n\ncompensated_ds = gs.compensate(\n    groundwater_ds, baro, fieldwork_dates=fieldwork_days, interpolate_method=\"linear\"\n)\n</pre> fieldwork_days = {     \"PB01A\": [         \"2020-08-25\",         \"2020-10-20\",         \"2020-11-18\",         \"2020-12-03\",         \"2020-12-08\",         \"2021-02-17\",         \"2021-03-10\",         \"2021-04-14\",         \"2021-05-18\",         \"2021-05-27\",         \"2021-08-17\",         \"2021-09-16\",     ],     \"PB02A\": [         \"2020-08-25\",         \"2020-10-20\",         \"2020-11-18\",         \"2020-12-03\",         \"2020-12-08\",         \"2021-02-17\",         \"2021-03-10\",         \"2021-04-14\",         \"2021-05-18\",         \"2021-05-27\",         \"2021-08-17\",         \"2021-09-16\",         \"2021-04-26\",     ], }  compensated_ds = gs.compensate(     groundwater_ds, baro, fieldwork_dates=fieldwork_days, interpolate_method=\"linear\" ) In\u00a0[12]: Copied! <pre>compensated_ds[1].plot()\n</pre> compensated_ds[1].plot() Out[12]: <pre>(&lt;Figure size 1000x500 with 1 Axes&gt;,\n &lt;Axes: title={'center': 'Head at PB02A (AV336)'}, xlabel='Time', ylabel='head (m asl)'&gt;)</pre> <p>As you can see, the measurements are now expressed as head and units changed to m asl. The compensation worked, but there are still some visible outliers. We will tackle this problem in the next tutorial Example 2: Transformation and outlier detection.</p> <p>If you are interested in creating a dataset and saving it in a sqlite database, follow Example 3: SQLite integration</p>"},{"location":"notebooks/000-basic-usage/#basic-usage-of-the-package","title":"Basic usage of the package\u00b6","text":""},{"location":"notebooks/000-basic-usage/#creating-a-timeseries","title":"Creating a Timeseries\u00b6","text":"<p>Essentially <code>Timeseries</code> wraps a <code>pandas.Series</code> in a layer of additional metadata (sensor name, location, elevation of the sensor, etc.) and functionality (serialization, compensation for atmospheric pressure, etc.) that are specific in groundwater sensor timeseries analysis.</p> <p>For a basic example, let's take a simple, generic pandas.Series of barometric pressure and dump the resulting model to a dictionary to see what attributes does it have:</p> <p>Note</p> <p>    The pandas.Series that goes into the Timeseries object as an attribute has have a timezone-aware DateTimeIndex type. </p>"},{"location":"notebooks/000-basic-usage/#loading-timeseries-with-built-in-functions","title":"Loading timeseries with built-in functions\u00b6","text":"<p>Timeseries from sensors are usually shared as some kind of file, often .csv. Those files may have various formats and for each file format, there needs to be a separate parser. In gensor, there are two parsers implemented so far: for van essen instruments diver and a plain serializer. The first one works well when one wants to load many files from a whole directory, because it can take Timeseries metadata directly from the van essen formatted csv file while the latter needs that information to be passed separately. Let's load some files with both parsers.</p> <p>Note, that for the 'vanessen' parser, you need to pass some regex patterns matching the sensor serial number and location name.</p>"},{"location":"notebooks/000-basic-usage/#compensation","title":"Compensation\u00b6","text":"<p>Now we have our <code>ds</code> object containing 6 <code>Timeseries</code>. It happens that those <code>Timeseries</code> are of pressure and temperature measurements but we only want to look at groundwater levels. Additionally, those measurements are raw pressure measurements which need to be compensated. Therefore we need to filter our dataset. We can use <code>Dataset.filter()</code> method to get only the pressure records, identify the barometric timeseries needed for compensation and define additional attributes of the <code>Timeseries</code>.</p>"},{"location":"notebooks/001-transformation-and-outliers/","title":"Data transformation and outlier detection","text":"In\u00a0[1]: Copied! <pre>import gensor as gs\nfrom gensor import read_from_csv\nfrom gensor.testdata import all_paths, pb02a_plain\n\npattern = r\"[A-Za-z]{2}\\d{2}[A-Za-z]{1}|Barodiver\"\n\nds = read_from_csv(path=all_paths, file_format=\"vanessen\", location_pattern=pattern)\n\n\nds2 = read_from_csv(\n    path=pb02a_plain, file_format=\"plain\", location=\"PB02A\", sensor=\"AV336\"\n)\n\nds.add(ds2)\n\nbaro = ds.filter(stations=\"Barodiver\", variables=\"pressure\")\n\nalts = {\"PB01A\": 31.48, \"PB02A\": 31.68}\n\ngroundwater_ds = ds.filter(stations=alts.keys(), variables=\"pressure\")\n\nfor ts in groundwater_ds:\n    ts.sensor_alt = alts.get(ts.location)\n\n# This is the version where we just exclude the fieldwork dates:\n# fieldwork_days = {\"PB01A\": ['2020-08-25', '2020-10-20', '2020-11-18', '2020-12-03', '2020-12-08',\n#                   '2021-02-17', '2021-03-10', '2021-04-14', '2021-05-18', '2021-05-27',\n#                   '2021-08-17', '2021-09-16'],\n#                   \"PB02A\": ['2020-08-25', '2020-10-20', '2020-11-18', '2020-12-03', '2020-12-08',\n#                   '2021-02-17', '2021-03-10', '2021-04-14', '2021-05-18', '2021-05-27',\n#                   '2021-08-17', '2021-09-16', '2021-04-26']}\n\n# compensated_ds = gs.compensate(groundwater_ds, baro, fieldwork_dates=fieldwork_days, interpolate_method='linear')\ncompensated_ds = gs.compensate(groundwater_ds, baro)\n\ncompensated_ds.plot()\n</pre> import gensor as gs from gensor import read_from_csv from gensor.testdata import all_paths, pb02a_plain  pattern = r\"[A-Za-z]{2}\\d{2}[A-Za-z]{1}|Barodiver\"  ds = read_from_csv(path=all_paths, file_format=\"vanessen\", location_pattern=pattern)   ds2 = read_from_csv(     path=pb02a_plain, file_format=\"plain\", location=\"PB02A\", sensor=\"AV336\" )  ds.add(ds2)  baro = ds.filter(stations=\"Barodiver\", variables=\"pressure\")  alts = {\"PB01A\": 31.48, \"PB02A\": 31.68}  groundwater_ds = ds.filter(stations=alts.keys(), variables=\"pressure\")  for ts in groundwater_ds:     ts.sensor_alt = alts.get(ts.location)  # This is the version where we just exclude the fieldwork dates: # fieldwork_days = {\"PB01A\": ['2020-08-25', '2020-10-20', '2020-11-18', '2020-12-03', '2020-12-08', #                   '2021-02-17', '2021-03-10', '2021-04-14', '2021-05-18', '2021-05-27', #                   '2021-08-17', '2021-09-16'], #                   \"PB02A\": ['2020-08-25', '2020-10-20', '2020-11-18', '2020-12-03', '2020-12-08', #                   '2021-02-17', '2021-03-10', '2021-04-14', '2021-05-18', '2021-05-27', #                   '2021-08-17', '2021-09-16', '2021-04-26']}  # compensated_ds = gs.compensate(groundwater_ds, baro, fieldwork_dates=fieldwork_days, interpolate_method='linear') compensated_ds = gs.compensate(groundwater_ds, baro)  compensated_ds.plot() <pre>INFO: Loading file: /home/runner/work/gensor/gensor/gensor/testdata/PB02A_plain.csv\n</pre> <pre>INFO: Skipping file /home/runner/work/gensor/gensor/gensor/testdata/PB02A_plain.csv due to missing metadata.\n</pre> <pre>INFO: Loading file: /home/runner/work/gensor/gensor/gensor/testdata/Barodiver_220427183008_BY222.csv\n</pre> <pre>INFO: Loading file: /home/runner/work/gensor/gensor/gensor/testdata/PB01A_moni_AV319_220427183019_AV319.csv\n</pre> <pre>INFO: Loading file: /home/runner/work/gensor/gensor/gensor/testdata/PB02A_plain.csv\n</pre> <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[1], line 16\n     10 ds2 = read_from_csv(\n     11     path=pb02a_plain, file_format=\"plain\", location=\"PB02A\", sensor=\"AV336\"\n     12 )\n     14 ds.add(ds2)\n---&gt; 16 baro = ds.filter(stations=\"Barodiver\", variables=\"pressure\")\n     18 alts = {\"PB01A\": 31.48, \"PB02A\": 31.68}\n     20 groundwater_ds = ds.filter(stations=alts.keys(), variables=\"pressure\")\n\nFile ~/work/gensor/gensor/gensor/core/dataset.py:129, in Dataset.filter(self, location, variable, unit, **kwargs)\n    126     if isinstance(value, str):\n    127         kwargs[key] = [value]\n--&gt; 129 matching_timeseries = [\n    130     ts\n    131     for ts in self.timeseries\n    132     if ts is not None\n    133     and (location is None or ts.location in location)\n    134     and (variable is None or ts.variable in variable)\n    135     and (unit is None or ts.unit in unit)\n    136     and all(matches(ts, attr, value) for attr, value in kwargs.items())\n    137 ]\n    139 if not matching_timeseries:\n    140     return Dataset()\n\nFile ~/work/gensor/gensor/gensor/core/dataset.py:136, in &lt;listcomp&gt;(.0)\n    126     if isinstance(value, str):\n    127         kwargs[key] = [value]\n    129 matching_timeseries = [\n    130     ts\n    131     for ts in self.timeseries\n    132     if ts is not None\n    133     and (location is None or ts.location in location)\n    134     and (variable is None or ts.variable in variable)\n    135     and (unit is None or ts.unit in unit)\n--&gt; 136     and all(matches(ts, attr, value) for attr, value in kwargs.items())\n    137 ]\n    139 if not matching_timeseries:\n    140     return Dataset()\n\nFile ~/work/gensor/gensor/gensor/core/dataset.py:136, in &lt;genexpr&gt;(.0)\n    126     if isinstance(value, str):\n    127         kwargs[key] = [value]\n    129 matching_timeseries = [\n    130     ts\n    131     for ts in self.timeseries\n    132     if ts is not None\n    133     and (location is None or ts.location in location)\n    134     and (variable is None or ts.variable in variable)\n    135     and (unit is None or ts.unit in unit)\n--&gt; 136     and all(matches(ts, attr, value) for attr, value in kwargs.items())\n    137 ]\n    139 if not matching_timeseries:\n    140     return Dataset()\n\nFile ~/work/gensor/gensor/gensor/core/dataset.py:116, in Dataset.filter.&lt;locals&gt;.matches(ts, attr, value)\n    114 if not hasattr(ts, attr):\n    115     message = f\"'{ts.__class__.__name__}' object has no attribute '{attr}'\"\n--&gt; 116     raise AttributeError(message)\n    117 return getattr(ts, attr) in value\n\nAttributeError: 'Timeseries' object has no attribute 'stations'</pre> In\u00a0[2]: Copied! <pre>ts = compensated_ds[0].model_copy(deep=True)\nts.plot()\n</pre> ts = compensated_ds[0].model_copy(deep=True) ts.plot() <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 ts = compensated_ds[0].model_copy(deep=True)\n      2 ts.plot()\n\nNameError: name 'compensated_ds' is not defined</pre> <p>We will obtain a new Timeseries object containing only the dips in the negative direction. In the case of Kinrooi sensor data, we first are trying to eliminate records which were taken when the diver was out of the water (which means that the water column was 0). It also includes some of the records taken after, when the groudwater level was recovering from pumping.</p> In\u00a0[3]: Copied! <pre>ts_diff = ts.transform(\"difference\", periods=12)\nts_diff_dips = ts_diff.loc[ts_diff.ts &lt; 0]\n\n# we've obtained a timeseries which has identified outliers. We can use those to mask our original series.\nts_identified_outliers = ts_diff_dips.detect_outliers(\"zscore\", threshold=1.0)\nts_identified_outliers.plot(include_outliers=True)\n</pre> ts_diff = ts.transform(\"difference\", periods=12) ts_diff_dips = ts_diff.loc[ts_diff.ts &lt; 0]  # we've obtained a timeseries which has identified outliers. We can use those to mask our original series. ts_identified_outliers = ts_diff_dips.detect_outliers(\"zscore\", threshold=1.0) ts_identified_outliers.plot(include_outliers=True) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 ts_diff = ts.transform(\"difference\", periods=12)\n      2 ts_diff_dips = ts_diff.loc[ts_diff.ts &lt; 0]\n      4 # we've obtained a timeseries which has identified outliers. We can use those to mask our original series.\n\nNameError: name 'ts' is not defined</pre> In\u00a0[4]: Copied! <pre>masked = ts.mask_with(ts_identified_outliers.outliers)\n</pre> masked = ts.mask_with(ts_identified_outliers.outliers) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 masked = ts.mask_with(ts_identified_outliers.outliers)\n\nNameError: name 'ts' is not defined</pre> In\u00a0[5]: Copied! <pre>masked.plot()\n</pre> masked.plot() <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 masked.plot()\n\nNameError: name 'masked' is not defined</pre> <p>By tweaking the paramters of each outlier detection method, we can actually quite accurately remove them from the dataset without doing it sevral times in a loop.</p>"},{"location":"notebooks/001-transformation-and-outliers/#data-transformation-and-outlier-detection","title":"Data transformation and outlier detection\u00b6","text":"<p>Gensor provides some simple ways to find and eliminate outliers from the data. It can also be used in combination, for example we can first transform the data and then remove outliers, what can be an effective strategy in some cases. Below we quickly get the dataset created in the previous tutorial.</p>"},{"location":"notebooks/001-transformation-and-outliers/#transformation","title":"Transformation\u00b6","text":"<p>In Gensor, transformations are implemented to allow flexible data processing and scaling of time series data for normalization, trend removal, variance stabilization, etc.. These transformations are important when working with sensor data, where raw measurements may need to be adjusted to enhance interpretation.</p> <p>The <code>Transformation</code> class in Gensor handles multiple types of transformations, including:</p> <ul> <li>Difference: Used to remove trends by differencing the data over a specified period.</li> <li>Logarithmic (Log): Applied to stabilize variance and reduce the impact of large outliers.</li> <li>Square Root: Another method for stabilizing variance, commonly used for data skewness.</li> <li>Box-Cox: A powerful transformation that normalizes non-normal data, often used when data contains only positive values.</li> <li>Scaling Methods (Standard, MinMax, Robust, MaxAbs): Common normalization techniques that adjust data based on its distribution, commonly used to prepare data for machine learning models.</li> </ul> <p>The Timeseries class integrates this functionality, allowing application of transformations to time series data. Subsequently, user can perform operations on that transformed serie, like outlier removal, and then filter the original timeseries.</p> <p>Below is an example workflow on how to use differencing to enhance outlier detection:</p>"},{"location":"notebooks/002-sqlite-integration/","title":"SQLite itegration","text":"In\u00a0[1]: Copied! <pre>import gensor as gs\nfrom gensor.testdata import all_paths, pb02a_plain\n\npattern = r\"[A-Za-z]{2}\\d{2}[A-Za-z]{1}|Barodiver\"\n\nds = gs.read_from_csv(path=all_paths, file_format=\"vanessen\", location_pattern=pattern)\n\n\nds2 = gs.read_from_csv(\n    path=pb02a_plain, file_format=\"plain\", location=\"PB02A\", sensor=\"AV336\"\n)\n\nds.add(ds2)\n</pre> import gensor as gs from gensor.testdata import all_paths, pb02a_plain  pattern = r\"[A-Za-z]{2}\\d{2}[A-Za-z]{1}|Barodiver\"  ds = gs.read_from_csv(path=all_paths, file_format=\"vanessen\", location_pattern=pattern)   ds2 = gs.read_from_csv(     path=pb02a_plain, file_format=\"plain\", location=\"PB02A\", sensor=\"AV336\" )  ds.add(ds2) <pre>INFO: Loading file: /home/runner/work/gensor/gensor/gensor/testdata/PB02A_plain.csv\n</pre> <pre>INFO: Skipping file /home/runner/work/gensor/gensor/gensor/testdata/PB02A_plain.csv due to missing metadata.\n</pre> <pre>INFO: Loading file: /home/runner/work/gensor/gensor/gensor/testdata/Barodiver_220427183008_BY222.csv\n</pre> <pre>INFO: Loading file: /home/runner/work/gensor/gensor/gensor/testdata/PB01A_moni_AV319_220427183019_AV319.csv\n</pre> <pre>INFO: Loading file: /home/runner/work/gensor/gensor/gensor/testdata/PB02A_plain.csv\n</pre> Out[1]: <pre>Dataset(6)</pre> In\u00a0[2]: Copied! <pre>db = gs.db.DatabaseConnection()\ndf = db.get_timeseries_metadata()\n</pre> db = gs.db.DatabaseConnection() df = db.get_timeseries_metadata() <p>Loading the dataset to the database is straightforward. You just need to call <code>.to_sql()</code> on the dataset instance and check the tables again to see that now there are a few.</p> In\u00a0[3]: Copied! <pre>ds.to_sql(db)\ndf = db.get_timeseries_metadata()\n</pre> ds.to_sql(db) df = db.get_timeseries_metadata() In\u00a0[4]: Copied! <pre>df\n</pre> df Out[4]: table_name location variable unit start end extra cls id 1 barodiver_pressure_cmh2o_7c8d4 Barodiver pressure cmh2o 20200704040000 20220330130000 {'sensor': 'BY222', 'sensor_alt': None} gensor.core.timeseries.Timeseries 2 barodiver_temperature_degc_51dba Barodiver temperature degc 20200704040000 20220330130000 {'sensor': 'BY222', 'sensor_alt': None} gensor.core.timeseries.Timeseries 3 pb01a_pressure_cmh2o_41138 PB01A pressure cmh2o 20200704040000 20220330090000 {'sensor': 'AV319', 'sensor_alt': None} gensor.core.timeseries.Timeseries 4 pb01a_temperature_degc_70e3f PB01A temperature degc 20200704040000 20220330090000 {'sensor': 'AV319', 'sensor_alt': None} gensor.core.timeseries.Timeseries 5 pb02a_pressure_cmh2o_6bf46 PB02A pressure cmh2o 20200704060000 20220207160000 {'sensor': 'AV336', 'sensor_alt': None} gensor.core.timeseries.Timeseries 6 pb02a_temperature_degc_f0ad2 PB02A temperature degc 20200704060000 20220207160000 {'sensor': 'AV336', 'sensor_alt': None} gensor.core.timeseries.Timeseries In\u00a0[5]: Copied! <pre>new_ds: gs.Dataset = gs.read_from_sql(db)\nnew_ds\n</pre> new_ds: gs.Dataset = gs.read_from_sql(db) new_ds Out[5]: <pre>Dataset(6)</pre> In\u00a0[6]: Copied! <pre>ts_with_sensor_alt = new_ds[2].model_copy(update={\"sensor_alt\": 32.0}, deep=True)\n</pre> ts_with_sensor_alt = new_ds[2].model_copy(update={\"sensor_alt\": 32.0}, deep=True) In\u00a0[7]: Copied! <pre>ts_with_sensor_alt\n</pre> ts_with_sensor_alt Out[7]: <pre>Timeseries(variable='pressure', unit='cmh2o', location='PB01A', sensor='AV319', sensor_alt=32.0, start=Timestamp('2020-07-04 04:00:00+0000', tz='UTC'), end=Timestamp('2022-03-30 09:00:00+0000', tz='UTC'))</pre> In\u00a0[8]: Copied! <pre>amended_ds = new_ds.add(ts_with_sensor_alt)\namended_ds.to_sql(db)\n</pre> amended_ds = new_ds.add(ts_with_sensor_alt) amended_ds.to_sql(db) <p>As you see now, we have a Dataset of 7, because the new timeseries is not equal to any of the existing timeseries (differs by <code>sensor_alt</code>).</p> <p>Even though we called <code>to_sql()</code> again on the same dataset extended by just one timeseries, we see that only one new table was created. This is because the method will figure out which timeseries are already there, and at best update those that have new records in the amended <code>Dataset</code>.</p> In\u00a0[9]: Copied! <pre>amended_ds\n</pre> amended_ds Out[9]: <pre>Dataset(7)</pre> In\u00a0[10]: Copied! <pre>df = db.get_timeseries_metadata()\n</pre> df = db.get_timeseries_metadata() In\u00a0[11]: Copied! <pre>df\n</pre> df Out[11]: table_name location variable unit start end extra cls id 1 barodiver_pressure_cmh2o_7c8d4 Barodiver pressure cmh2o 20200704040000 20220330130000 {'sensor': 'BY222', 'sensor_alt': None} gensor.core.timeseries.Timeseries 2 barodiver_temperature_degc_51dba Barodiver temperature degc 20200704040000 20220330130000 {'sensor': 'BY222', 'sensor_alt': None} gensor.core.timeseries.Timeseries 3 pb01a_pressure_cmh2o_41138 PB01A pressure cmh2o 20200704040000 20220330090000 {'sensor': 'AV319', 'sensor_alt': None} gensor.core.timeseries.Timeseries 4 pb01a_temperature_degc_70e3f PB01A temperature degc 20200704040000 20220330090000 {'sensor': 'AV319', 'sensor_alt': None} gensor.core.timeseries.Timeseries 5 pb02a_pressure_cmh2o_6bf46 PB02A pressure cmh2o 20200704060000 20220207160000 {'sensor': 'AV336', 'sensor_alt': None} gensor.core.timeseries.Timeseries 6 pb02a_temperature_degc_f0ad2 PB02A temperature degc 20200704060000 20220207160000 {'sensor': 'AV336', 'sensor_alt': None} gensor.core.timeseries.Timeseries 7 pb01a_pressure_cmh2o_f3363 PB01A pressure cmh2o 20200704040000 20220330090000 {'sensor': 'AV319', 'sensor_alt': 32.0} gensor.core.timeseries.Timeseries"},{"location":"notebooks/002-sqlite-integration/#sqlite-itegration","title":"SQLite itegration\u00b6","text":"<p>If you want to process your sensor data and store it for later, you can use the sqlite integration. Gensor's <code>Timeseries</code> and <code>Dataset</code> come with a <code>.to_sql()</code> method which is uses <code>pandas.Series.to_sql()</code> method under the hood to save the data in a SQLite database.</p> <p>It is a simple implementation, where each timeseries is stored in a separate schema (database table) which is named in the following pattern: <code>f\"{location}_{sensor}_{variable}_{unit}\".lower()</code>. There is a double check on duplicates. First, when you create a <code>Dataset</code>, duplicates are nicely handled by merging timeseries from the same location, sensor and of the same variable and unit. Secondly the <code>Timeseries.to_sql()</code> method is designed to ignore conflicts, so only new records are inserted into the database if you attempt to run the same commend twice.</p>"},{"location":"notebooks/002-sqlite-integration/#load-test-data","title":"Load test data\u00b6","text":""},{"location":"notebooks/002-sqlite-integration/#create-databaseconnection","title":"Create <code>DatabaseConnection</code>\u00b6","text":"<p>Both saving and loading data from sqlite require a <code>DatabaseConnection</code> object to be passed as attribute. You can just instanciate it with empty parentheses to create a new database in the current working directory, or specify the path and name of the database.</p> <p>If you have an existing Gensor database, you can use <code>DatabaseConnection.get_timeseries_metadata()</code> to see if there already are some tables in the database that you want to use. If no arguments are provided, all records are returned.</p>"},{"location":"notebooks/002-sqlite-integration/#saving-dataset-to-sqlite-database","title":"Saving dataset to SQLite database\u00b6","text":"<p>Dataset, like Timeseries, can be saved to a SQLite database by simply calling <code>.to_sql()</code> method and passing the <code>DatabaseConneciton</code> object as argument.</p> <p>You can also check which tables are currently in the database by calling <code>DatabaseConnection.get_timeseries_metadata()</code>. That method will give you a dataframe with all the tables in the database. The names of the tables are composed of the location name, variable measured, unit and a uniqur 5 character hash. This is a compromise between ensuring possible addition of slightly varrying timeseries to the dataset (e.g., the same sensor at the same location but with different rope length).</p> <p>After running the cells below, you should see a dataframe with 6 entries.</p>"},{"location":"notebooks/002-sqlite-integration/#reading-data-from-sqlite","title":"Reading data from SQLite\u00b6","text":"<p>Use <code>read_from_sql()</code> to retrieve timeseries from the database. By default, <code>load_all</code> parameter is set to True, so all tables from the database are loaded as <code>Dataset</code>. You can also provide parameters to retrieve only some of the tables.</p>"},{"location":"notebooks/002-sqlite-integration/#adding-more-timeseries-to-sqlite","title":"Adding more timeseries to SQLite\u00b6","text":"<p>You can always add more timeseries to the same database. Below, we make a copy of one of the timeseries, updating it's <code>sensor_alt</code>, hence, making it slightly different from the origina. Then we add it to the dataset and call <code>to_sql()</code> method again with the same <code>DatabaseConnection</code> object.</p>"},{"location":"notebooks/003-advanced-plotting/","title":"Advanced plotting","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\n\nimport gensor as gs\nfrom gensor.testdata import all_paths\n\ngs.set_log_level(\"WARNING\")\n</pre> import matplotlib.pyplot as plt  import gensor as gs from gensor.testdata import all_paths  gs.set_log_level(\"WARNING\") In\u00a0[2]: Copied! <pre>pattern = r\"[A-Za-z]{2}\\d{2}[A-Za-z]{1}|Barodiver\"\n\nds = gs.read_from_csv(path=all_paths, file_format=\"vanessen\", location_pattern=pattern)\n</pre> pattern = r\"[A-Za-z]{2}\\d{2}[A-Za-z]{1}|Barodiver\"  ds = gs.read_from_csv(path=all_paths, file_format=\"vanessen\", location_pattern=pattern) In\u00a0[3]: Copied! <pre>fig, axs = ds.plot()\nplt.show()\n</pre> fig, axs = ds.plot() plt.show() In\u00a0[4]: Copied! <pre>plot_kwargs = {\"linestyle\": \"--\"}\n\nlegend_kwargs = {\"loc\": \"center left\", \"bbox_to_anchor\": (1, 0.95), \"ncol\": 1}\n</pre> plot_kwargs = {\"linestyle\": \"--\"}  legend_kwargs = {\"loc\": \"center left\", \"bbox_to_anchor\": (1, 0.95), \"ncol\": 1} In\u00a0[5]: Copied! <pre>fig, axs = ds.plot(plot_kwargs=plot_kwargs, legend_kwargs=legend_kwargs)\nplt.show()\n</pre> fig, axs = ds.plot(plot_kwargs=plot_kwargs, legend_kwargs=legend_kwargs) plt.show() <p>This works exactly the same for a Dataset and for a Timeseries.</p> In\u00a0[6]: Copied! <pre>fig, axs = ds.filter(location=\"PB01A\", variable=\"pressure\").plot(\n    plot_kwargs=plot_kwargs, legend_kwargs=legend_kwargs\n)\nplt.show()\n</pre> fig, axs = ds.filter(location=\"PB01A\", variable=\"pressure\").plot(     plot_kwargs=plot_kwargs, legend_kwargs=legend_kwargs ) plt.show() In\u00a0[7]: Copied! <pre>def get_axis_by_title(axes, title):\n    \"\"\"Retrieve an axis from an array of axes based on the title text.\"\"\"\n    for ax in axes.flat:\n        if ax.get_title() == title:\n            return ax\n    message = f\"No axis found with title: '{title}'\"\n    raise ValueError(message)\n</pre> def get_axis_by_title(axes, title):     \"\"\"Retrieve an axis from an array of axes based on the title text.\"\"\"     for ax in axes.flat:         if ax.get_title() == title:             return ax     message = f\"No axis found with title: '{title}'\"     raise ValueError(message) In\u00a0[8]: Copied! <pre>fig, axs = ds.plot(plot_kwargs=plot_kwargs, legend_kwargs=legend_kwargs)\n\nfor ax in axs:\n    ax.grid(True)\n    ax.label_outer()\n\n\nax_temperature = get_axis_by_title(axs, \"Timeseries for Temperature\")\n\nx = ax_temperature.get_lines()[0].get_xdata()\ny = ax_temperature.get_lines()[0].get_ydata()\n\nax_temperature.fill_between(x, y, color=\"skyblue\", alpha=0.7)\n\nplt.show()\n</pre> fig, axs = ds.plot(plot_kwargs=plot_kwargs, legend_kwargs=legend_kwargs)  for ax in axs:     ax.grid(True)     ax.label_outer()   ax_temperature = get_axis_by_title(axs, \"Timeseries for Temperature\")  x = ax_temperature.get_lines()[0].get_xdata() y = ax_temperature.get_lines()[0].get_ydata()  ax_temperature.fill_between(x, y, color=\"skyblue\", alpha=0.7)  plt.show()"},{"location":"notebooks/003-advanced-plotting/#advanced-plotting","title":"Advanced plotting\u00b6","text":"<p>When <code>.plot()</code> is called on a <code>Timeseries</code> or <code>Dataset</code> object, matplotlib axes and fig are returned. Parameters <code>plot_kwargs</code> and <code>legend_kwargs</code> can be used to customize the plot.</p>"},{"location":"notebooks/003-advanced-plotting/#standard-plot","title":"Standard plot\u00b6","text":"<p>Standard plotting will work great if you need to quickly check out a dataset that is not too large. With more timeseries, the standard way of potting becomes less performant and the visual suffers.</p>"},{"location":"notebooks/003-advanced-plotting/#customized-plot","title":"Customized plot\u00b6","text":"<p>When you have a large number of timeseries, it would be good to have control over, for example, the shape and location of the legend or the style of the line. Define the settings in two dictionaries and pass them to the <code>.plot()</code> method.</p>"},{"location":"notebooks/003-advanced-plotting/#further-customization","title":"Further customization\u00b6","text":"<p>The tuple returned from .plot() contains fix and an array of axes. We can iterate over them to attach certain properties to all (e.g., adding grid), or retrieve just one and change properties for a particular ax (e.g., add shading under the graph).</p>"}]}